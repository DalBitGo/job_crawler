# 박준현 | Data Engineer 지원 (토스)

---

## 지원 포지션과 매칭되는 핵심 경험

### 토스 요구사항 → 내 경험

| 토스 요구 | 내 경험 | 상세 |
|----------|--------|------|
| **Python (Airflow 수준)** | ✅ 22개 고객사 DAG 운영 | 의존성 관리, 백필, 모니터링 |
| **데이터 마트 개발** | ✅ BigQuery DW 설계/운영 | 일일 수백만 건 처리 |
| **SQL 능숙** | ✅ BigQuery + PostgreSQL | 복잡한 분석 쿼리 작성 |
| **데이터 일관성** | ✅ Unified Converter 통합 | 3개→1개 통합, 60% 성능↑ |

---

## 핵심 경력 요약

### 하이퍼라운지 | 플랫폼 엔지니어 (3년)

**1. Airflow 기반 대규모 파이프라인 운영**
```
• 22개 고객사 데이터 파이프라인 설계/운영
• 일일 수백만 건 데이터 처리
• DAG 모니터링 시스템 직접 개발 → 장애 대응 50% 단축
```

**2. 데이터 아키텍처 개선**
```
• Unified Converter: 3개 Cloud Function → 1개 Cloud Run 통합
• 결과: 성능 60%↑, API 호출 67%↓, 장애 포인트 67%↓
```

**3. 데이터 품질 관리**
```
• 4개 모니터링 시스템 자체 개발
  - airflow_dag_monitor (DAG 상태 추적)
  - history_checker (이력 관리)
  - converter_failure_monitor (실패 알림)
  - sli_slo (서비스 수준 지표)
```

---

## 토스에서 기여할 수 있는 점

### 1. Airflow 전문성
토스의 요구사항인 "Airflow 사용 가능 수준"을 넘어, **22개 고객사 규모**의 복잡한 파이프라인을 운영한 경험이 있습니다. 의존성 관리, 백필 처리, 실패 복구 등 실전 노하우를 보유하고 있습니다.

### 2. 데이터 일관성 확보 경험
토스에서 강조하는 "일관된 데이터 제공"에 부합하는 경험이 있습니다. 분산된 3개 서비스를 1개로 통합하여 데이터 일관성을 확보하고 성능도 개선했습니다.

### 3. 대규모 서비스 도전 의지
현재 일일 수백만 건을 처리하고 있지만, **토스의 100TB/일 규모**에 도전하고 싶습니다. 개인 프로젝트로 Kafka + Spark를 학습하며 대용량 처리 역량을 키우고 있습니다.

---

## 기술 스택

```
주력:    Python, SQL, Airflow, GCP (Cloud Run, BigQuery, Composer)
사용:    Docker, PostgreSQL, Flask/FastAPI, Grafana
학습중:  Kafka (Producer 구현 완료), Spark Streaming (진행 중)
```

---

## 개인 프로젝트

### realtime-crypto-pipeline
- Binance 실시간 데이터 → Kafka → Spark → DB
- 목표: 10,000 msg/sec 처리
- 현재: Kafka Producer 완료, Spark Streaming 진행 중

---

## 연락처

- **Email**: [이메일]
- **GitHub**: [GitHub URL]
- **Phone**: [전화번호]

---

*토스의 데이터 일관성과 대규모 처리에 기여하고 싶습니다.*
