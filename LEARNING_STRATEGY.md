# 학습 전략: 프로젝트 중심 + 병행 학습

> **목표**: Senior Data Engineer로 성장하기 위한 실전 중심 학습 전략
> **핵심 원칙**: "GitHub에 보여줄 수 있는 결과물"이 최우선
> **작성일**: 2026-01-23

---

## 1. 전략 개요

### 왜 프로젝트 중심인가?

```
면접관: "Kafka 써본 적 있나요?"

❌ kb만 공부한 경우:
   "네, 가이드로 공부했습니다"
   → "실제로 써본 적은요?" → 막힘

✅ 프로젝트 + OSS 분석한 경우:
   "네, realtime-crypto-pipeline에서 사용했습니다.
    초당 10,000건 처리하는데, aiokafka 소스 분석해서
    배치 전송 최적화했더니 처리량 3배 향상됐습니다"
   → GitHub 링크 제시 → 신뢰도 UP
```

### 3단계 병행 학습 구조

```
┌─────────────────────────────────────────────────────────┐
│                    개인 프로젝트 (메인 60%)               │
│                 realtime-crypto-pipeline                 │
│            → GitHub에 보여줄 수 있는 결과물                │
└───────────────────────┬─────────────────────────────────┘
                        │
        ┌───────────────┴───────────────┐
        ▼                               ▼
┌───────────────────┐         ┌───────────────────┐
│   kb 공부 (15%)    │         │  OSS 분석 (25%)    │
│  막힐 때 찾아보기   │         │  "왜 이렇게?" 이해  │
│  Spark/Kafka 챕터  │         │   설계 철학 습득    │
└───────────────────┘         └───────────────────┘
```

---

## 2. 시간 배분

### 주간 계획 (15시간 기준)

| 활동 | 비율 | 시간 | 요일 |
|------|------|------|------|
| **프로젝트 개발** | 60% | 9시간 | 평일 저녁 + 토요일 |
| **OSS 분석** | 25% | 4시간 | 일요일 |
| **kb 참고** | 15% | 2시간 | 필요시 (막힐 때) |

### 일별 루틴

```
┌─────────────────────────────────────────────────────┐
│ 월~금 (평일 저녁 1-2시간)                             │
├─────────────────────────────────────────────────────┤
│ • 프로젝트 코딩                                      │
│ • 막히면 → kb 챕터 찾아서 읽기                        │
│ • 작은 단위로 커밋 (일 1커밋 목표)                     │
└─────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────┐
│ 토요일 (4-5시간) - 프로젝트 집중                      │
├─────────────────────────────────────────────────────┤
│ • 큰 기능 개발                                       │
│ • 한 주 진행사항 정리                                │
│ • README 업데이트                                   │
│ • 성능 측정 및 기록                                  │
└─────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────┐
│ 일요일 (3-4시간) - OSS 분석 + 회고                   │
├─────────────────────────────────────────────────────┤
│ • OSS 소스 코드 읽기                                 │
│ • 배운 점 → 프로젝트 적용 아이디어 메모               │
│ • 다음 주 계획 수립                                  │
│ • (선택) 블로그 포스팅                               │
└─────────────────────────────────────────────────────┘
```

---

## 3. 프로젝트: realtime-crypto-pipeline

### 프로젝트 목표

실시간 암호화폐 가격 수집 → 처리 → 분석 파이프라인

### 기술 스택 & Gap 스킬 매핑

| 컴포넌트 | 기술 | Gap 스킬 해소 |
|----------|------|---------------|
| 데이터 수집 | Kafka Producer | Kafka ✅ |
| 스트림 처리 | Spark Streaming | Spark ✅ |
| 배치 처리 | Spark Batch | Spark ✅ |
| 저장소 | Delta Lake / Parquet | 데이터 레이크 ✅ |
| 오케스트레이션 | Airflow | (기존 역량) |
| 인프라 | Docker + K8s | Kubernetes ✅ |
| 모니터링 | Prometheus + Grafana | Observability ✅ |

### 개발 단계

```
Phase 1: 데이터 수집 (2주)
├── 거래소 API 연동 (Binance, Upbit)
├── Kafka Producer 구현
├── 메시지 스키마 설계
└── 마일스톤: 실시간 가격 데이터 Kafka 토픽에 적재

Phase 2: 스트림 처리 (3주)
├── Spark Streaming Consumer 구현
├── 실시간 캔들 데이터 생성 (1분, 5분, 15분)
├── 이상 탐지 로직 (급등/급락 알림)
└── 마일스톤: 실시간 처리 파이프라인 동작

Phase 3: 배치 처리 (2주)
├── 일간/주간 집계 Spark Job
├── Delta Lake 적재
├── 히스토리 데이터 백필
└── 마일스톤: 일간 리포트 자동 생성

Phase 4: 인프라 & 모니터링 (2주)
├── Docker Compose → Kubernetes 마이그레이션
├── Prometheus 메트릭 수집
├── Grafana 대시보드
└── 마일스톤: 전체 시스템 K8s 배포

Phase 5: 문서화 & 정리 (1주)
├── README 완성
├── 아키텍처 다이어그램
├── 성능 벤치마크 결과
└── 마일스톤: 포트폴리오 완성
```

### 측정 지표 (면접에서 말할 수 있는 숫자)

| 지표 | 목표 | 비고 |
|------|------|------|
| 처리량 | 10,000 msg/sec | Kafka throughput |
| 지연 시간 | < 100ms | end-to-end latency |
| 데이터 규모 | 100GB+ | 3개월 히스토리 |
| 가동 시간 | 99.9% | K8s 자동 복구 |

---

## 4. OSS 분석 계획

### 분석 대상 (프로젝트 단계별)

| 프로젝트 단계 | 분석할 OSS | 배울 점 | 위치 |
|--------------|-----------|---------|------|
| Kafka Producer | `aiokafka` | 비동기 처리, 배치 전송 | ~/oss/aiokafka |
| Spark 처리 | `delta-rs` | 데이터 레이크 설계 | 신규 클론 필요 |
| 스트리밍 | `bytewax` | Python 스트리밍 패턴 | 신규 클론 필요 |
| 모니터링 | `prometheus-client` | 메트릭 수집 패턴 | 신규 클론 필요 |

### OSS 분석 방법론

```
1. 진입점 파악
   └── main.py, __init__.py, CLI 엔트리포인트

2. 핵심 클래스/함수 추적
   └── "이 기능은 어디서 처리하지?"

3. 설계 의도 파악
   └── "왜 이렇게 구현했을까?"
   └── 대안은 뭐가 있었을까?

4. 내 프로젝트에 적용
   └── 배운 패턴 → 코드에 반영
   └── README에 "참고: aiokafka의 X 패턴 적용"

5. (선택) 블로그 포스팅
   └── "aiokafka 소스 분석: 배치 전송의 비밀"
```

### OSS 분석 → 면접 답변 연계

```
면접관: "Kafka Producer 최적화 경험이 있나요?"

✅ 좋은 답변:
"네, aiokafka 소스를 분석해보니 linger_ms와 batch_size 설정이
배치 효율에 핵심이더라고요. 제 프로젝트에서 linger_ms=10,
batch_size=16KB로 설정했더니 처리량이 3,000 → 10,000 msg/sec로
개선됐습니다. 단, 지연이 10ms 늘어나는 트레이드오프가 있어서
실시간성이 중요한 알림 토픽은 별도로 분리했습니다."

→ OSS 분석 + 프로젝트 경험 + 트레이드오프 이해
→ 시니어 레벨 답변
```

---

## 5. kb 활용 가이드

### 위치

```
/home/junhyun/kb/
├── Apache-Spark/          # Spark 15+ 챕터
├── Kafka-Stream-Processing/ # Kafka 15 챕터
├── Kubernetes/            # K8s 가이드
├── Data-Engineering/      # DE 전반
└── ...
```

### 활용 시점

| 상황 | kb 활용 |
|------|---------|
| Spark DataFrame 문법 모를 때 | `kb/Apache-Spark/chapters/chapter-02.md` |
| Kafka Consumer Group 이해 안 될 때 | `kb/Kafka-Stream-Processing/chapters/chapter-03.md` |
| K8s Pod 배포 방법 | `kb/Kubernetes/chapters/` |
| 면접 준비 | `kb/CAREER_DEVELOPMENT_PLAN.md` |

### kb vs 구글링

```
❌ 구글링: 단편적, 품질 불균일, 시간 낭비
✅ kb: 이미 정리됨, 체계적, 바로 적용 가능

→ 먼저 kb 확인 → 없으면 구글링 → 배운 내용 kb에 추가
```

---

## 6. 주간 체크리스트

### 매주 확인할 것

- [ ] GitHub 커밋 5회 이상
- [ ] 프로젝트 진행률 업데이트
- [ ] OSS 분석 노트 작성
- [ ] 다음 주 목표 설정

### 월간 회고

- [ ] 마일스톤 달성 여부
- [ ] 측정 지표 업데이트
- [ ] 학습 시간 분석
- [ ] 전략 조정 필요 여부

---

## 7. 3개월 후 결과물

### 포트폴리오

| 산출물 | 설명 |
|--------|------|
| **GitHub 프로젝트** | realtime-crypto-pipeline |
| **README** | 아키텍처, 성능 지표, 기술 선택 이유 |
| **기술 블로그** | OSS 분석 포스팅 2-3개 |
| **kb 업데이트** | 실습하면서 배운 내용 추가 |

### 면접 준비 완료 상태

```
✅ Spark 경험: "프로젝트에서 일 100GB 배치 처리"
✅ Kafka 경험: "초당 10,000건 실시간 처리"
✅ K8s 경험: "전체 파이프라인 K8s 배포"
✅ 깊이: "aiokafka/delta-rs 소스 분석 경험"
✅ 숫자: "처리량 3배 개선, 지연 100ms 이하"
```

---

## 8. 관련 문서

| 문서 | 내용 |
|------|------|
| `JOB_DEFINITION.md` | 현재 직무 정의 |
| `SKILL_GAP_ANALYSIS.md` | Gap 분석 상세 |
| `CAREER_LEARNING_ROADMAP.md` | 전체 로드맵 |
| `/home/junhyun/kb/` | 학습 자료 (Knowledge Base) |

---

## 9. 즉시 실행 액션

### 이번 주 할 일

- [ ] realtime-crypto-pipeline 저장소 현황 확인
- [ ] Phase 1 세부 태스크 분해
- [ ] aiokafka 클론 및 구조 파악
- [ ] 개발 환경 세팅 (Docker, Kafka 로컬)

### 다음 주 목표

- [ ] Kafka Producer 기본 구현
- [ ] 거래소 API 연동 (1개)
- [ ] 첫 번째 메시지 Kafka 전송 성공

---

*"완성된 하나가 미완성 열 개보다 낫다"*

*프로젝트 하나 제대로 완성하면, 면접에서 30분은 이야기할 수 있다.*

---

*마지막 업데이트: 2026-01-23*
