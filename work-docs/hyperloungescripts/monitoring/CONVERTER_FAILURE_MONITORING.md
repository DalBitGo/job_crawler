# Converter ì‹¤íŒ¨ ëª¨ë‹ˆí„°ë§ ì„¤ê³„

## ë°°ê²½

### í˜„ì¬ ìƒí™©
- **Converter**: ì—‘ì…€ íŒŒì¼ì—ì„œ í…Œì´ë¸” ì¶”ì¶œí•˜ëŠ” í•µì‹¬ ì»´í¬ë„ŒíŠ¸
- **Convert Config**: JSON í˜•íƒœë¡œ ì‹œíŠ¸ë³„/í…Œì´ë¸”ë³„ ì¶”ì¶œ ê·œì¹™ ì •ì˜
  - ì˜ˆ: `c0159c00/eml_gi00_011.json`
  - í—¤ë” ìœ„ì¹˜, ì»¬ëŸ¼ ë²”ìœ„, ì‹œíŠ¸ëª… ë“± ì •ì˜
- **ì‹¤í–‰ ê¸°ë¡**: `convert_job_history` í…Œì´ë¸” (customer_code íŒŒí‹°ì…˜)
  - `status`: "success" / "fail"
  - `error_message`: ì—ëŸ¬ ë‚´ìš©
  - `convert_config_id`: ì‚¬ìš©í•œ config ID
  - `gcs_path`: ì›ë³¸ íŒŒì¼ ê²½ë¡œ

### ìš”êµ¬ì‚¬í•­
1. **ì•„ì¹¨ë§ˆë‹¤ Teamsë¡œ ìë™ ë¦¬í¬íŠ¸** (ê¸°ì¡´ `airflow_dag_monitor` ìŠ¤íƒ€ì¼)
2. **ê³ ê°ì‚¬ë³„ë¡œ í•œëˆˆì— íŒŒì•… ê°€ëŠ¥**
3. **ì—ëŸ¬ ìœ í˜• ë¶„ë¥˜** (í—¤ë” ì—ëŸ¬, ì‹œíŠ¸ ì—ëŸ¬ ë“±)
4. **ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ** ê³ ë ¤ (Teams ë©”ì‹œì§€)
5. **(ì„ íƒ) LLM ë¶„ì„** ì¶”ê°€ ê°€ëŠ¥ì„±
6. **ìƒì„¸ ì •ë³´ëŠ” ë§í¬**ë¡œ ì œê³µ

---

## Converter ë¡œì§ êµ¬ì¡°

### í•µì‹¬ íë¦„
```
1. Config ë¡œë“œ (JSON)
   â†“
2. ì—‘ì…€ ì‹œíŠ¸ ì½ê¸° (pandas)
   â†“
3. í—¤ë” ìŠ¤ìº” (ë™ì ìœ¼ë¡œ í—¤ë” ìœ„ì¹˜ ì°¾ê¸°)
   - "êµ¬ë¶„", "ë§¤ì¶œ" ê°™ì€ í‚¤ì›Œë“œë¡œ ì‹œì‘ í–‰ íƒìƒ‰
   â†“
4. ë°ì´í„° ì¶”ì¶œ (cols, rows ë²”ìœ„)
   â†“
5. Normalization (í—¤ë”ëª… ì •ê·œí™”)
   â†“
6. Type Conversion
   â†“
7. Tagging (ë©”íƒ€ë°ì´í„° íƒœê¹…)
```

### ì£¼ìš” ì‹¤íŒ¨ ì§€ì 
1. **ì‹œíŠ¸ ë§¤ì¹­ ì‹¤íŒ¨**
   - Configì˜ ì‹œíŠ¸ëª… regexì™€ ì‹¤ì œ ì‹œíŠ¸ëª… ë¶ˆì¼ì¹˜
   - ì˜ˆ: "ì•½íš¨" â†’ "ì•½íš¨ë¶„ë¥˜"ë¡œ ë³€ê²½

2. **í—¤ë” ì°¾ê¸° ì‹¤íŒ¨**
   - `header_coordinate`ì— ì •ì˜ëœ í—¤ë”ëª…ì„ ì°¾ì§€ ëª»í•¨
   - ì˜ˆ: "êµ¬ë¶„" â†’ "ì¢…ë¥˜"ë¡œ ë³€ê²½
   - ì˜ˆ: "ë§¤ì¶œ" â†’ "ë§¤ì¶œì•¡"ìœ¼ë¡œ ë³€ê²½

3. **ì»¬ëŸ¼ ë²”ìœ„ ì—ëŸ¬**
   - `cols: "A:D"` ë²”ìœ„ê°€ ì‹¤ì œ ì—‘ì…€ê³¼ ë¶ˆì¼ì¹˜
   - `usecols out of bounds` ì—ëŸ¬

4. **ì‹œìŠ¤í…œ ì—ëŸ¬**
   - Timeout (ëŒ€ìš©ëŸ‰ íŒŒì¼)
   - ë©”ëª¨ë¦¬ ë¶€ì¡±
   - íŒŒì¼ ì†ìƒ

---

## ì—ëŸ¬ ë¶„ë¥˜ ì²´ê³„

### ìë™ ë¶„ë¥˜ ë¡œì§

```python
def classify_error(error_message: str) -> str:
    """ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë¶„ë¥˜"""
    if not error_message:
        return "Unknown"

    patterns = {
        "í—¤ë” ì—ëŸ¬": [
            r"Header .* not found",
            r"Cannot find header",
            r"Missing column",
            r"header_coordinate",
        ],
        "ì‹œíŠ¸ ì—ëŸ¬": [
            r"Sheet .* not found",
            r"Worksheet .* does not exist",
            r"No sheet named",
        ],
        "Timeout": [
            r"[Tt]imeout",
            r"exceed.*time limit",
            r"timed out",
        ],
        "ë©”ëª¨ë¦¬ ë¶€ì¡±": [
            r"[Oo]ut of [Mm]emory",
            r"MemoryError",
        ],
        "íŒŒì¼ ì†ìƒ": [
            r"corrupt",
            r"damaged",
            r"cannot.*read.*file",
        ],
        "ì»¬ëŸ¼ ë²”ìœ„": [
            r"usecols.*out of bounds",
            r"invalid column range",
        ],
    }

    for error_type, regex_list in patterns.items():
        for regex in regex_list:
            if re.search(regex, error_message, re.IGNORECASE):
                return error_type

    return "ê¸°íƒ€"
```

### ì—ëŸ¬ ìœ í˜•ë³„ ì¡°ì¹˜ ê°€ì´ë“œ

| ì—ëŸ¬ ìœ í˜• | ì£¼ìš” ì›ì¸ | ì¡°ì¹˜ ë°©ë²• |
|---------|---------|---------|
| í—¤ë” ì—ëŸ¬ | ì—‘ì…€ í—¤ë”ëª… ë³€ê²½ | Config JSONì—ì„œ í—¤ë”ëª… ì—…ë°ì´íŠ¸ |
| ì‹œíŠ¸ ì—ëŸ¬ | ì‹œíŠ¸ëª… ë³€ê²½ | Config JSONì—ì„œ ì‹œíŠ¸ëª… íŒ¨í„´ ìˆ˜ì • |
| ì»¬ëŸ¼ ë²”ìœ„ | ì—‘ì…€ ì»¬ëŸ¼ êµ¬ì¡° ë³€ê²½ | Config JSONì—ì„œ cols ë²”ìœ„ ì¡°ì • |
| Timeout | íŒŒì¼ í¬ê¸° ê³¼ëŒ€ | GCF ë©”ëª¨ë¦¬/ì‹œê°„ ì¦ì„¤ ë˜ëŠ” íŒŒì¼ ë¶„í•  |
| ë©”ëª¨ë¦¬ ë¶€ì¡± | ë°ì´í„° ê³¼ë‹¤ | GCF ë©”ëª¨ë¦¬ ì¦ì„¤ |
| íŒŒì¼ ì†ìƒ | ì—…ë¡œë“œ ì—ëŸ¬ | ì›ë³¸ íŒŒì¼ ì¬ìˆ˜ì§‘ |

---

## ëª¨ë‹ˆí„°ë§ ë©”ì‹œì§€ ì„¤ê³„ ì˜µì…˜

### ì˜µì…˜ A: ì—ëŸ¬ ìœ í˜•ë³„ ìš”ì•½ (ê°„ê²°)

**ì¥ì **:
- ì—ëŸ¬ íŒ¨í„´ íŒŒì•… ìš©ì´
- ì§§ê³  ìŠ¤ìº” ë¹ ë¦„
- ì—ëŸ¬ ìœ í˜•ë³„ íŠ¸ë Œë“œ í™•ì¸ ê°€ëŠ¥

**ë‹¨ì **:
- íŠ¹ì • ê³ ê°ì‚¬ ë¹ ë¥´ê²Œ ì°¾ê¸° ì–´ë ¤ì›€
- ê³ ê°ì‚¬ë³„ ìƒí™© íŒŒì•…ì—ëŠ” ë¶€ì í•©

**ì˜ˆì‹œ**:
```
ğŸ“Š Converter ì‹¤íŒ¨ ë¦¬í¬íŠ¸ - 2025-11-09

ì „ì²´ í˜„í™©: ì´ 1,247ê±´ ì‹¤í–‰ / 15ê±´ ì‹¤íŒ¨ (1.2%)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”´ í—¤ë” ì—ëŸ¬ (7ê±´)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ GCë…¹ì‹­ì (c0159c00) - 3ê±´
  â”” erp_sa00_011: Header 'GST' not found
â€¢ ê³ í”¼ì (c7005b01) - 2ê±´
  â”” pos_daily: Header 'ë§¤ì¶œ' not found
â€¢ í•œíˆ¬ (c8cd3500) - 2ê±´

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“„ ì‹œíŠ¸ ì—ëŸ¬ (5ê±´)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ ë§¤ì¼í™€ë”©ìŠ¤ (c2026600) - 3ê±´
  â”” sales_report: Sheet 'ì¼ë³„ë§¤ì¶œ' not found
â€¢ ìŠ¤íŒŒì  ë·°í‹° (caaa3b00) - 2ê±´

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸ Timeout (2ê±´)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ ì œì£¼ë§¥ì£¼ (c4cd3b00) - 1ê±´
  â”” monthly_stock: íŒŒì¼ í¬ê¸° 25MB
â€¢ í•œí™”ìƒëª… (c1d66200) - 1ê±´

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ ê¸°íƒ€ (1ê±´)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ ë³´ë ¹ì œì•½ (c3a40f00) - 1ê±´
  â”” usecols out of bounds

ğŸ”— ìƒì„¸ ë³´ê¸°: [BigQuery ì½˜ì†”]
```

---

### ì˜µì…˜ B: ê³ ê°ì‚¬ë³„ í…Œì´ë¸” (ìƒì„¸)

**ì¥ì **:
- ê³ ê°ì‚¬ ì¤‘ì‹¬ ê´€ì 
- íŠ¹ì • ê³ ê°ì‚¬ ë¬¸ì œ ë¹ ë¥´ê²Œ í™•ì¸
- Airflow DAG Monitorì™€ ìœ ì‚¬í•œ í˜•íƒœ

**ë‹¨ì **:
- ê³ ê°ì‚¬ ë§ìœ¼ë©´ ë©”ì‹œì§€ ë„ˆë¬´ ê¸¸ì–´ì§
- ì—ëŸ¬ íŒ¨í„´ íŒŒì•… ì–´ë ¤ì›€
- Teams ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ ì´ˆê³¼ ê°€ëŠ¥

**ì˜ˆì‹œ**:
```
ğŸ“Š Converter ì‹¤íŒ¨ ë¦¬í¬íŠ¸ - 2025-11-09
ì „ì²´: 1,247ê±´ / ì‹¤íŒ¨: 15ê±´ (1.2%)

ê³ ê°ì‚¬           Config ID        ì‹¤íŒ¨ ìœ í˜•    ê±´ìˆ˜
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GCë…¹ì‹­ì         erp_sa00_011     í—¤ë”ì—ëŸ¬     3
(c0159c00)       â”” Header 'GST' not found

ê³ í”¼ì           pos_daily        í—¤ë”ì—ëŸ¬     2
(c7005b01)       â”” Header 'ë§¤ì¶œ' not found

ë§¤ì¼í™€ë”©ìŠ¤       sales_report     ì‹œíŠ¸ì—ëŸ¬     3
(c2026600)       â”” Sheet 'ì¼ë³„ë§¤ì¶œ' not found

ì œì£¼ë§¥ì£¼         monthly_stock    Timeout      1
(c4cd3b00)       â”” 25MB file

ë³´ë ¹ì œì•½         inventory        ê¸°íƒ€         1
(c3a40f00)       â”” usecols out of bounds

ğŸ”— ìƒì„¸: [BigQuery ë§í¬]
```

---

### ì˜µì…˜ C: í•˜ì´ë¸Œë¦¬ë“œ (ê°„ê²° ìš”ì•½ + ì¤‘ìš” í•­ëª©ë§Œ ìƒì„¸) â­ ì¶”ì²œ

**ì¥ì **:
- ì „ì²´ ìƒí™© + ì¤‘ìš” í•­ëª© ëª¨ë‘ íŒŒì•… ê°€ëŠ¥
- ë©”ì‹œì§€ ê¸¸ì´ ì œì–´ ê°€ëŠ¥
- ì—ëŸ¬ íŒ¨í„´ê³¼ ê³ ê°ì‚¬ë³„ ìƒí™© ëª¨ë‘ í™•ì¸ ê°€ëŠ¥
- **ë°˜ë³µ ì‹¤íŒ¨ í•­ëª©ë§Œ** ìƒì„¸ í‘œì‹œë¡œ ì§‘ì¤‘ë„ ë†’ì„

**ë‹¨ì **:
- êµ¬í˜„ ë³µì¡ë„ ì•½ê°„ ë†’ìŒ

**ì˜ˆì‹œ**:
```
ğŸ“Š Converter ì‹¤íŒ¨ ë¦¬í¬íŠ¸ - 2025-11-09
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… ì„±ê³µ: 1,232ê±´ | âŒ ì‹¤íŒ¨: 15ê±´ (1.2%)

ğŸ”´ ì£¼ìš” ì‹¤íŒ¨ ìœ í˜•
â€¢ í—¤ë” ì—ëŸ¬: 7ê±´ (GCë…¹ì‹­ì 3, ê³ í”¼ì 2, í•œíˆ¬ 2)
â€¢ ì‹œíŠ¸ ì—ëŸ¬: 5ê±´ (ë§¤ì¼í™€ë”©ìŠ¤ 3, ìŠ¤íŒŒì  ë·°í‹° 2)
â€¢ Timeout: 2ê±´ (ì œì£¼ë§¥ì£¼, í•œí™”ìƒëª…)
â€¢ ê¸°íƒ€: 1ê±´

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ ì¦‰ì‹œ í™•ì¸ í•„ìš” (ë°˜ë³µ ì‹¤íŒ¨ 3íšŒ ì´ìƒ)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. GCë…¹ì‹­ì (c0159c00) - 5íšŒ ì—°ì† ì‹¤íŒ¨
   â”” erp_sa00_011: Header 'GST' not found
   ğŸ”— ìƒì„¸: [BigQuery ë§í¬] | ğŸ“ Config: [GitHub ë§í¬]

2. ë§¤ì¼í™€ë”©ìŠ¤ (c2026600) - 3íšŒ ì—°ì† ì‹¤íŒ¨
   â”” sales_report: Sheet 'ì¼ë³„ë§¤ì¶œ' not found
   ğŸ”— ìƒì„¸: [BigQuery ë§í¬] | ğŸ“ Config: [GitHub ë§í¬]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ ì „ì²´ ìƒì„¸ ë¦¬í¬íŠ¸
ğŸ”— BigQuery ì¿¼ë¦¬: [ì „ì²´ ì‹¤íŒ¨ ëª©ë¡ ë³´ê¸°]
ğŸ”— Grafana ëŒ€ì‹œë³´ë“œ: [Convert Task Dashboard]
```

**í•µì‹¬ ì•„ì´ë””ì–´**:
- ìƒë‹¨: ì „ì²´ í†µê³„ (ìˆ«ìë§Œ)
- ì¤‘ê°„: ì—ëŸ¬ ìœ í˜•ë³„ ê°„ë‹¨ ìš”ì•½
- **í•˜ë‹¨: ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”í•œ í•­ëª©ë§Œ** (ë°˜ë³µ ì‹¤íŒ¨ NíšŒ ì´ìƒ)
- ë‚˜ë¨¸ì§€ëŠ” ë§í¬ë¡œ

---

## ì¶”ì²œ ì˜ê²¬: ì˜µì…˜ C (í•˜ì´ë¸Œë¦¬ë“œ)

### ì¶”ì²œ ì´ìœ 

1. **ì •ë³´ ê³„ì¸µí™”**
   - ìƒë‹¨: ë¹ ë¥¸ ìŠ¤ìº” (ì „ì²´ ìƒí™©)
   - ì¤‘ê°„: ì—ëŸ¬ íŒ¨í„´ íŒŒì•…
   - í•˜ë‹¨: ì•¡ì…˜ ì•„ì´í…œ (ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”)

2. **ë…¸ì´ì¦ˆ ì œê±°**
   - 1-2íšŒ ì‹¤íŒ¨ëŠ” ì¼ì‹œì ì¼ ìˆ˜ ìˆìŒ (ë„¤íŠ¸ì›Œí¬, íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ë“±)
   - **ë°˜ë³µ ì‹¤íŒ¨ë§Œ** ìƒì„¸ í‘œì‹œ â†’ S/Në¹„ ë†’ìŒ

3. **ê¸¸ì´ ì œì–´**
   - í‰ìƒì‹œ: ì§§ê³  ê°„ê²°
   - ë¬¸ì œ ë§ì„ ë•Œ: ë°˜ë³µ ì‹¤íŒ¨ í•­ëª©ë§Œ í‘œì‹œ
   - ì „ì²´ëŠ” ë§í¬ë¡œ

4. **ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° ìœ ì§€**
   - íŒ€ì›ë“¤ì´ ì´ë¯¸ ìµìˆ™í•œ Airflow DAG Monitorì™€ ìœ ì‚¬í•œ êµ¬ì¡°
   - í•™ìŠµ ê³¡ì„  ë‚®ìŒ

### ë°˜ë³µ ì‹¤íŒ¨ ê¸°ì¤€ ì œì•ˆ

- **3íšŒ ì´ìƒ ì—°ì† ì‹¤íŒ¨**: ì¦‰ì‹œ í™•ì¸ í•„ìš” ì„¹ì…˜ì— í‘œì‹œ
- **1-2íšŒ ì‹¤íŒ¨**: ìš”ì•½ì—ë§Œ í¬í•¨, ìƒì„¸ëŠ” ë§í¬ë¡œ

### êµ¬í˜„ ë³µì¡ë„

**ê°„ë‹¨í•œ ë¶€ë¶„**:
- BigQuery ì¿¼ë¦¬
- ì—ëŸ¬ ë¶„ë¥˜ (ì •ê·œì‹)
- Teams ë©”ì‹œì§€ í¬ë§·íŒ…

**ì¡°ê¸ˆ ë³µì¡í•œ ë¶€ë¶„**:
- ë°˜ë³µ ì‹¤íŒ¨ ê°ì§€ (ìµœê·¼ Nì¼ê°„ ê°™ì€ config_id ì‹¤íŒ¨ ì¹´ìš´íŠ¸)
- ë§í¬ ìƒì„± (BigQuery ì½˜ì†”, GitHub config íŒŒì¼)

---

## LLM ë¶„ì„ ì¶”ê°€ (ì„ íƒì‚¬í•­)

### ê°€ëŠ¥í•œ ë¶„ì„

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¤– AI ë¶„ì„ ìš”ì•½
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ GCë…¹ì‹­ì (c0159c00): ìµœê·¼ ì—‘ì…€ íŒŒì¼ í¬ë§· ë³€ê²½ ì¶”ì •
  â”” 'GST' â†’ 'GST_ê¸ˆì•¡'ìœ¼ë¡œ í—¤ë”ëª… ë³€ê²½ëœ ê²ƒìœ¼ë¡œ ì¶”ì •
  ğŸ’¡ ì¡°ì¹˜: config erp_sa00_011.jsonì˜ í—¤ë”ëª… ì—…ë°ì´íŠ¸ í•„ìš”

â€¢ ë§¤ì¼í™€ë”©ìŠ¤ (c2026600): ì‹œíŠ¸ëª… ë³€ê²½ ê°ì§€
  â”” 'ì¼ë³„ë§¤ì¶œ' â†’ 'ì¼ë³„_ë§¤ì¶œí˜„í™©'ìœ¼ë¡œ ë³€ê²½ëœ ê²ƒìœ¼ë¡œ ì¶”ì •
  ğŸ’¡ ì¡°ì¹˜: config sales_report.jsonì˜ ì‹œíŠ¸ëª… ìˆ˜ì • í•„ìš”
```

### í•œê³„ì 
- **ì—ëŸ¬ ë©”ì‹œì§€ë§Œìœ¼ë¡œëŠ” ì •í™•í•œ ì›ì¸ íŒŒì•… ì–´ë ¤ì›€**
- ì‹¤ì œ ì—‘ì…€ íŒŒì¼ì„ ì½ì–´ë´ì•¼ ì •í™•í•œ ë¶„ì„ ê°€ëŠ¥
- LLM ë¹„ìš© ê³ ë ¤ í•„ìš”

### ëŒ€ì•ˆ: íœ´ë¦¬ìŠ¤í‹± ë¶„ì„
LLM ëŒ€ì‹  ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜:
```python
if "Header 'GST' not found":
    suggest = "ğŸ’¡ Configì—ì„œ 'GST' í—¤ë”ëª…ì„ í™•ì¸í•˜ê³ , ì‹¤ì œ íŒŒì¼ì˜ í—¤ë”ëª…ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦ í•„ìš”"
elif "Sheet 'ì¼ë³„ë§¤ì¶œ' not found":
    suggest = "ğŸ’¡ ì‹¤ì œ ì—‘ì…€ íŒŒì¼ì˜ ì‹œíŠ¸ ëª©ë¡ì„ í™•ì¸í•˜ê³ , Configì˜ ì‹œíŠ¸ëª… ì—…ë°ì´íŠ¸ í•„ìš”"
```

---

## êµ¬í˜„ ê³„íš

### Phase 1: ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ (1-2ì¼)
- [ ] BigQuery ì¿¼ë¦¬ ì‘ì„±
  - ì–´ì œ ì‹¤íŒ¨ ê±´ ì¡°íšŒ
  - ì—ëŸ¬ ë©”ì‹œì§€ë³„ ê·¸ë£¹í•‘
  - ê³ ê°ì‚¬ë³„ ì§‘ê³„
- [ ] ì—ëŸ¬ ë¶„ë¥˜ ë¡œì§ êµ¬í˜„
- [ ] Teams ë©”ì‹œì§€ í¬ë§·íŒ… (ì˜µì…˜ C)
- [ ] Cloud Function ë°°í¬
- [ ] Cloud Scheduler ì„¤ì • (ë§¤ì¼ ì˜¤ì „ 8ì‹œ)

### Phase 2: ë°˜ë³µ ì‹¤íŒ¨ ê°ì§€ (0.5ì¼)
- [ ] ìµœê·¼ Nì¼ê°„ ì‹¤íŒ¨ ì´ë ¥ ì¡°íšŒ
- [ ] ê°™ì€ config_id ì‹¤íŒ¨ ì¹´ìš´íŠ¸
- [ ] ì„ê³„ê°’ ì„¤ì • (3íšŒ ì´ìƒ)

### Phase 3: ë§í¬ ìƒì„± (0.5ì¼)
- [ ] BigQuery ì½˜ì†” URL ìë™ ìƒì„±
- [ ] GitHub Config íŒŒì¼ ë§í¬
- [ ] Grafana ëŒ€ì‹œë³´ë“œ ë§í¬

### Phase 4: (ì„ íƒ) LLM ë¶„ì„ (1-2ì¼)
- [ ] ì—ëŸ¬ ë©”ì‹œì§€ â†’ LLM í”„ë¡¬í”„íŠ¸
- [ ] ì¡°ì¹˜ ì œì•ˆ ìƒì„±
- [ ] ë¹„ìš©/íš¨ê³¼ ê²€ì¦

---

## BigQuery ì¿¼ë¦¬ (ì´ˆì•ˆ)

### ì–´ì œ ì‹¤íŒ¨ ê±´ ì¡°íšŒ + ì—ëŸ¬ ë¶„ë¥˜
```sql
WITH error_classification AS (
  SELECT
    customer_code,
    customer_name,
    convert_config_id,
    error_message,
    COUNT(*) as fail_count,
    ARRAY_AGG(
      STRUCT(gcs_path, created_at)
      ORDER BY created_at DESC
      LIMIT 3
    ) as recent_cases,

    -- ì—ëŸ¬ ë¶„ë¥˜
    CASE
      WHEN REGEXP_CONTAINS(error_message, r"(?i)Header .* not found") THEN 'í—¤ë” ì—ëŸ¬'
      WHEN REGEXP_CONTAINS(error_message, r"(?i)Sheet .* not found") THEN 'ì‹œíŠ¸ ì—ëŸ¬'
      WHEN REGEXP_CONTAINS(error_message, r"(?i)timeout") THEN 'Timeout'
      WHEN REGEXP_CONTAINS(error_message, r"(?i)out of memory") THEN 'ë©”ëª¨ë¦¬ ë¶€ì¡±'
      WHEN REGEXP_CONTAINS(error_message, r"(?i)usecols.*out of bounds") THEN 'ì»¬ëŸ¼ ë²”ìœ„'
      ELSE 'ê¸°íƒ€'
    END as error_type

  FROM `hyperlounge-dev.dashboard.convert_job_history`
  WHERE DATE(created_at, 'Asia/Seoul') = CURRENT_DATE('Asia/Seoul') - 1
    AND status = 'fail'
  GROUP BY customer_code, customer_name, convert_config_id, error_message
)

SELECT
  error_type,
  customer_code,
  customer_name,
  convert_config_id,
  error_message,
  fail_count,
  recent_cases
FROM error_classification
ORDER BY error_type, fail_count DESC
```

### ë°˜ë³µ ì‹¤íŒ¨ ê°ì§€
```sql
WITH recent_failures AS (
  SELECT
    customer_code,
    customer_name,
    convert_config_id,
    DATE(created_at, 'Asia/Seoul') as fail_date,
    COUNT(*) as daily_fail_count
  FROM `hyperlounge-dev.dashboard.convert_job_history`
  WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
    AND status = 'fail'
  GROUP BY customer_code, customer_name, convert_config_id, fail_date
)

SELECT
  customer_code,
  customer_name,
  convert_config_id,
  COUNT(DISTINCT fail_date) as consecutive_fail_days,
  SUM(daily_fail_count) as total_fail_count
FROM recent_failures
GROUP BY customer_code, customer_name, convert_config_id
HAVING consecutive_fail_days >= 3  -- 3ì¼ ì´ìƒ ì—°ì† ì‹¤íŒ¨
ORDER BY consecutive_fail_days DESC, total_fail_count DESC
```

---

## ì°¸ê³  ìë£Œ

### ê¸°ì¡´ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
- `airflow_dag_monitor/main.py`: Airflow DAG ì‹¤í–‰ ìƒíƒœ ëª¨ë‹ˆí„°ë§
- `collector/deploy/collect_history_checker/`: Collect ì‘ì—… íˆìŠ¤í† ë¦¬ ì²´í¬

### ê´€ë ¨ í…Œì´ë¸”
- `dashboard.convert_job_history`: Converter ì‹¤í–‰ ê¸°ë¡
- `dashboard.convert_job_history_v2`: V2 ìŠ¤í‚¤ë§ˆ

### Config íŒŒì¼ ìœ„ì¹˜
- `collector/c{customer_code}/{config_id}.json`
- ì˜ˆ: `collector/c0159c00/eml_gi00_011.json`

---

## ì¶”ê°€ ê³ ë ¤ì‚¬í•­ (2025-11-10 ë…¼ì˜)

### ë¬¸ì œ 1: ì—ëŸ¬ ì–‘ì´ ë§¤ìš° ë§ì„ ìˆ˜ ìˆìŒ

**ì›ì¸**:
- ConverterëŠ” **í…Œì´ë¸” ë‹¨ìœ„**ë¡œ ì‹¤íŒ¨ ê¸°ë¡
- ì—‘ì…€ íŒŒì¼ 1ê°œ â†’ ì‹œíŠ¸ 10ê°œ â†’ ê° ì‹œíŠ¸ì— í…Œì´ë¸” 2-3ê°œ
- **í•˜ë‚˜ì˜ íŒŒì¼ ë¬¸ì œ**ê°€ **ìˆ˜ì‹­ ê±´ì˜ ì—ëŸ¬**ë¡œ ì¦í­ë¨

**ì˜ˆì‹œ**:
```
íŒŒì¼: sales_2025-11-09.xlsx (ì‹œíŠ¸ 10ê°œ)
â”œâ”€ ì‹œíŠ¸1 "ì¼ë³„ë§¤ì¶œ" â†’ í…Œì´ë¸” 3ê°œ ì‹¤íŒ¨ (í—¤ë” ë³€ê²½)
â”œâ”€ ì‹œíŠ¸2 "ì›”ë³„ë§¤ì¶œ" â†’ í…Œì´ë¸” 2ê°œ ì‹¤íŒ¨ (í—¤ë” ë³€ê²½)
â”œâ”€ ...
â””â”€ ì´ 25ê±´ ì‹¤íŒ¨ (ì‹¤ì œë¡œëŠ” 1ê°œ íŒŒì¼ì˜ 1ê°œ ë¬¸ì œ)
```

**í•´ê²° ë°©ì•ˆ**:
1. **íŒŒì¼ë³„ ê·¸ë£¹í•‘** â­ ì¶”ì²œ
   ```
   âŒ GCë…¹ì‹­ì - sales_2025-11-09.xlsx (25ê±´ ì‹¤íŒ¨)
      â”” ì›ì¸: Header 'GST' not found
      â”” ì˜í–¥ í…Œì´ë¸”: erp_sa00_011 ì™¸ 12ê°œ
   ```
   - ê°™ì€ `gcs_path` ë¬¶ì–´ì„œ 1ê°œ í•­ëª©ìœ¼ë¡œ í‘œì‹œ
   - ì‹¤ì œ íŒŒì¼ ìˆ˜ = ì‹¤ì œ ëŒ€ì‘í•´ì•¼ í•  ê±´ìˆ˜

2. **ì—ëŸ¬ ë©”ì‹œì§€ë³„ ê·¸ë£¹í•‘**
   ```
   âŒ Header 'GST' not found - 25ê±´
      â”” GCë…¹ì‹­ì: 3ê°œ íŒŒì¼
      â”” í•œíˆ¬: 2ê°œ íŒŒì¼
   ```
   - ê°™ì€ ì—ëŸ¬ = ê°™ì€ config ìˆ˜ì •ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥

3. **ì„ê³„ê°’ ì¡°ì •**
   - ~~ê±´ìˆ˜ ê¸°ì¤€~~ì´ ì•„ë‹ˆë¼ **íŒŒì¼ ê°œìˆ˜ ê¸°ì¤€**
   - "ê°™ì€ configê°€ 3ê°œ íŒŒì¼ì—ì„œ ì—°ì† ì‹¤íŒ¨" â†’ ê²½ê³ 

---

### ë¬¸ì œ 2: Noise - ì˜ëª»ëœ íŒŒì¼ ì—…ë¡œë“œ

**ì‹œë‚˜ë¦¬ì˜¤**:
1. **ê³ ê°ì‚¬ê°€ ì˜ëª»ëœ íŒŒì¼ ì—…ë¡œë“œ**
   - ë‹¤ë¥¸ ì–‘ì‹ì˜ íŒŒì¼
   - ë¹ˆ íŒŒì¼
   - ì†ìƒëœ íŒŒì¼
2. ConverterëŠ” ë‹¹ì—°íˆ ì‹¤íŒ¨
3. ê·¼ë° ì´ê±´ **config ë¬¸ì œê°€ ì•„ë‹˜** (ì›ë³¸ íŒŒì¼ ë¬¸ì œ)
4. **êµ¬ë¶„ì´ ì•ˆ ë¨** â† í•µì‹¬ ë¬¸ì œ!

**í˜„ì¬ ìƒí™©**:
```sql
-- convert_job_history í…Œì´ë¸”
status = 'fail'
error_message = 'Header êµ¬ë¶„ not found'

Q: ì´ê²Œ config ë¬¸ì œ? íŒŒì¼ ë¬¸ì œ?
A: ì•Œ ìˆ˜ ì—†ìŒ
```

**ê°€ëŠ¥í•œ êµ¬ë¶„ ë°©ë²•**:

#### ì˜µì…˜ 1: íŒŒì¼ëª… íŒ¨í„´ ì²´í¬
```python
# ì •ìƒ íŒŒì¼ëª… íŒ¨í„´
normal_pattern = r"sales_\d{4}-\d{2}-\d{2}\.xlsx"

if not re.match(normal_pattern, filename):
    noise_type = "íŒŒì¼ëª… ì´ìƒ"
```
**í•œê³„**: íŒŒì¼ëª…ì€ ë§ëŠ”ë° ë‚´ìš©ì´ ë‹¤ë¥¸ ê²½ìš° êµ¬ë¶„ ë¶ˆê°€

#### ì˜µì…˜ 2: ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´ ë¶„ì„ â­
```python
# Config ë¬¸ì œì¼ ê°€ëŠ¥ì„± ë†’ìŒ
config_error_patterns = [
    "Header .* not found",      # í—¤ë”ëª… ë³€ê²½
    "Sheet .* not found",        # ì‹œíŠ¸ëª… ë³€ê²½
    "usecols out of bounds",     # ì»¬ëŸ¼ ë²”ìœ„ ë³€ê²½
]

# íŒŒì¼ ë¬¸ì œì¼ ê°€ëŠ¥ì„± ë†’ìŒ
file_error_patterns = [
    "corrupt",                   # íŒŒì¼ ì†ìƒ
    "cannot read file",          # ì½ê¸° ë¶ˆê°€
    "empty.*sheet",              # ë¹ˆ ì‹œíŠ¸
    "no sheets found",           # ì‹œíŠ¸ ì—†ìŒ
]

# ì• ë§¤í•¨ - ë‘˜ ë‹¤ ê°€ëŠ¥
ambiguous_patterns = [
    "timeout",                   # íŒŒì¼ í¬ê¸°? ì‹œìŠ¤í…œ?
    "out of memory",             # íŒŒì¼ í¬ê¸°? ì‹œìŠ¤í…œ?
]
```
**ì¥ì **: ì¶”ê°€ ì¸í”„ë¼ ë¶ˆí•„ìš”
**í•œê³„**: 100% ì •í™•í•˜ì§€ ì•ŠìŒ

#### ì˜µì…˜ 3: ê³¼ê±° ì„±ê³µ ì´ë ¥ ë¹„êµ â­â­
```sql
WITH recent_history AS (
  SELECT
    customer_code,
    convert_config_id,
    COUNT(CASE WHEN status = 'success' THEN 1 END) as success_count,
    COUNT(CASE WHEN status = 'fail' THEN 1 END) as fail_count
  FROM convert_job_history
  WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
  GROUP BY customer_code, convert_config_id
)

-- ê³¼ê±° 30ì¼ê°„ ì„±ê³µí•œ ì  ìˆìŒ â†’ ConfigëŠ” ì •ìƒ, íŒŒì¼ ë¬¸ì œì¼ ê°€ëŠ¥ì„± ë†’ìŒ
-- ê³¼ê±° 30ì¼ê°„ í•œ ë²ˆë„ ì„±ê³µ ì•ˆ í•¨ â†’ Config ë¬¸ì œì¼ ê°€ëŠ¥ì„± ë†’ìŒ
```
**ì¥ì **: ê°€ì¥ ì‹ ë¢°ë„ ë†’ìŒ
**ë‹¨ì **: ì‹ ê·œ ê³ ê°ì‚¬ëŠ” íŒë‹¨ ë¶ˆê°€

#### ì˜µì…˜ 4: ê°™ì€ ë‚  ë‹¤ë¥¸ íŒŒì¼ ì„±ê³µ ì—¬ë¶€ â­â­â­ ìµœê³ 
```sql
-- ê°™ì€ ë‚ , ê°™ì€ config_idë¡œ ë‹¤ë¥¸ íŒŒì¼ë“¤ì€ ì„±ê³µí–ˆëŠ”ê°€?
WITH todays_results AS (
  SELECT
    customer_code,
    convert_config_id,
    DATE(created_at) as date,
    COUNT(DISTINCT CASE WHEN status = 'success' THEN gcs_path END) as success_files,
    COUNT(DISTINCT CASE WHEN status = 'fail' THEN gcs_path END) as fail_files
  FROM convert_job_history
  WHERE DATE(created_at) = CURRENT_DATE() - 1
  GROUP BY customer_code, convert_config_id, date
)

-- success_files > 0 AND fail_files > 0
-- â†’ ê°™ì€ configë¡œ ì–´ë–¤ íŒŒì¼ì€ ì„±ê³µ, ì–´ë–¤ íŒŒì¼ì€ ì‹¤íŒ¨
-- â†’ ì‹¤íŒ¨í•œ íŒŒì¼ì´ ë¬¸ì œì¼ ê°€ëŠ¥ì„± ë†’ìŒ (Noise)

-- success_files = 0 AND fail_files > 0
-- â†’ ëª¨ë“  íŒŒì¼ì´ ì‹¤íŒ¨
-- â†’ Config ë¬¸ì œì¼ ê°€ëŠ¥ì„± ë†’ìŒ (ì§„ì§œ ì—ëŸ¬)
```
**ì¥ì **:
- ê°™ì€ ë‚  ê¸°ì¤€ì´ë¼ ì‹ ë¢°ë„ ë†’ìŒ
- ì‹ ê·œ ê³ ê°ì‚¬ë„ íŒë‹¨ ê°€ëŠ¥
**ë‹¨ì **:
- í•˜ë£¨ì— íŒŒì¼ 1ê°œë§Œ ì˜¤ëŠ” ê²½ìš° íŒë‹¨ ë¶ˆê°€

#### ì˜µì…˜ 5: í•˜ì´ë¸Œë¦¬ë“œ (3 + 4) â­â­â­ ìµœì¢… ì¶”ì²œ
```python
def classify_failure_type(row):
    """
    ì‹¤íŒ¨ë¥¼ 'ì§„ì§œ ì—ëŸ¬' vs 'Noise'ë¡œ ë¶„ë¥˜
    """
    # 1. ê°™ì€ ë‚  ë‹¤ë¥¸ íŒŒì¼ ì„±ê³µ ì—¬ë¶€ í™•ì¸
    if row['same_day_success_files'] > 0:
        return "Noise (íŠ¹ì • íŒŒì¼ ë¬¸ì œ)"

    # 2. ê³¼ê±° 30ì¼ê°„ ì„±ê³µ ì´ë ¥ í™•ì¸
    if row['past_30d_success_count'] > 0:
        return "Noise (ì¼ì‹œì  ë¬¸ì œ)"

    # 3. ê³¼ê±° ì´ë ¥ ì—†ìŒ â†’ Config ë¬¸ì œ ê°€ëŠ¥ì„± ë†’ìŒ
    if row['past_30d_total_count'] == 0:
        return "ì‹ ê·œ Config (ê²€ì¦ í•„ìš”)"

    # 4. ê³¼ê±°ì—ë„ ê³„ì† ì‹¤íŒ¨
    if row['past_30d_fail_rate'] > 0.8:
        return "ì§„ì§œ ì—ëŸ¬ (Config ìˆ˜ì • í•„ìš”)"

    return "í™•ì¸ í•„ìš”"
```

---

### ë¬¸ì œ 3: ì‹¤ì œ ëŒ€ì‘ì´ ëª©í‘œ

**í˜„ì¬ ë¬¸ì œ**:
- ì—ëŸ¬ 100ê±´ ë‚˜ì™€ë„ ì‹¤ì œë¡œëŠ” íŒŒì¼ 2-3ê°œ ë¬¸ì œì¼ ìˆ˜ ìˆìŒ
- ë˜ëŠ” Config 1ê°œë§Œ ìˆ˜ì •í•˜ë©´ í•´ê²°ë  ìˆ˜ ìˆìŒ
- **ë­˜ ê³ ì³ì•¼ í•˜ëŠ”ì§€ê°€ ëª…í™•í•˜ì§€ ì•ŠìŒ**

**í•„ìš”í•œ ì •ë³´**:
```
âœ… ì´ìƒì ì¸ ë¦¬í¬íŠ¸:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš” (Config ìˆ˜ì •)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. GCë…¹ì‹­ì - erp_sa00_011
   â”œ ì‹¤íŒ¨: ì—°ì† 5ì¼, ì´ 125ê±´
   â”œ ì›ì¸: Header 'GST' not found
   â”œ ì˜í–¥: ë§¤ì¼ 25ê°œ í…Œì´ë¸” ì‹¤íŒ¨ ì¤‘
   â”” ì¡°ì¹˜: Config í—¤ë”ëª… 'GST' â†’ 'GST_ê¸ˆì•¡' ìˆ˜ì •
   ğŸ”— Config: [GitHub] | ğŸ”— ì‹¤íŒ¨ íŒŒì¼: [GCS]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ Noise (íŒŒì¼ ë¬¸ì œ - Config ìˆ˜ì • ë¶ˆí•„ìš”)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ ë§¤ì¼í™€ë”©ìŠ¤ - sales_report
  â”” ì–´ì œ 3ê°œ íŒŒì¼ ì¤‘ 1ê°œë§Œ ì‹¤íŒ¨ (ë‚˜ë¨¸ì§€ 2ê°œ ì„±ê³µ)
  â”” ì›ì¸: íŒŒì¼ ì†ìƒ or ì˜ëª»ëœ ì–‘ì‹
  â”” ì¡°ì¹˜: ê³ ê°ì‚¬ì— íŒŒì¼ ì¬ì „ì†¡ ìš”ì²­
```

**í•µì‹¬**:
1. **"ì§„ì§œ ì—ëŸ¬" vs "Noise" êµ¬ë¶„**
2. **íŒŒì¼ ê°œìˆ˜ ê¸°ì¤€**ìœ¼ë¡œ í‘œì‹œ (í…Œì´ë¸” ê±´ìˆ˜ ì•„ë‹˜)
3. **ì¡°ì¹˜ ë°©ë²•** ëª…ì‹œ
   - Config ìˆ˜ì • í•„ìš” â†’ ì–´ëŠ í•„ë“œ?
   - íŒŒì¼ ë¬¸ì œ â†’ ê³ ê°ì‚¬ ë¬¸ì˜
   - ì‹œìŠ¤í…œ ë¬¸ì œ â†’ ì¸í”„ë¼ íŒ€

---

## ê°œì„ ëœ ì„¤ê³„ ë°©í–¥

### 1. ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„

```sql
WITH base_failures AS (
  -- ì–´ì œ ì‹¤íŒ¨ ê±´
  SELECT *
  FROM convert_job_history
  WHERE DATE(created_at, 'Asia/Seoul') = CURRENT_DATE('Asia/Seoul') - 1
    AND status = 'fail'
),

file_level_summary AS (
  -- íŒŒì¼ë³„ ê·¸ë£¹í•‘
  SELECT
    customer_code,
    customer_name,
    convert_config_id,
    gcs_path,
    error_message,
    COUNT(*) as table_fail_count,
    MIN(created_at) as first_fail,
    MAX(created_at) as last_fail
  FROM base_failures
  GROUP BY 1,2,3,4,5
),

same_day_context AS (
  -- ê°™ì€ ë‚  ê°™ì€ configì˜ ì„±ê³µ/ì‹¤íŒ¨ íŒŒì¼ ê°œìˆ˜
  SELECT
    customer_code,
    convert_config_id,
    COUNT(DISTINCT CASE WHEN status = 'success' THEN gcs_path END) as success_files,
    COUNT(DISTINCT CASE WHEN status = 'fail' THEN gcs_path END) as fail_files
  FROM convert_job_history
  WHERE DATE(created_at, 'Asia/Seoul') = CURRENT_DATE('Asia/Seoul') - 1
  GROUP BY 1,2
),

historical_context AS (
  -- ê³¼ê±° 30ì¼ ì„±ê³µë¥ 
  SELECT
    customer_code,
    convert_config_id,
    COUNT(*) as total,
    COUNTIF(status = 'success') as success,
    COUNTIF(status = 'fail') as fail,
    SAFE_DIVIDE(COUNTIF(status = 'fail'), COUNT(*)) as fail_rate
  FROM convert_job_history
  WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
  GROUP BY 1,2
)

SELECT
  f.*,
  s.success_files,
  s.fail_files,
  h.total as hist_total,
  h.success as hist_success,
  h.fail_rate as hist_fail_rate,

  -- Noise íŒì •
  CASE
    WHEN s.success_files > 0 THEN 'Noise - íŠ¹ì • íŒŒì¼ ë¬¸ì œ'
    WHEN h.success > 0 AND h.fail_rate < 0.2 THEN 'Noise - ì¼ì‹œì  ë¬¸ì œ'
    WHEN h.total = 0 THEN 'ì‹ ê·œ Config'
    WHEN h.fail_rate > 0.8 THEN 'ì§„ì§œ ì—ëŸ¬ - Config ìˆ˜ì • í•„ìš”'
    ELSE 'í™•ì¸ í•„ìš”'
  END as failure_category

FROM file_level_summary f
LEFT JOIN same_day_context s USING (customer_code, convert_config_id)
LEFT JOIN historical_context h USING (customer_code, convert_config_id)
```

### 2. ë©”ì‹œì§€ í¬ë§· (ê°œì„ )

```
ğŸ“Š Converter ì‹¤íŒ¨ ë¦¬í¬íŠ¸ - 2025-11-09
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… ì„±ê³µ: 1,232ê±´ | âŒ ì‹¤íŒ¨: 125ê±´ (ì‹¤ì œ íŒŒì¼ 12ê°œ)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”´ ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš” (Config ìˆ˜ì •) - 2ê°œ Config
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. GCë…¹ì‹­ì (c0159c00) - erp_sa00_011
   â”œ ì—°ì† 5ì¼ ì‹¤íŒ¨ (ì´ 125ê±´ = 5ê°œ íŒŒì¼ x 25ê°œ í…Œì´ë¸”)
   â”œ ì—ëŸ¬: Header 'GST' not found
   â”œ ê³¼ê±° 30ì¼: ì„±ê³µë¥  95% â†’ ìµœê·¼ 0%
   â”” ğŸ’¡ ì¡°ì¹˜: Config í—¤ë”ëª… í™•ì¸ (ì—‘ì…€ í¬ë§· ë³€ê²½ ì¶”ì •)
   ğŸ”— [Config] | ğŸ”— [ì‹¤íŒ¨ íŒŒì¼] | ğŸ”— [BigQuery]

2. ë§¤ì¼í™€ë”©ìŠ¤ (c2026600) - sales_report
   â”œ ì—°ì† 3ì¼ ì‹¤íŒ¨ (ì´ 45ê±´ = 3ê°œ íŒŒì¼ x 15ê°œ í…Œì´ë¸”)
   â”œ ì—ëŸ¬: Sheet 'ì¼ë³„ë§¤ì¶œ' not found
   â”” ğŸ’¡ ì¡°ì¹˜: Config ì‹œíŠ¸ëª… í™•ì¸
   ğŸ”— [Config] | ğŸ”— [ì‹¤íŒ¨ íŒŒì¼] | ğŸ”— [BigQuery]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ Noise (íŒŒì¼ ë¬¸ì œ) - 5ê°œ íŒŒì¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ ê³ í”¼ì (c7005b01): ì–´ì œ 10ê°œ íŒŒì¼ ì¤‘ 2ê°œë§Œ ì‹¤íŒ¨
  â”” ğŸ’¡ ì¡°ì¹˜: ê³ ê°ì‚¬ì— íŒŒì¼ ì¬ì „ì†¡ ìš”ì²­ ë˜ëŠ” ë¬´ì‹œ

â€¢ ì œì£¼ë§¥ì£¼ (c4cd3b00): íŒŒì¼ ì†ìƒ
  â”” ğŸ’¡ ì¡°ì¹˜: ì›ë³¸ íŒŒì¼ ì¬ìˆ˜ì§‘

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š ìƒì„¸ í†µê³„
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì—ëŸ¬ ìœ í˜•ë³„:
â€¢ í—¤ë” ì—ëŸ¬: 75ê±´ (3ê°œ íŒŒì¼)
â€¢ ì‹œíŠ¸ ì—ëŸ¬: 45ê±´ (3ê°œ íŒŒì¼)
â€¢ íŒŒì¼ ì†ìƒ: 5ê±´ (5ê°œ íŒŒì¼)

ğŸ”— [ì „ì²´ ë¦¬í¬íŠ¸ ë³´ê¸° - BigQuery]
```

---

### 3. êµ¬í˜„ ìš°ì„ ìˆœìœ„ ì¬ì¡°ì •

#### Phase 1: í•µì‹¬ ê¸°ëŠ¥ (2-3ì¼)
- [ ] íŒŒì¼ë³„ ê·¸ë£¹í•‘ ë¡œì§
- [ ] Noise íŒì • (ê°™ì€ ë‚  ì„±ê³µ/ì‹¤íŒ¨ ë¹„êµ)
- [ ] ê³¼ê±° ì„±ê³µë¥  ê³„ì‚°
- [ ] "ì§„ì§œ ì—ëŸ¬" vs "Noise" ë¶„ë¥˜
- [ ] ê¸°ë³¸ Teams ë©”ì‹œì§€ (ê°„ë‹¨ ë²„ì „)

#### Phase 2: ì •í™•ë„ í–¥ìƒ (1-2ì¼)
- [ ] ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´ ë§¤ì¹­ ì •êµí™”
- [ ] ì—°ì† ì‹¤íŒ¨ì¼ìˆ˜ ê³„ì‚°
- [ ] ì˜í–¥ë„ ê³„ì‚° (í…Œì´ë¸” ê°œìˆ˜, íŒŒì¼ ê°œìˆ˜)
- [ ] ì¡°ì¹˜ ë°©ë²• ìë™ ì œì•ˆ

#### Phase 3: ë§í¬ ë° í¸ì˜ ê¸°ëŠ¥ (1ì¼)
- [ ] BigQuery ì½˜ì†” ë§í¬
- [ ] GCS íŒŒì¼ ë§í¬
- [ ] GitHub Config íŒŒì¼ ë§í¬
- [ ] Grafana ëŒ€ì‹œë³´ë“œ ë§í¬

#### Phase 4: (ì„ íƒ) ê³ ê¸‰ ê¸°ëŠ¥ (2-3ì¼)
- [ ] LLM ê¸°ë°˜ ì›ì¸ ë¶„ì„
- [ ] ìë™ config diff ìƒì„±
- [ ] Slack/Teams ì¸í„°ë™í‹°ë¸Œ ë²„íŠ¼ (ë¬´ì‹œ/ì¡°ì¹˜ì™„ë£Œ ë“±)

---

## ìˆ˜ì§‘ ë°©ì‹ì— ë”°ë¥¸ ì°¨ë³„í™” (2025-11-10 ì¶”ê°€ ë…¼ì˜)

### ë°°ê²½: ìˆ˜ì§‘ ë°©ì‹ì´ ë‹¤ë¦„

#### 1. **ì¼ë³„ ìˆ˜ì§‘ (RPA/Board)** - ìš°ë¦¬ê°€ ì œì–´
- **íŠ¹ì§•**: ë§¤ì¼ ì •í•´ì§„ ì‹œê°„ì— ìš°ë¦¬ ì‹œìŠ¤í…œì´ ìˆ˜ì§‘
- **íŒŒì¼ ìƒì„±**: ê·œì¹™ì  (ë§¤ì¼)
- **ì—°ì† ì‹¤íŒ¨ ì˜ë¯¸**: âœ… **ìˆìŒ**
  - 3ì¼ ì—°ì† ì‹¤íŒ¨ = Configê°€ ì˜ëª»ëœ ê²Œ ê±°ì˜ í™•ì‹¤
  - ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”

#### 2. **NonRPA (PC/Email/shared_drive)** - ê³ ê°ì´ ì œì–´
- **íŠ¹ì§•**: ê³ ê°ì‚¬ê°€ íŒŒì¼ ì˜¬ë¦´ ë•Œ ìˆ˜ì§‘
- **íŒŒì¼ ìƒì„±**: ë¶ˆê·œì¹™ (ì›” 1íšŒ, ì£¼ 1íšŒ, ëœë¤)
- **ì—°ì† ì‹¤íŒ¨ ì˜ë¯¸**: âŒ **ì—†ìŒ**
  - "3íšŒ ë°˜ë³µ ì‹¤íŒ¨" ê°™ì€ ê°œë… ë¶ˆí•„ìš”
  - **ì–´ì œ ì‹¤íŒ¨í•œ ê²ƒë§Œ ë³´ì—¬ì£¼ë©´ ë¨** (ë‹¨ìˆœ)

### êµ¬ë¶„ ê¸°ì¤€: `source_type`

```sql
source_type IN ('rpa', 'board') â†’ ì¼ë³„ ìˆ˜ì§‘ (RPA/Board)
source_type IN ('pc', 'email', 'shared_drive') â†’ ê³ ê°ì‚¬ ì—…ë¡œë“œ (NonRPA)
```

### íŒì • ë¡œì§ ë¶„ê¸°

```python
def get_failure_category(row):
    source_type = row['source_type']

    if source_type in ['rpa', 'board']:
        # ì¼ë³„ ìˆ˜ì§‘: ì—°ì† ì¼ìˆ˜ë¡œ íŒë‹¨
        if row['consecutive_fail_days'] >= 3:
            return {
                'section': 'RPA/Board ì—°ì† ì‹¤íŒ¨',
                'urgency': 'ğŸ”´ ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”',
                'description': f"ì—°ì† {row['consecutive_fail_days']}ì¼ ì‹¤íŒ¨",
                'action': 'Config ìˆ˜ì • í•„ìš” (ê±°ì˜ í™•ì‹¤)'
            }

    elif source_type in ['pc', 'email', 'shared_drive']:
        # NonRPA: ì–´ì œ ì‹¤íŒ¨í–ˆìœ¼ë©´ í‘œì‹œ
        if row['failed_yesterday']:
            return {
                'section': 'NonRPA ì–´ì œ ì‹¤íŒ¨',
                'urgency': 'âš ï¸ í™•ì¸ í•„ìš”',
                'description': f"ì–´ì œ ì‹¤íŒ¨",
                'action': 'Config í™•ì¸ ë˜ëŠ” ê³ ê°ì‚¬ íŒŒì¼ ë¬¸ì˜'
            }

    return None
```

---

## Airflow DAG ìƒíƒœ ì—°ë™ (2025-11-10 ì¶”ê°€)

### ë°°ê²½
ConverterëŠ” Airflow DAG ë‚´ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ, **DAG ìƒíƒœì— ë”°ë¼ ì–´ëŠ ë‚ ì§œì˜ ë°ì´í„°ë¥¼ ì¡°íšŒí• ì§€ ê²°ì •**í•´ì•¼ í•©ë‹ˆë‹¤.

### ë¬¸ì œ ìƒí™©
```
í˜„ì¬ ì‹œê°: 2025-11-10 08:00 (ì˜¤ì „ 8ì‹œ)
ì–´ì œ(11-09) DAG ìƒíƒœ: Running (ì•„ì§ ì‹¤í–‰ ì¤‘)

Q: ì–´ì œ(11-09) convert_job_historyë¥¼ ì¡°íšŒí•´ì•¼ í•˜ë‚˜?
A: âŒ ì•„ì§ ì™„ë£Œ ì•ˆ ë¨ â†’ ì „ì¼(11-08) ë°ì´í„°ë¥¼ ì¡°íšŒí•´ì•¼ í•¨
```

### ì°¸ê³  ì‹œìŠ¤í…œ
#### 1. `airflow_dag_monitor` - ì—…ë¬´ì¼ ê¸°ì¤€ ë¡œì§
```python
# ì˜¤ì „ 9ì‹œ ì „ì´ë©´ ì „ë‚  ê¸°ì¤€ìœ¼ë¡œ
if actual_now.hour < 9:
    reference_date = actual_now - timedelta(days=1)
else:
    reference_date = actual_now

# íœ´ì¼/ì£¼ë§ ì²˜ë¦¬
is_holiday = check_bq_holiday(reference_date)
if is_holiday:
    reference_date = get_previous_business_day(reference_date)
```

#### 2. `collect_history_checker` - DAG ìƒíƒœ í™•ì¸ ë¡œì§
```python
# ingestion_idë¡œ ìˆ˜ì§‘ ë°°ì¹˜ ì¶”ì 
latest_ingestion = get_latest_ingestion_id(customer_code, source_id)

# DAG ì‹¤í–‰ ìƒíƒœ í™•ì¸ (Airflow Metadata DB)
dag_state = check_dag_run_state(dag_id, execution_date)

if dag_state in ['running', 'failed', 'upstream_failed']:
    # ì•„ì§ ì™„ë£Œ ì•ˆ ë¨ â†’ ì´ì „ ì™„ë£Œëœ ë°°ì¹˜ ì‚¬ìš©
    target_ingestion = get_previous_completed_ingestion(customer_code, source_id)
```

---

### êµ¬í˜„ ì „ëµ

#### Phase 1: ê°„ë‹¨í•œ ì‹œê°„ ê¸°ë°˜ (ì´ˆê¸° êµ¬í˜„)
```python
def get_target_date():
    """
    ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ ë‚ ì§œ ê²°ì •
    - ì˜¤ì „ 9ì‹œ ì „: ê·¸ì €ê»˜ ë°ì´í„° (ì–´ì œ DAGê°€ ì•„ì§ ì™„ë£Œ ì•ˆ ëì„ ê°€ëŠ¥ì„±)
    - ì˜¤ì „ 9ì‹œ í›„: ì–´ì œ ë°ì´í„° (ì–´ì œ DAGê°€ ì™„ë£Œëì„ ê²ƒìœ¼ë¡œ ê°€ì •)
    """
    now = datetime.now(timezone('Asia/Seoul'))

    if now.hour < 9:
        # ì˜¤ì „ 9ì‹œ ì „ â†’ ê·¸ì €ê»˜ ë°ì´í„°
        target_date = (now - timedelta(days=2)).date()
    else:
        # ì˜¤ì „ 9ì‹œ í›„ â†’ ì–´ì œ ë°ì´í„°
        target_date = (now - timedelta(days=1)).date()

    return target_date
```

**ì¥ì **:
- êµ¬í˜„ ê°„ë‹¨
- ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì‘ë™

**ë‹¨ì **:
- ì–´ì œ DAGê°€ ëŠ¦ê²Œ ëë‚˜ê±°ë‚˜ ì‹¤íŒ¨í•˜ë©´ ë¶€ì •í™•
- ì •í™•í•œ DAG ìƒíƒœ ë°˜ì˜ ì•ˆ í•¨

---

#### Phase 2: DAG ìƒíƒœ ê¸°ë°˜ (ì •í™•í•œ êµ¬í˜„) â­ ì¶”ì²œ
```python
from google.cloud import bigquery
from airflow.models import DagRun
from datetime import datetime, timedelta

def get_target_date_with_dag_check(customer_code: str):
    """
    Airflow DAG ìƒíƒœë¥¼ í™•ì¸í•˜ì—¬ ì¡°íšŒ ëŒ€ìƒ ë‚ ì§œ ê²°ì •

    ë¡œì§:
    1. ì–´ì œ ë‚ ì§œ ê³„ì‚°
    2. í•´ë‹¹ ê³ ê°ì‚¬ì˜ ì–´ì œ DAG Run ìƒíƒœ í™•ì¸
    3. Running/Failedì´ë©´ ì „ì¼ ë°ì´í„° ì‚¬ìš©
    4. Successì´ë©´ ì–´ì œ ë°ì´í„° ì‚¬ìš©
    """
    now = datetime.now(timezone('Asia/Seoul'))
    yesterday = (now - timedelta(days=1)).date()

    # ê³ ê°ì‚¬ DAG ID (ì˜ˆ: c0159c00)
    dag_id = customer_code

    # Airflow Metadataì—ì„œ DAG Run ìƒíƒœ í™•ì¸
    dag_state = check_dag_run_state(dag_id, yesterday)

    if dag_state == 'success':
        # ì–´ì œ DAG ì„±ê³µ â†’ ì–´ì œ ë°ì´í„° ì‚¬ìš©
        return yesterday
    elif dag_state in ['running', 'queued', 'failed', 'upstream_failed']:
        # ì–´ì œ DAG ë¯¸ì™„ë£Œ/ì‹¤íŒ¨ â†’ ì „ì¼ ì™„ë£Œ ë°ì´í„° ì°¾ê¸°
        return get_last_successful_date(dag_id, yesterday)
    else:
        # DAG Run ì—†ìŒ â†’ ê¸°ë³¸ê°’ (ê·¸ì €ê»˜)
        return (now - timedelta(days=2)).date()


def check_dag_run_state(dag_id: str, execution_date: date) -> str:
    """
    Airflow Metadata DBì—ì„œ DAG Run ìƒíƒœ ì¡°íšŒ

    ë°©ë²• 1: Airflow API ì‚¬ìš© (ì¶”ì²œ)
    ë°©ë²• 2: Metadata DB ì§ì ‘ ì¡°íšŒ
    """
    from airflow.api.client.local_client import Client

    try:
        client = Client(None, None)
        dag_runs = client.get_dag_runs(dag_id, execution_date)

        if dag_runs:
            latest_run = dag_runs[-1]
            return latest_run.state  # 'success', 'running', 'failed' ë“±
        else:
            return None  # DAG Run ì—†ìŒ
    except Exception as e:
        logger.error(f"Failed to check DAG state: {e}")
        return None


def get_last_successful_date(dag_id: str, before_date: date) -> date:
    """
    íŠ¹ì • ë‚ ì§œ ì´ì „ì— ì„±ê³µí•œ ë§ˆì§€ë§‰ DAG Run ë‚ ì§œ ì°¾ê¸°
    """
    from airflow.models import DagRun

    # before_date ì´ì „ 7ì¼ê°„ ê²€ìƒ‰
    for i in range(1, 8):
        check_date = before_date - timedelta(days=i)
        state = check_dag_run_state(dag_id, check_date)

        if state == 'success':
            return check_date

    # 7ì¼ ì•ˆì— ì„±ê³µ ì—†ìŒ â†’ ì¼ë‹¨ ê·¸ì €ê»˜ ë°˜í™˜
    return before_date - timedelta(days=1)
```

**ì¿¼ë¦¬ ì˜ˆì‹œ**:
```sql
-- ê³ ê°ì‚¬ë³„ DAG ìƒíƒœì— ë”°ë¼ ë‹¤ë¥¸ ë‚ ì§œ ì¡°íšŒ
WITH target_dates AS (
  SELECT
    'c0159c00' as customer_code,
    -- DAG ì„±ê³µ: ì–´ì œ, ì‹¤íŒ¨/ì‹¤í–‰ì¤‘: ì „ì¼ ì„±ê³µ ë‚ ì§œ
    CASE
      WHEN check_dag_state('c0159c00', CURRENT_DATE() - 1) = 'success'
        THEN CURRENT_DATE() - 1
      ELSE get_last_success_date('c0159c00', CURRENT_DATE() - 1)
    END as target_date
  UNION ALL
  SELECT 'c2026600', ...
  -- ëª¨ë“  ê³ ê°ì‚¬
)

SELECT
  f.*
FROM convert_job_history f
INNER JOIN target_dates t
  ON f.customer_code = t.customer_code
  AND DATE(f.created_at, 'Asia/Seoul') = t.target_date
WHERE f.status = 'fail'
```

---

#### Phase 3: ingestion_id ê¸°ë°˜ (collect_history_checker ë°©ì‹)
```python
def get_target_ingestion_id(customer_code: str, source_id: str):
    """
    ê°€ì¥ ìµœê·¼ ì™„ë£Œëœ ingestion_id ì°¾ê¸°

    collect_history_checkerì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹:
    1. filter_job_historyì—ì„œ ìµœì‹  ingestion_id ì¡°íšŒ
    2. í•´ë‹¹ ingestion_idì˜ DAG ìƒíƒœ í™•ì¸
    3. ì™„ë£Œ ì•ˆ ëìœ¼ë©´ ì´ì „ ingestion_id ì‚¬ìš©
    """
    bq_client = bigquery.Client()

    # ìµœê·¼ 7ì¼ê°„ì˜ ingestion_id ì¡°íšŒ (ë‚´ë¦¼ì°¨ìˆœ)
    query = f"""
    SELECT DISTINCT
      ingestion_id,
      DATE(created_at, 'Asia/Seoul') as date,
      COUNT(*) as record_count
    FROM `{PROJECT}.dashboard.convert_job_history`
    WHERE customer_code = '{customer_code}'
      AND created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
    GROUP BY ingestion_id, date
    ORDER BY ingestion_id DESC
    LIMIT 10
    """

    results = bq_client.query(query).result()

    for row in results:
        ingestion_id = row.ingestion_id
        date = row.date

        # í•´ë‹¹ ë‚ ì§œì˜ DAG ìƒíƒœ í™•ì¸
        dag_state = check_dag_run_state(customer_code, date)

        if dag_state == 'success':
            # ì™„ë£Œëœ ë°°ì¹˜ ë°œê²¬
            return ingestion_id, date

    # ì™„ë£Œëœ ë°°ì¹˜ ì—†ìŒ â†’ ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì‚¬ìš©
    return None, None
```

**ì¥ì **:
- ê°€ì¥ ì •í™• (ì‹¤ì œ ì™„ë£Œëœ ë°ì´í„°ë§Œ ì‚¬ìš©)
- DAG ì¬ì‹¤í–‰, ìŠ¤ì¼€ì¤„ ë³€ê²½ ë“±ì—ë„ ëŒ€ì‘

**ë‹¨ì **:
- ë³µì¡ë„ ë†’ìŒ
- BigQuery ì¿¼ë¦¬ ì¶”ê°€ ë°œìƒ

---

### ìµœì¢… ì¶”ì²œ êµ¬í˜„

#### í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ (Phase 1 + Phase 2)
```python
def get_monitoring_target_date(customer_code: str = None):
    """
    Converter ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ ë‚ ì§œ ê²°ì •

    ìš°ì„ ìˆœìœ„:
    1. ê³ ê°ì‚¬ë³„ DAG ìƒíƒœ í™•ì¸ (ê°€ëŠ¥í•˜ë©´)
    2. ì¼ë°˜ ê·œì¹™: ì˜¤ì „ 9ì‹œ ê¸°ì¤€
    """
    now = datetime.now(timezone('Asia/Seoul'))
    yesterday = (now - timedelta(days=1)).date()

    # íŠ¹ì • ê³ ê°ì‚¬ ì¡°íšŒ ì‹œ DAG ìƒíƒœ í™•ì¸
    if customer_code:
        dag_state = check_dag_run_state(customer_code, yesterday)

        if dag_state == 'success':
            return yesterday
        elif dag_state in ['running', 'failed']:
            # ì‹¤íŒ¨/ì‹¤í–‰ì¤‘ â†’ ì „ì¼ ì„±ê³µ ë°ì´í„°
            return get_last_successful_date(customer_code, yesterday)

    # ì „ì²´ ëª¨ë‹ˆí„°ë§ ì‹œ ê°„ë‹¨í•œ ì‹œê°„ ê¸°ì¤€
    if now.hour < 9:
        # ì˜¤ì „ 9ì‹œ ì „ â†’ ê·¸ì €ê»˜ (ì•ˆì „í•˜ê²Œ)
        return (now - timedelta(days=2)).date()
    else:
        # ì˜¤ì „ 9ì‹œ í›„ â†’ ì–´ì œ
        return yesterday


# ì‚¬ìš© ì˜ˆì‹œ
def generate_converter_report():
    """ì „ì²´ ê³ ê°ì‚¬ ë¦¬í¬íŠ¸"""
    # ê¸°ë³¸ ëŒ€ìƒ ë‚ ì§œ (ì‹œê°„ ê¸°ë°˜)
    default_target = get_monitoring_target_date()

    # ê³ ê°ì‚¬ë³„ë¡œ DAG ìƒíƒœ í™•ì¸í•˜ì—¬ ê°œë³„ ë‚ ì§œ ê²°ì •
    customers = get_all_customers()

    all_failures = []
    for customer in customers:
        # ê³ ê°ì‚¬ë³„ ìµœì  ë‚ ì§œ
        target_date = get_monitoring_target_date(customer['customer_code'])

        # í•´ë‹¹ ë‚ ì§œ ë°ì´í„° ì¡°íšŒ
        failures = fetch_failures_from_bq(
            customer_code=customer['customer_code'],
            target_date=target_date
        )
        all_failures.extend(failures)

    # ë‚˜ë¨¸ì§€ ì²˜ë¦¬...
```

---

### ì£¼ì˜ì‚¬í•­

#### 1. Airflow Metadata ì ‘ê·¼
- **GCP Composer**: Airflow API ë˜ëŠ” Postgres Metadata DB ì ‘ê·¼ í•„ìš”
- **ê¶Œí•œ ì„¤ì •**: Cloud Functionì—ì„œ Composer í™˜ê²½ ì ‘ê·¼ ê¶Œí•œ
- **ëŒ€ì•ˆ**: BigQueryì— DAG ìƒíƒœ ë¡œê·¸ í…Œì´ë¸” ë³„ë„ ê´€ë¦¬

#### 2. íœ´ì¼/ì£¼ë§ ì²˜ë¦¬
```python
def get_monitoring_target_date_with_holiday():
    """
    íœ´ì¼/ì£¼ë§ ê³ ë ¤
    - ì›”ìš”ì¼ ì˜¤ì „: ê¸ˆìš”ì¼ ë°ì´í„° (ì£¼ë§ ê±´ë„ˆë›°ê¸°)
    - íœ´ì¼ ë‹¤ìŒë‚ : íœ´ì¼ ì „ ì˜ì—…ì¼
    """
    now = datetime.now(timezone('Asia/Seoul'))
    target = get_monitoring_target_date()

    # BigQuery íœ´ì¼ í…Œì´ë¸” í™•ì¸
    if is_holiday(target):
        return get_previous_business_day(target)

    return target
```

#### 3. ì—ëŸ¬ í•¸ë“¤ë§
```python
try:
    target_date = get_monitoring_target_date(customer_code)
except AirflowException as e:
    logger.warning(f"Failed to check DAG state: {e}")
    # Fallback: ì‹œê°„ ê¸°ë°˜
    target_date = get_monitoring_target_date(customer_code=None)
```

---

### ë©”ì‹œì§€ì— í‘œì‹œ
```
ğŸ“Š Converter ì‹¤íŒ¨ ë¦¬í¬íŠ¸ - 2025-11-10
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì¡°íšŒ ê¸°ì¤€ì¼: 2025-11-09 (ì–´ì œ)
â€» ì¼ë¶€ ê³ ê°ì‚¬ëŠ” DAG ë¯¸ì™„ë£Œë¡œ 2025-11-08 ë°ì´í„° ì¡°íšŒ

ì „ì²´: 1,357ê±´ | âœ… ì„±ê³µ: 1,232ê±´ | âŒ ì‹¤íŒ¨: 125ê±´
...
```

---

## Firestore ì—°ë™: env='ops' ê³ ê°ì‚¬ë§Œ ëª¨ë‹ˆí„°ë§ (2025-11-10 ì¶”ê°€)

### ë°°ê²½
- Firestore `sources` collectionì— ê³ ê°ì‚¬ë³„ ì„¤ì • ì €ì¥
- `env` í•„ë“œë¡œ í™˜ê²½ êµ¬ë¶„: `'ops'` (ìš´ì˜), `'dev'` (ê°œë°œ), `'test'` (í…ŒìŠ¤íŠ¸)
- **Airflow DAGëŠ” `env='ops'`ì¸ ê³ ê°ì‚¬ë§Œ ì‹¤í–‰**
- ë”°ë¼ì„œ ëª¨ë‹ˆí„°ë§ë„ ìš´ì˜ í™˜ê²½ë§Œ ëŒ€ìƒìœ¼ë¡œ í•´ì•¼ í•¨

### ë¬¸ì œ
```sql
-- âŒ ì˜ëª»ëœ ì¿¼ë¦¬: ëª¨ë“  ê³ ê°ì‚¬ ì¡°íšŒ
SELECT *
FROM `hyperlounge-dev.dashboard.convert_job_history`
WHERE DATE(created_at) = '2025-11-09'
  AND status = 'fail'

-- ë¬¸ì œì : dev/test í™˜ê²½ ë°ì´í„°ê¹Œì§€ í¬í•¨ â†’ ë…¸ì´ì¦ˆ
```

### í•´ê²° ë°©ì•ˆ

#### 1. Firestoreì—ì„œ í™œì„± ê³ ê°ì‚¬ ì¡°íšŒ
```python
from google.cloud import firestore

def get_active_customers():
    """
    Firestoreì—ì„œ env='ops'ì¸ ìš´ì˜ ê³ ê°ì‚¬ ëª©ë¡ ì¡°íšŒ

    Returns:
        list: ìš´ì˜ ê³ ê°ì‚¬ ì½”ë“œ ëª©ë¡ ['c0159c00', 'c2026600', ...]
    """
    db = firestore.Client()

    active_customers = set()

    # sources collectionì—ì„œ env='ops'ì¸ ë¬¸ì„œë§Œ
    docs = db.collection('sources').where('env', '==', 'ops').stream()

    for doc in docs:
        data = doc.to_dict()
        customer_code = data.get('customer_code')

        if customer_code:
            active_customers.add(customer_code)

    return sorted(list(active_customers))


# ì‚¬ìš© ì˜ˆì‹œ
active_customers = get_active_customers()
print(f"ìš´ì˜ ê³ ê°ì‚¬: {len(active_customers)}ê°œ")
# ì¶œë ¥: ìš´ì˜ ê³ ê°ì‚¬: 45ê°œ
# ['c0159c00', 'c2026600', 'c7005b01', ...]
```

#### 2. BigQuery ì¿¼ë¦¬ì— ì ìš©
```sql
-- âœ… ì˜¬ë°”ë¥¸ ì¿¼ë¦¬: env='ops'ì¸ ê³ ê°ì‚¬ë§Œ
WITH active_customers AS (
  -- Pythonì—ì„œ Firestore ì¡°íšŒ í›„ IN ì ˆë¡œ ì „ë‹¬
  SELECT customer_code
  FROM UNNEST(@active_customer_codes) as customer_code
),

failures AS (
  SELECT
    h.*
  FROM `hyperlounge-dev.dashboard.convert_job_history` h
  INNER JOIN active_customers a
    ON h.customer_code = a.customer_code
  WHERE DATE(h.created_at, 'Asia/Seoul') = '2025-11-09'
    AND h.status = 'fail'
)

SELECT * FROM failures
ORDER BY customer_code, created_at
```

#### 3. Pythonì—ì„œ íŒŒë¼ë¯¸í„° ë°”ì¸ë”©
```python
from google.cloud import bigquery

def fetch_failures_from_bq(target_date: str):
    """
    env='ops' ê³ ê°ì‚¬ì˜ ì‹¤íŒ¨ ê±´ë§Œ ì¡°íšŒ
    """
    # 1. Firestoreì—ì„œ í™œì„± ê³ ê°ì‚¬ ì¡°íšŒ
    active_customers = get_active_customers()

    if not active_customers:
        logger.warning("í™œì„± ê³ ê°ì‚¬ ì—†ìŒ (env='ops')")
        return []

    # 2. BigQuery ì¿¼ë¦¬ ì‹¤í–‰
    client = bigquery.Client()

    query = """
    SELECT
      customer_code,
      customer_name,
      source_type,
      source_id,
      filter_condition_id,
      target_table_name,
      gcs_src,
      ingestion_id,
      error_message,
      created_at
    FROM `hyperlounge-dev.dashboard.convert_job_history`
    WHERE customer_code IN UNNEST(@active_customers)
      AND DATE(created_at, 'Asia/Seoul') = @target_date
      AND status = 'fail'
    ORDER BY customer_code, created_at
    """

    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ArrayQueryParameter(
                "active_customers",
                "STRING",
                active_customers
            ),
            bigquery.ScalarQueryParameter(
                "target_date",
                "DATE",
                target_date
            ),
        ]
    )

    results = client.query(query, job_config=job_config).result()

    failures = []
    for row in results:
        failures.append(dict(row))

    logger.info(f"í™œì„± ê³ ê°ì‚¬ {len(active_customers)}ê°œ ì¤‘ ì‹¤íŒ¨: {len(failures)}ê±´")
    return failures
```

### ìºì‹± ìµœì í™”

ë§¤ë²ˆ Firestore ì¡°íšŒëŠ” ë¹„íš¨ìœ¨ì ì´ë¯€ë¡œ ìºì‹± ì ìš©:

```python
import time
from functools import lru_cache

# ë°©ë²• 1: ë©”ëª¨ë¦¬ ìºì‹œ (ê°„ë‹¨)
@lru_cache(maxsize=1)
def get_active_customers_cached():
    """
    Firestore ì¡°íšŒ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ìºì‹œ
    Cloud Function ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš© ì‹œ íš¨ê³¼ì 
    """
    return get_active_customers()


# ë°©ë²• 2: TTL ìºì‹œ (ì‹œê°„ ì œí•œ)
_cache = {
    'active_customers': None,
    'last_updated': 0,
    'ttl': 3600  # 1ì‹œê°„
}

def get_active_customers_with_ttl():
    """
    1ì‹œê°„ë§ˆë‹¤ Firestore ì¬ì¡°íšŒ
    """
    now = time.time()

    if (_cache['active_customers'] is None or
        now - _cache['last_updated'] > _cache['ttl']):

        logger.info("Firestoreì—ì„œ í™œì„± ê³ ê°ì‚¬ ì¬ì¡°íšŒ")
        _cache['active_customers'] = get_active_customers()
        _cache['last_updated'] = now

    return _cache['active_customers']


# ë°©ë²• 3: í™˜ê²½ ë³€ìˆ˜ (ì •ì )
# deploy ì‹œì ì— ê³ ì •
# â†’ ê³ ê°ì‚¬ ì¶”ê°€/ì œê±° ì‹œ ì¬ë°°í¬ í•„ìš”
# â†’ ë¹„ì¶”ì²œ
```

### ëŒ€ì•ˆ: BigQueryì— ê³ ê°ì‚¬ ë©”íƒ€ í…Œì´ë¸”

Firestore ëŒ€ì‹  BigQueryì— ê³ ê°ì‚¬ ë©”íƒ€ í…Œì´ë¸” ê´€ë¦¬:

```sql
-- dashboard.customer_metadata í…Œì´ë¸” ìƒì„±
CREATE TABLE `hyperlounge-dev.dashboard.customer_metadata` (
  customer_code STRING NOT NULL,
  customer_name STRING,
  env STRING NOT NULL,  -- 'ops', 'dev', 'test'
  is_active BOOL DEFAULT TRUE,
  created_at TIMESTAMP,
  updated_at TIMESTAMP
);

-- ì¿¼ë¦¬ ê°„ì†Œí™”
WITH failures AS (
  SELECT h.*
  FROM `hyperlounge-dev.dashboard.convert_job_history` h
  INNER JOIN `hyperlounge-dev.dashboard.customer_metadata` m
    ON h.customer_code = m.customer_code
  WHERE m.env = 'ops'
    AND m.is_active = TRUE
    AND DATE(h.created_at, 'Asia/Seoul') = '2025-11-09'
    AND h.status = 'fail'
)
SELECT * FROM failures;
```

**ì¥ì **:
- Firestore ì˜ì¡´ì„± ì œê±°
- ì¿¼ë¦¬ ê°„ì†Œí™”
- BigQuery ë„¤ì´í‹°ë¸Œ ì¡°ì¸ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ

**ë‹¨ì **:
- ë©”íƒ€ í…Œì´ë¸” ë™ê¸°í™” í•„ìš” (Firestore â†’ BigQuery)
- ì¶”ê°€ í…Œì´ë¸” ê´€ë¦¬

### ê¸°ì¡´ êµ¬í˜„ ì°¸ê³ 

#### 1. `history_checker/clients/firestore_client.py` â­ ì¶”ì²œ
```python
# ì‹¤ì œ êµ¬í˜„ëœ ë¡œì§ - env != 'dev'ì¸ ê²ƒë§Œ í•„í„°ë§
class FirestoreClient:
    def __init__(self):
        self.db = firestore.Client()
        self.company_collection = self.db.collection("company").document("version").collection("v1.0")

    def get_crawl_actions_tree(self):
        """env != 'dev'ì¸ ì†ŒìŠ¤ë§Œ ì¡°íšŒ"""
        crawl_actions_tree = {}

        customer_docs = self.company_collection.list_documents()

        for customer_doc in customer_docs:
            customer_code = customer_doc.id

            # source_metasì—ì„œ env í™•ì¸
            source_metas_ref = customer_doc.collection("source_metas")
            source_metas = {doc.id: doc.to_dict() for doc in source_metas_ref.stream()}

            for source_id, source_meta_data in source_metas.items():
                env = source_meta_data.get("env", "dev")
                source_type = source_meta_data.get("source_type", "unknown")

                # dev í™˜ê²½ ì œì™¸ (= ops, test ë“± í¬í•¨)
                if env == "dev" or source_type not in ["rpa", "board"]:
                    continue

                # ì—¬ê¸°ì„œ ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬...
```

**íŠ¹ì§•**:
- `company/version/v1.0/{customer_code}/source_metas` êµ¬ì¡°
- `env == "dev"`ë¥¼ **ì œì™¸** (= ops, test í¬í•¨)
- source_typeë„ ê°™ì´ í•„í„°ë§

#### 2. `customer_env_checker/clients/firestore_client.py`
```python
# ê³ ê°ì‚¬ë³„ env ë¶„ë¥˜ (ops > dev > other ìš°ì„ ìˆœìœ„)
def classify_customers_by_env(self):
    """
    ê³ ê°ì‚¬ì˜ ëª¨ë“  sourceë¥¼ í™•ì¸í•˜ì—¬ env ë¶„ë¥˜
    - í•˜ë‚˜ë¼ë„ 'ops'ë©´ â†’ ops ê³ ê°ì‚¬
    - ops ì—†ê³  'dev' ìˆìœ¼ë©´ â†’ dev ê³ ê°ì‚¬
    - ë‘˜ ë‹¤ ì—†ìœ¼ë©´ â†’ other
    """
    classified_customers = {
        "ops": [],
        "dev": [],
        "other": []
    }

    customer_docs = self.company_collection.stream()

    for customer_doc in customer_docs:
        customer_code = customer_doc.id
        final_env_status = "other"

        source_metas_ref = customer_doc.reference.collection("source_metas")
        for source_meta_doc in source_metas_ref.stream():
            source_env = source_meta_doc.to_dict().get("env", "").lower()

            if source_env == "ops":
                final_env_status = "ops"
                break  # ops ë°œê²¬ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
            elif source_env == "dev":
                if final_env_status != "ops":
                    final_env_status = "dev"

        classified_customers[final_env_status].append({
            "code": customer_code,
            "name": customer_data.get("name", "ì´ë¦„ ì—†ìŒ")
        })

    return classified_customers
```

#### 3. `airflow_dag_monitor/clients/airflow_client.py`
```python
# Airflow APIë¡œ DAG ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (Firestore ì‚¬ìš© ì•ˆ í•¨)
def get_all_customer_dags(self):
    """
    Airflow APIì—ì„œ í™œì„± ê³ ê°ì‚¬ DAG ì¡°íšŒ
    - is_paused = False (í™œì„± ìƒíƒœ)
    - "Develop" íƒœê·¸ ì—†ìŒ (ìš´ì˜ í™˜ê²½)
    """
    dags_data = self.make_request(endpoint="dags?limit=1000")

    customer_dags = []
    for dag in dags_data["dags"]:
        # ì¤‘ì§€ëœ DAG ì œì™¸
        if dag.get('is_paused', True):
            continue

        # "Develop" íƒœê·¸ ìˆëŠ” DAG ì œì™¸
        tags = dag.get("tags", [])
        is_develop_dag = any("Develop" in t.get("name", "") for t in tags)
        if is_develop_dag:
            continue

        # ê³ ê°ì‚¬ ì´ë¦„ íƒœê·¸ ì¶”ì¶œ
        tag_info = self.extract_from_tags(tags)
        if tag_info.get("customer_name"):
            dag['customer_name'] = tag_info['customer_name']
            customer_dags.append(dag)

    return customer_dags
```

**íŠ¹ì§•**:
- Firestore ì˜ì¡´ì„± ì—†ìŒ
- Airflow Metadataì—ì„œ ì§ì ‘ ì¡°íšŒ
- DAG íƒœê·¸ë¡œ í™˜ê²½ êµ¬ë¶„ ("Develop" íƒœê·¸ ìœ ë¬´)

---

### Converter ëª¨ë‹ˆí„°ë§ ì ìš© ë°©ì•ˆ

#### ì˜µì…˜ A: history_checker ë°©ì‹ (Firestore) â­ ì¶”ì²œ
```python
def get_active_customers_for_converter():
    """
    history_checker ë¡œì§ ì¬ì‚¬ìš©
    env != 'dev'ì¸ ê³ ê°ì‚¬ë§Œ (= ops, test í¬í•¨)
    """
    db = firestore.Client()
    company_collection = db.collection("company").document("version").collection("v1.0")

    active_customers = set()
    customer_docs = company_collection.list_documents()

    for customer_doc in customer_docs:
        customer_code = customer_doc.id

        # source_metasì—ì„œ í•˜ë‚˜ë¼ë„ env != 'dev'ì´ë©´ í¬í•¨
        source_metas_ref = customer_doc.collection("source_metas")
        for source_meta_doc in source_metas_ref.stream():
            env = source_meta_doc.to_dict().get("env", "dev")

            if env != "dev":  # ops, test ë“± í¬í•¨
                active_customers.add(customer_code)
                break  # í•˜ë‚˜ë§Œ ë°œê²¬í•´ë„ ì¶©ë¶„

    return sorted(list(active_customers))
```

**ì¥ì **:
- history_checkerì™€ ë™ì¼í•œ ë¡œì§ (ì¼ê´€ì„±)
- opsë¿ ì•„ë‹ˆë¼ test í™˜ê²½ë„ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥

**ë‹¨ì **:
- test í™˜ê²½ê¹Œì§€ í¬í•¨ (ë…¸ì´ì¦ˆ ê°€ëŠ¥ì„±)

#### ì˜µì…˜ B: opsë§Œ ì—„ê²©í•˜ê²Œ í•„í„°ë§
```python
def get_ops_only_customers():
    """
    customer_env_checker ë¡œì§ í™œìš©
    í•˜ë‚˜ë¼ë„ env='ops'ì¸ ê³ ê°ì‚¬ë§Œ í¬í•¨
    """
    db = firestore.Client()
    company_collection = db.collection("company").document("version").collection("v1.0")

    ops_customers = set()
    customer_docs = company_collection.list_documents()

    for customer_doc in customer_docs:
        customer_code = customer_doc.id

        source_metas_ref = customer_doc.collection("source_metas")
        for source_meta_doc in source_metas_ref.stream():
            env = source_meta_doc.to_dict().get("env", "").lower()

            if env == "ops":
                ops_customers.add(customer_code)
                break

    return sorted(list(ops_customers))
```

**ì¥ì **:
- ìš´ì˜ í™˜ê²½ë§Œ ì •í™•í•˜ê²Œ í•„í„°ë§
- ë…¸ì´ì¦ˆ ìµœì†Œí™”

**ë‹¨ì **:
- test í™˜ê²½ ëˆ„ë½ (í•„ìš”ì‹œ ìˆ˜ë™ í™•ì¸ í•„ìš”)

#### ì˜µì…˜ C: Airflow DAG ê¸°ë°˜ (API)
```python
def get_customers_from_airflow():
    """
    airflow_dag_monitor ë°©ì‹
    í™œì„± DAG (is_paused=False, Develop íƒœê·¸ ì—†ìŒ)
    """
    from airflow_dag_monitor.clients.airflow_client import AirflowClient

    airflow_client = AirflowClient(AIRFLOW_API_URL)
    customer_dags = airflow_client.get_all_customer_dags()

    # dag_idì—ì„œ customer_code ì¶”ì¶œ (ì˜ˆ: c0159c00-...)
    active_customers = set()
    for dag in customer_dags:
        dag_id = dag['dag_id']
        customer_code = dag_id.split('-')[0]  # c0159c00
        active_customers.add(customer_code)

    return sorted(list(active_customers))
```

**ì¥ì **:
- Firestore ì˜ì¡´ì„± ì—†ìŒ
- ì‹¤ì œ ì‹¤í–‰ ì¤‘ì¸ DAG ê¸°ì¤€ (ê°€ì¥ ì •í™•)

**ë‹¨ì **:
- Airflow API í˜¸ì¶œ í•„ìš” (ë„¤íŠ¸ì›Œí¬ ì˜ì¡´)
- Airflow êµ¬ì¡° ë³€ê²½ ì‹œ ì˜í–¥

---

### ìµœì¢… ì¶”ì²œ

**Phase 1 (ì´ˆê¸°)**: ì˜µì…˜ A (history_checker ë°©ì‹)
- `env != 'dev'` í•„í„°ë§ (ops, test í¬í•¨)
- ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©ìœ¼ë¡œ ë¹ ë¥¸ êµ¬í˜„
- TTL ìºì‹± ì ìš© (1ì‹œê°„)

**Phase 2 (ì•ˆì •í™”)**: ì˜µì…˜ B (opsë§Œ)
- `env == 'ops'` ì—„ê²© í•„í„°ë§
- ìš´ì˜ ë°ì´í„°ë§Œ ëª¨ë‹ˆí„°ë§
- í•„ìš”ì‹œ ê³ ê°ì‚¬ë³„ ì„¤ì •ìœ¼ë¡œ test í™˜ê²½ í¬í•¨ ì˜µì…˜

**Phase 3 (ìµœì í™”)**: BigQuery ë©”íƒ€ í…Œì´ë¸”
- Firestore â†’ BigQuery ë™ê¸°í™”
- ì¿¼ë¦¬ ë‹¨ìˆœí™” ë° ì„±ëŠ¥ í–¥ìƒ

---

## BigQuery í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸ (2025-11-10 ì¶”ê°€)

### ì‹¤ì œ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ
`hyperlounge-dev.dashboard.convert_job_history` í…Œì´ë¸” (íŒŒí‹°ì…˜ í…Œì´ë¸”)

**ì „ì²´ ìŠ¤í‚¤ë§ˆ:**

| í•„ë“œëª… | íƒ€ì… | ëª¨ë“œ | ì„¤ëª… |
|--------|------|------|------|
| hostname | STRING | REQUIRED | Airflow DAGê°€ ìˆ˜í–‰ëœ hostname |
| run_id | STRING | REQUIRED | Airflow DAG Run Id |
| task_id | STRING | REQUIRED | Airflow Task Id |
| customer_code | STRING | REQUIRED | ê³ ê°ì‚¬ ì½”ë“œ |
| customer_name | STRING | REQUIRED | ê³ ê°ì‚¬ ì´ë¦„ |
| source_type | STRING | REQUIRED | ì†ŒìŠ¤ íƒ€ì… (rpa/board/pc/email/shared_drive) |
| source_id | STRING | REQUIRED | ì†ŒìŠ¤ ID |
| filter_condition_id | STRING | REQUIRED | Filter condition document ID |
| convert_config_id | STRING | REQUIRED | Convert config ID |
| **status** | STRING | REQUIRED | âš ï¸ **ì‹¤íŒ¨ ì—¬ë¶€: "success" or "fail"** |
| error_message | STRING | NULLABLE | ì—ëŸ¬ ë©”ì‹œì§€ |
| ingestion_id | TIMESTAMP | REQUIRED | Timestamp í˜•ì‹ì˜ ingestion ID |
| **gcs_path** | STRING | NULLABLE | âš ï¸ **GCS íŒŒì¼ ê²½ë¡œ** |
| created_at | TIMESTAMP | REQUIRED | ë¡œê·¸ ì‘ì„± ì‹œê°„ |
| convert_item_names | RECORD (REPEATED) | REPEATED | convert result item info list |
| **env** | STRING | NULLABLE | âš ï¸ **í™˜ê²½ êµ¬ë¶„ (ops/dev/test), ìš´ì˜íŒ€ í™•ì¸ìš©** |
| job_names | RECORD (REPEATED) | REPEATED | convert run job info list |

**ì¤‘ìš”í•œ ì°¨ì´ì  (ë¬¸ì„œ v2 ìŠ¤í‚¤ë§ˆì™€ ì‹¤ì œ ì°¨ì´):**
- âŒ `result` â†’ âœ… `status`
- âŒ `gcs_src` â†’ âœ… `gcs_path`
- âŒ `target_table_name` â†’ ì‹¤ì œ í…Œì´ë¸”ì—ëŠ” ì—†ìŒ (convert_item_names RECORDì— í¬í•¨)
- âœ… `env` í•„ë“œ ìˆìŒ â†’ **ëª¨ë‹ˆí„°ë§ì—ì„œ env != 'dev' í•„í„°ë§ ê°€ëŠ¥!**

### ê°œë°œìš© ì¿¼ë¦¬ ì˜ˆì‹œ

#### 1. íŠ¹ì • ë‚ ì§œ ì „ì²´ ì‹¤íŒ¨ ê±´ í™•ì¸
```sql
-- 2025-11-07 ì „ì²´ ì‹¤íŒ¨ ê±´ìˆ˜ í™•ì¸ (íŒŒì¼ë³„ ê·¸ë£¹í•‘)
SELECT
  customer_code,
  customer_name,
  source_type,
  COUNT(*) as fail_count,
  COUNT(DISTINCT gcs_path) as fail_file_count  -- íŒŒì¼ ê°œìˆ˜
FROM
  `hyperlounge-dev.dashboard.convert_job_history`
WHERE
  DATE(created_at, 'Asia/Seoul') = '2025-11-07'
  AND status = 'fail'
GROUP BY customer_code, customer_name, source_type
ORDER BY fail_count DESC
```

#### 2. ê³ ê°ì‚¬ë³„ ì—ëŸ¬ ë©”ì‹œì§€ ìƒ˜í”Œ í™•ì¸
```sql
-- íŠ¹ì • ê³ ê°ì‚¬ì˜ ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸
SELECT
  customer_code,
  source_type,
  convert_config_id,
  error_message,
  gcs_path,
  created_at
FROM
  `hyperlounge-dev.dashboard.convert_job_history`
WHERE
  customer_code = 'c0159c00'
  AND DATE(created_at, 'Asia/Seoul') = '2025-11-07'
  AND status = 'fail'
ORDER BY created_at DESC
LIMIT 20
```

#### 3. ì—ëŸ¬ ìœ í˜•ë³„ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸
```sql
-- ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´ í™•ì¸ (ë¶„ë¥˜ ë¡œì§ í…ŒìŠ¤íŠ¸ìš©)
SELECT
  CASE
    WHEN REGEXP_CONTAINS(error_message, r"(?i)Header .* not found") THEN 'í—¤ë” ì—ëŸ¬'
    WHEN REGEXP_CONTAINS(error_message, r"(?i)Sheet .* not found") THEN 'ì‹œíŠ¸ ì—ëŸ¬'
    WHEN REGEXP_CONTAINS(error_message, r"(?i)timeout") THEN 'Timeout'
    WHEN REGEXP_CONTAINS(error_message, r"(?i)usecols.*out of bounds") THEN 'ì»¬ëŸ¼ ë²”ìœ„'
    ELSE 'ê¸°íƒ€'
  END as error_type,
  COUNT(*) as count,
  ARRAY_AGG(DISTINCT error_message LIMIT 3) as sample_messages
FROM
  `hyperlounge-dev.dashboard.convert_job_history`
WHERE
  DATE(created_at, 'Asia/Seoul') = '2025-11-07'
  AND status = 'fail'
GROUP BY error_type
ORDER BY count DESC
```

#### 4. source_typeë³„ ë¶„í¬ í™•ì¸ (RPA/Board vs NonRPA)
```sql
SELECT
  CASE
    WHEN source_type IN ('rpa', 'board') THEN 'RPA/Board'
    WHEN source_type IN ('pc', 'email', 'shared_drive') THEN 'NonRPA'
    ELSE 'Unknown'
  END as source_category,
  source_type,
  COUNT(*) as fail_count,
  COUNT(DISTINCT customer_code) as customer_count
FROM
  `hyperlounge-dev.dashboard.convert_job_history`
WHERE
  DATE(created_at, 'Asia/Seoul') = '2025-11-07'
  AND status = 'fail'
GROUP BY source_category, source_type
ORDER BY fail_count DESC
```

#### 5. íŒŒì¼ë³„ ê·¸ë£¹í•‘ (ì‹¤ì œ ëª¨ë‹ˆí„°ë§ ë¡œì§) â­ í•µì‹¬
```sql
-- íŒŒì¼ë³„ë¡œ ê·¸ë£¹í•‘í•´ì„œ ì‹¤ì œ ëŒ€ì‘ ê±´ìˆ˜ í™•ì¸
-- ì´ê²Œ ì‹¤ì œ ëª¨ë‹ˆí„°ë§ì—ì„œ ì‚¬ìš©í•  í•µì‹¬ ë¡œì§!
SELECT
  customer_code,
  customer_name,
  source_type,
  gcs_path,
  COUNT(*) as table_fail_count,  -- ì´ íŒŒì¼ë¡œ ì¸í•œ í…Œì´ë¸” ì‹¤íŒ¨ ê±´ìˆ˜
  ARRAY_AGG(DISTINCT convert_config_id LIMIT 5) as failed_configs,
  ANY_VALUE(error_message) as sample_error
FROM
  `hyperlounge-dev.dashboard.convert_job_history`
WHERE
  DATE(created_at, 'Asia/Seoul') = '2025-11-07'
  AND status = 'fail'
GROUP BY customer_code, customer_name, source_type, gcs_path
ORDER BY table_fail_count DESC
LIMIT 20
```

#### 6. env í•„í„°ë§ í¬í•¨ (ìš´ì˜ í™˜ê²½ë§Œ)
```sql
-- env='ops' ë˜ëŠ” env != 'dev'ì¸ ê³ ê°ì‚¬ë§Œ ì¡°íšŒ
SELECT
  customer_code,
  customer_name,
  env,
  COUNT(*) as fail_count
FROM
  `hyperlounge-dev.dashboard.convert_job_history`
WHERE
  DATE(created_at, 'Asia/Seoul') = '2025-11-07'
  AND status = 'fail'
  AND env != 'dev'  -- ìš´ì˜/í…ŒìŠ¤íŠ¸ í™˜ê²½ë§Œ (dev ì œì™¸)
GROUP BY customer_code, customer_name, env
ORDER BY fail_count DESC
```

### Pythonì—ì„œ BigQuery í˜¸ì¶œ (í† í° ìƒì„±)

```python
from google.cloud import bigquery
import google.auth

# ì¸ì¦ (ê¸°ë³¸ ìê²©ì¦ëª… ì‚¬ìš©)
credentials, project = google.auth.default(
    scopes=['https://www.googleapis.com/auth/cloud-platform']
)

# BigQuery í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = bigquery.Client(
    credentials=credentials,
    project='hyperlounge-dev'
)

# ì¿¼ë¦¬ ì‹¤í–‰
query = """
SELECT
  customer_code,
  customer_name,
  source_type,
  COUNT(*) as fail_count
FROM
  `hyperlounge-dev.dashboard.convert_job_history`
WHERE
  DATE(created_at, 'Asia/Seoul') = '2025-11-07'
  AND status = 'fail'
  AND env != 'dev'
GROUP BY customer_code, customer_name, source_type
ORDER BY fail_count DESC
"""

query_job = client.query(query)
results = query_job.result()

for row in results:
    print(f"{row.customer_code} ({row.customer_name}): {row.fail_count}ê±´")
```

### ê°œë°œ ì§„í–‰ ìˆœì„œ

1. **ë°ì´í„° íƒìƒ‰** (BigQuery ì½˜ì†”)
   - ì¿¼ë¦¬ 5ë²ˆìœ¼ë¡œ ì‹¤ì œ íŒŒì¼ë³„ ì‹¤íŒ¨ íŒ¨í„´ í™•ì¸
   - ì—ëŸ¬ ë©”ì‹œì§€ ìƒ˜í”Œ í™•ì¸ (ì¿¼ë¦¬ 3ë²ˆ)
   - source_type ë¶„í¬ í™•ì¸ (ì¿¼ë¦¬ 4ë²ˆ)

2. **í•„í„°ë§ ë¡œì§ ê°œë°œ**
   - Firestoreì—ì„œ env != 'dev' ê³ ê°ì‚¬ ì¡°íšŒ (ì˜µì…˜ A)
   - BigQuery ì¿¼ë¦¬ì— ê³ ê°ì‚¬ ëª©ë¡ íŒŒë¼ë¯¸í„° ë°”ì¸ë”©
   - Noise íŒì • ë¡œì§ (ê°™ì€ ë‚  ì„±ê³µë¥  í™•ì¸)

3. **ì—ëŸ¬ ë¶„ë¥˜ ë¡œì§**
   - ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´ ë§¤ì¹­ (ì •ê·œì‹)
   - LLM fallback êµ¬í˜„

4. **ë©”ì‹œì§€ í¬ë§·íŒ…**
   - RPA/Board vs NonRPA êµ¬ë¶„
   - í…Œì´ë¸” í˜•ì‹ ìƒì„±
   - Teams ë©”ì‹œì§€ ì „ì†¡

---

## LLM ì—ëŸ¬ ë¶„ë¥˜ ì „ëµ

### ëª©ì 
- `error_message`ë¥¼ ì½ê³  **ì—ëŸ¬ ìœ í˜• ìë™ ë¶„ë¥˜**
- **ì¤‘ìš”**: ëª¨ë“  ì—ëŸ¬ê°€ ë¶„ë¥˜ë˜ì–´ì•¼ í•¨ (ëˆ„ë½ X)

### ì ‘ê·¼: ê·œì¹™ ê¸°ë°˜ + LLM Fallback â­

#### Phase 1: ê·œì¹™ ê¸°ë°˜ (ì •ê·œì‹) - ë¹ ë¥´ê³  í™•ì‹¤

```python
ERROR_PATTERNS = {
    "í—¤ë” ì—ëŸ¬": [
        r"Header .* not found",
        r"Cannot find header",
        r"Missing column",
        r"header_coordinate",
    ],
    "ì‹œíŠ¸ ì—ëŸ¬": [
        r"Sheet .* not found",
        r"Worksheet .* does not exist",
        r"No sheet named",
    ],
    "ì»¬ëŸ¼ ë²”ìœ„ ì—ëŸ¬": [
        r"usecols.*out of bounds",
        r"invalid column range",
    ],
    "Timeout": [
        r"[Tt]imeout",
        r"exceed.*time limit",
    ],
    "ë©”ëª¨ë¦¬ ë¶€ì¡±": [
        r"[Oo]ut of [Mm]emory",
        r"MemoryError",
    ],
    "íŒŒì¼ ì†ìƒ": [
        r"corrupt",
        r"damaged",
        r"cannot.*read.*file",
        r"empty.*sheet",
    ],
}

def classify_error_rule_based(error_message):
    """ì •ê·œì‹ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë¶„ë¥˜ (ë¬´ë£Œ, ë¹ ë¦„, ì •í™•)"""
    for error_type, patterns in ERROR_PATTERNS.items():
        for pattern in patterns:
            if re.search(pattern, error_message, re.IGNORECASE):
                return {
                    'type': error_type,
                    'confidence': 'high',
                    'method': 'rule'
                }
    return None  # ë§¤ì¹­ ì•ˆ ë¨
```

#### Phase 2: LLM Fallback - ê·œì¹™ ë§¤ì¹­ ì•ˆ ë˜ëŠ” ê²ƒë§Œ

```python
def classify_error_llm(error_message):
    """ê·œì¹™ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œì—ë§Œ LLM ì‚¬ìš© (ë¹„ìš© ì ˆê°)"""

    prompt = f"""
ë‹¤ìŒ ì—‘ì…€ ë³€í™˜(converter) ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ê³  ê°€ì¥ ì í•©í•œ ì—ëŸ¬ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”.

ì—ëŸ¬ ë©”ì‹œì§€:
{error_message}

ì—ëŸ¬ ìœ í˜• ëª©ë¡:
1. í—¤ë” ì—ëŸ¬ - ì—‘ì…€ í—¤ë”ëª…ì„ ì°¾ì§€ ëª»í•¨
2. ì‹œíŠ¸ ì—ëŸ¬ - ì—‘ì…€ ì‹œíŠ¸ëª…ì„ ì°¾ì§€ ëª»í•¨
3. ì»¬ëŸ¼ ë²”ìœ„ ì—ëŸ¬ - ì»¬ëŸ¼ ë²”ìœ„ê°€ ì˜ëª»ë¨
4. Timeout - ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼
5. ë©”ëª¨ë¦¬ ë¶€ì¡± - ë©”ëª¨ë¦¬ ë¶€ì¡±
6. íŒŒì¼ ì†ìƒ - íŒŒì¼ì´ ì†ìƒë¨
7. ê¸°íƒ€ - ìœ„ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ì§€ ì•ŠìŒ

ì‘ë‹µ í˜•ì‹ (JSONë§Œ):
{{
  "type": "ì—ëŸ¬ ìœ í˜•",
  "reason": "ë¶„ë¥˜ ê·¼ê±° (í•œ ì¤„)"
}}
"""

    response = llm_api_call(prompt)
    return {
        'type': response['type'],
        'confidence': 'medium',
        'method': 'llm',
        'reason': response['reason']
    }
```

#### Phase 3: í•˜ì´ë¸Œë¦¬ë“œ (ìµœì¢…)

```python
def classify_error(error_message):
    """
    1ì°¨: ê·œì¹™ ê¸°ë°˜ (ë¹ ë¦„, ì •í™•, ë¬´ë£Œ)
    2ì°¨: LLM (ëŠë¦¼, ìœ ì—°, ë¹„ìš© ë°œìƒ)
    """
    # 1. ê·œì¹™ ê¸°ë°˜ ì‹œë„
    rule_result = classify_error_rule_based(error_message)
    if rule_result:
        return rule_result

    # 2. LLMìœ¼ë¡œ fallback
    llm_result = classify_error_llm(error_message)

    # 3. ë¡œê¹… (ë‚˜ì¤‘ì— ê·œì¹™ ì¶”ê°€ìš©)
    log_unclassified_error(error_message, llm_result)

    return llm_result
```

**ì˜ˆìƒ ë¹„ìœ¨**:
- ê·œì¹™ ê¸°ë°˜: 90-95% (ëŒ€ë¶€ë¶„)
- LLM: 5-10% (ìƒˆë¡œìš´ ì—ëŸ¬ íŒ¨í„´)

---

### LLM ì‚¬ìš© ìµœì í™”

#### 1. ë°°ì¹˜ ì²˜ë¦¬ (ë¹„ìš© ì ˆê°)
```python
# í•œ ë²ˆì— ì—¬ëŸ¬ ì—ëŸ¬ ë¶„ë¥˜
unclassified_errors = [...]  # ê·œì¹™ ë§¤ì¹­ ì•ˆ ëœ ê²ƒë“¤ë§Œ

prompt = f"""
ë‹¤ìŒ {len(unclassified_errors)}ê°œì˜ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”.

ì—ëŸ¬ ë©”ì‹œì§€ë“¤:
1. {errors[0]}
2. {errors[1]}
...

ì‘ë‹µ í˜•ì‹ (JSON ë°°ì—´):
[
  {{"index": 1, "type": "í—¤ë” ì—ëŸ¬", "reason": "..."}},
  {{"index": 2, "type": "ì‹œíŠ¸ ì—ëŸ¬", "reason": "..."}}
]
"""
```
**íš¨ê³¼**: API í˜¸ì¶œ 10íšŒ â†’ 1íšŒ (ë¹„ìš© 1/10)

#### 2. ìºì‹± (ì¤‘ë³µ ë°©ì§€)
```python
# ê°™ì€ ì—ëŸ¬ ë©”ì‹œì§€ëŠ” ì¬ë¶„ë¥˜ ì•ˆ í•¨
error_cache = {}  # ë˜ëŠ” Redis

def classify_error_cached(error_message):
    cache_key = hashlib.md5(error_message.encode()).hexdigest()

    if cache_key in error_cache:
        return error_cache[cache_key]

    result = classify_error(error_message)
    error_cache[cache_key] = result
    return result
```

#### 3. í•™ìŠµ ë£¨í”„ (ê·œì¹™ í™•ì¥)
```python
# LLMì´ ë¶„ë¥˜í•œ ê²ƒì„ ì£¼ê¸°ì ìœ¼ë¡œ ê²€í† í•˜ê³  ê·œì¹™ì— ì¶”ê°€
# ì£¼ 1íšŒ ì •ë„

def review_llm_classifications():
    """
    LLMì´ ë¶„ë¥˜í•œ ì—ëŸ¬ë“¤ì„ ë³´ê³ 
    íŒ¨í„´ì´ ë³´ì´ë©´ ê·œì¹™ì— ì¶”ê°€
    """
    llm_classified = get_llm_classified_last_week()

    # ê°™ì€ ë¶„ë¥˜ê°€ ë§ì´ ë‚˜ì˜¨ ê²ƒë“¤
    for error_type, messages in llm_classified.groupby('type'):
        if len(messages) >= 5:
            print(f"\n{error_type}: {len(messages)}ê±´")
            for msg in messages[:3]:
                print(f"  - {msg}")

            # íŒ¨í„´ ë³´ì´ë©´ ê·œì¹™ ì¶”ê°€
            new_pattern = input("ê·œì¹™ ì¶”ê°€í•  íŒ¨í„´ (ì •ê·œì‹): ")
            if new_pattern:
                add_to_error_patterns(error_type, new_pattern)
```

**íš¨ê³¼**:
- ì´ˆê¸°: LLM 20% ì‚¬ìš©
- 1ë‹¬ í›„: LLM 5% ì‚¬ìš© (ê·œì¹™ í™•ì¥ë¨)
- 3ë‹¬ í›„: LLM 1-2% ì‚¬ìš© (ê±°ì˜ ëª¨ë“  íŒ¨í„´ ê·œì¹™í™”)

---

### ëˆ„ë½ ë°©ì§€ ì „ëµ

#### 1. ê¸°ë³¸ê°’ ì„¤ì • (Fail-safe)
```python
def classify_error(error_message):
    try:
        # ê·œì¹™ ë˜ëŠ” LLM ì‹œë„
        result = ...
        return result
    except Exception as e:
        # ì—ëŸ¬ ë‚˜ë„ ì¼ë‹¨ "ê¸°íƒ€"ë¡œ ë¶„ë¥˜ (ëˆ„ë½ ë°©ì§€)
        logger.error(f"Classification failed: {e}")
        return {
            'type': 'ê¸°íƒ€',
            'confidence': 'low',
            'method': 'fallback',
            'error': str(e),
            'original_message': error_message
        }
```

#### 2. ê²€ì¦ ë¡œì§
```python
# ëª¨ë“  ì‹¤íŒ¨ ê±´ì´ ë¶„ë¥˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
all_failures = get_failures_yesterday()
classified = [classify_error(f.error_message) for f in all_failures]

assert len(all_failures) == len(classified), "ëˆ„ë½ ë°œìƒ!"
assert all(c is not None for c in classified), "None ë¶„ë¥˜ ìˆìŒ!"
```

#### 3. ë¯¸ë¶„ë¥˜ ì•Œë¦¼
```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ ë¶„ë¥˜ ì‹¤íŒ¨ (ê°œë°œíŒ€ í™•ì¸ í•„ìš”)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ 2ê±´ì˜ ì—ëŸ¬ë¥¼ "ê¸°íƒ€"ë¡œ ë¶„ë¥˜í–ˆìŠµë‹ˆë‹¤
ğŸ”— [BigQueryì—ì„œ í™•ì¸í•˜ì—¬ ê·œì¹™ ì¶”ê°€]
```

---

## ìµœì¢… ë©”ì‹œì§€ í¬ë§· (Airflow ìŠ¤íƒ€ì¼ í…Œì´ë¸”)

### ì¼€ì´ìŠ¤ 1: ì •ìƒ (ì‹¤íŒ¨ ì—†ìŒ)

```
ğŸ“Š Converter ì‹¤íŒ¨ ë¦¬í¬íŠ¸ - 2025-11-09
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì „ì²´: 1,357ê±´ | âœ… ì„±ê³µ: 1,357ê±´ (100%) | âŒ ì‹¤íŒ¨: 0ê±´

ğŸ‰ ì‹¤íŒ¨ ê±´ì´ ì—†ìŠµë‹ˆë‹¤!

ğŸ”— ìƒì„¸ ë³´ê¸°: [BigQuery] | [Grafana]
```

---

### ì¼€ì´ìŠ¤ 2: ì‹¤íŒ¨ ìˆìŒ (RPA/Boardë§Œ)

```
ğŸ“Š Converter ì‹¤íŒ¨ ë¦¬í¬íŠ¸ - 2025-11-09
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì „ì²´: 1,507ê±´ | âœ… ì„±ê³µ: 1,232ê±´ (82%) | âŒ ì‹¤íŒ¨: 275ê±´ (18%)

ì‹¤íŒ¨ ë¶„ë¥˜:
â”œâ”€ ë¶„ì„ ëŒ€ìƒ: 125ê±´ (2ê°œ ê³ ê°ì‚¬) â†’ ì•„ë˜ í…Œì´ë¸”
â”œâ”€ ì •ì±…ìƒ ì œì™¸: 145ê±´ (xlsb 140ê±´, ì•”í˜¸í™” 5ê±´)
â””â”€ Noise: 5ê±´ (íŠ¹ì • íŒŒì¼ ë¬¸ì œ)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”´ [RPA/Board] ì¼ë³„ ìˆ˜ì§‘ ì—°ì† ì‹¤íŒ¨ - 2ê°œ ê³ ê°ì‚¬
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ê³ ê°ì‚¬        ìˆ˜ì§‘    í—¤ë”ì—ëŸ¬    ì‹œíŠ¸ì—ëŸ¬    ì»¬ëŸ¼ë²”ìœ„    Timeout    íŒŒì¼ì†ìƒ    ê¸°íƒ€    ë¶„ë¥˜ì‹¤íŒ¨
              ë°©ì‹
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
GCë…¹ì‹­ì      RPA     75ê±´        -           -           -          -           -       -
[c0159c00]            5ì¼ì—°ì†

ë§¤ì¼í™€ë”©ìŠ¤    Board   -           45ê±´        -           -          -           -       -
[c2026600]                        3ì¼ì—°ì†
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì†Œê³„                  75ê±´        45ê±´        0ê±´         0ê±´        0ê±´         0ê±´     0ê±´
                     (60%)       (40%)       (0%)        (0%)       (0%)        (0%)    (0%)

âš ï¸ [NonRPA] ê³ ê°ì‚¬ ì—…ë¡œë“œ ì‹¤íŒ¨ ì—†ìŒ

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ ì œì™¸ëœ ì‹¤íŒ¨ (ì°¸ê³ ìš©)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì •ì±…ìƒ ì œì™¸: 145ê±´ (ì¡°ì¹˜ ë¶ˆí•„ìš”)
â€¢ xlsb íŒŒì¼: 140ê±´ - í•œíˆ¬(80), ë³´ë ¹(35), í•œí™”(25)
â€¢ ì•”í˜¸í™” íŒŒì¼: 5ê±´ - ì œì£¼ë§¥ì£¼(3), ìŠ¤íŒŒì  (2)

Noise: 5ê±´ (ê°™ì€ ë‚  ë‹¤ë¥¸ íŒŒì¼ ì •ìƒ ì²˜ë¦¬)
â€¢ ê³ í”¼ì pc: 2ê±´ / 8ê±´ ì„±ê³µ
â€¢ í•œíˆ¬ email: 3ê±´ / 12ê±´ ì„±ê³µ

ë¶„ë¥˜ ë°©ë²•: ê·œì¹™ 115ê±´ (92%) | LLM 10ê±´ (8%)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”— ìƒì„¸ ì •ë³´
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ ë¶„ì„ ëŒ€ìƒ ìƒì„¸: [BigQuery - ë¶„ì„ í•„ìš” ì‹¤íŒ¨]
ğŸ“‹ ì œì™¸ í•­ëª© ìƒì„¸: [BigQuery - ì •ì±…/Noise]
ğŸ“ˆ íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ: [Grafana]
```

---

### ì¼€ì´ìŠ¤ 3: ì‹¤íŒ¨ ìˆìŒ (RPA/Board + PC/Email/Drive ëª¨ë‘)

```
ğŸ“Š Converter ì‹¤íŒ¨ ë¦¬í¬íŠ¸ - 2025-11-09
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì „ì²´: 1,507ê±´ | âœ… ì„±ê³µ: 1,232ê±´ (82%) | âŒ ì‹¤íŒ¨: 275ê±´ (18%)

ì‹¤íŒ¨ ë¶„ë¥˜:
â”œâ”€ ë¶„ì„ ëŒ€ìƒ: 157ê±´ (5ê°œ ê³ ê°ì‚¬) â†’ ì•„ë˜ í…Œì´ë¸”
â”œâ”€ ì •ì±…ìƒ ì œì™¸: 113ê±´ (xlsb 105ê±´, ì•”í˜¸í™” 8ê±´)
â””â”€ Noise: 5ê±´ (íŠ¹ì • íŒŒì¼ ë¬¸ì œ)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”´ [RPA/Board] ì¼ë³„ ìˆ˜ì§‘ ì—°ì† ì‹¤íŒ¨ - 2ê°œ ê³ ê°ì‚¬
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ê³ ê°ì‚¬        ìˆ˜ì§‘    í—¤ë”ì—ëŸ¬    ì‹œíŠ¸ì—ëŸ¬    ì»¬ëŸ¼ë²”ìœ„    Timeout    íŒŒì¼ì†ìƒ    ê¸°íƒ€    ë¶„ë¥˜ì‹¤íŒ¨
              ë°©ì‹
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
GCë…¹ì‹­ì      RPA     75ê±´        -           -           -          -           -       -
[c0159c00]            5ì¼ì—°ì†

ë§¤ì¼í™€ë”©ìŠ¤    Board   -           45ê±´        -           -          -           -       -
[c2026600]                        3ì¼ì—°ì†
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì†Œê³„                  75ê±´        45ê±´        0ê±´         0ê±´        0ê±´         0ê±´     0ê±´
                     (60%)       (40%)       (0%)        (0%)       (0%)        (0%)    (0%)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ [NonRPA] ê³ ê°ì‚¬ ì—…ë¡œë“œ ì–´ì œ ì‹¤íŒ¨ - 3ê°œ ê³ ê°ì‚¬
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ê³ ê°ì‚¬        ìˆ˜ì§‘         í—¤ë”ì—ëŸ¬    ì‹œíŠ¸ì—ëŸ¬    ì»¬ëŸ¼ë²”ìœ„    Timeout    íŒŒì¼ì†ìƒ    ê¸°íƒ€    ë¶„ë¥˜ì‹¤íŒ¨
              ë°©ì‹
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ê³ í”¼ì        pc           12ê±´        -           -           -          -           -       -
[c7005b01]                ì–´ì œ

ì œì£¼ë§¥ì£¼      email        -           -           8ê±´         -          -           -       -
[c4cd3b00]                                        ì–´ì œ

ìŠ¤íŒŒì  ë·°í‹°    shared_drive -           -           -           -          -           12ê±´    -
[caaa3b00]                                                                           ì–´ì œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì†Œê³„                  12ê±´        0ê±´         8ê±´         0ê±´        0ê±´         12ê±´    0ê±´
                     (38%)       (0%)        (25%)       (0%)       (0%)        (38%)   (0%)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š ì „ì²´ í•©ê³„
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì—ëŸ¬ ìœ í˜•: í—¤ë” 87ê±´ (55%) | ì‹œíŠ¸ 45ê±´ (29%) | ì»¬ëŸ¼ë²”ìœ„ 8ê±´ (5%) | ê¸°íƒ€ 12ê±´ (8%)
ë¶„ë¥˜ ë°©ë²•: ê·œì¹™ 145ê±´ (92%) | LLM 12ê±´ (8%) | ì‹¤íŒ¨ 0ê±´ (0%)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ ì œì™¸ëœ ì‹¤íŒ¨ (ì°¸ê³ ìš©)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì •ì±…ìƒ ì œì™¸: 113ê±´ (ì¡°ì¹˜ ë¶ˆí•„ìš”)
â€¢ xlsb íŒŒì¼: 105ê±´ - í•œíˆ¬(70), ë³´ë ¹(25), í•œí™”(10)
â€¢ ì•”í˜¸í™” íŒŒì¼: 8ê±´ - ì œì£¼ë§¥ì£¼(5), ìŠ¤íŒŒì  (3)

Noise: 5ê±´ (ê°™ì€ ë‚  ë‹¤ë¥¸ íŒŒì¼ ì •ìƒ ì²˜ë¦¬)
â€¢ í•œíˆ¬ pc: 3ê±´ / 15ê±´ ì„±ê³µ
â€¢ ë³´ë ¹ email: 2ê±´ / 8ê±´ ì„±ê³µ

ë¶„ë¥˜ ë°©ë²•: ê·œì¹™ 145ê±´ (92%) | LLM 12ê±´ (8%)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”— ìƒì„¸ ì •ë³´
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ ë¶„ì„ ëŒ€ìƒ ìƒì„¸: [BigQuery - ë¶„ì„ í•„ìš” ì‹¤íŒ¨]
ğŸ“‹ ì œì™¸ í•­ëª© ìƒì„¸: [BigQuery - ì •ì±…/Noise]
ğŸ“ˆ íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ: [Grafana]
```

---

## ì•„í‚¤í…ì²˜ ì„¤ê³„ (ê³ ê°ì‚¬ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì§€ì›)

### ì „ì²´ êµ¬ì¡°

```
converter_failure_monitor/
â”œâ”€ config/
â”‚  â”œâ”€ common_config.json           # ê³µí†µ ì„¤ì •
â”‚  â”œâ”€ customers/
â”‚  â”‚  â”œâ”€ c0159c00.json             # GCë…¹ì‹­ì ì»¤ìŠ¤í…€ ì„¤ì •
â”‚  â”‚  â”œâ”€ c2026600.json             # ë§¤ì¼í™€ë”©ìŠ¤ ì»¤ìŠ¤í…€ ì„¤ì •
â”‚  â”‚  â””â”€ ...
â”‚  â””â”€ exclude_rules/
â”‚     â”œâ”€ common_exclude.json       # ê³µí†µ ì œì™¸ ê·œì¹™
â”‚     â””â”€ customer_exclude/
â”‚        â”œâ”€ c0159c00.json          # ê³ ê°ì‚¬ë³„ ì œì™¸ ê·œì¹™
â”‚        â””â”€ ...
â”œâ”€ main.py
â”œâ”€ classifier.py                   # ì—ëŸ¬ ë¶„ë¥˜ê¸°
â”œâ”€ filter.py                       # í•„í„°ë§ ë¡œì§
â”œâ”€ formatter.py                    # ë©”ì‹œì§€ í¬ë§·í„°
â””â”€ requirements.txt
```

---

### 1. ê³µí†µ ì„¤ì • (`config/common_config.json`)

```json
{
  "error_types": [
    "í—¤ë” ì—ëŸ¬",
    "ì‹œíŠ¸ ì—ëŸ¬",
    "ì»¬ëŸ¼ ë²”ìœ„ ì—ëŸ¬",
    "Timeout",
    "íŒŒì¼ ì†ìƒ",
    "ê¸°íƒ€"
  ],

  "source_type_mapping": {
    "rpa": "RPA",
    "board": "Board",
    "pc": "pc",
    "email": "email",
    "shared_drive": "shared_drive"
  },

  "consecutive_fail_threshold": {
    "rpa": 3,
    "board": 3
  },

  "table_max_rows": {
    "rpa_board": 10,
    "pc_email_drive": 10
  },

  "llm_settings": {
    "model": "gpt-4o-mini",
    "temperature": 0,
    "max_tokens": 500,
    "batch_size": 20
  }
}
```

---

### 2. ê³µí†µ ì œì™¸ ê·œì¹™ (`config/exclude_rules/common_exclude.json`)

```json
{
  "policy_exclusions": [
    {
      "id": "xlsb_not_supported",
      "pattern": "xlsb.*not supported|xlsb file format",
      "reason": "xlsb íŒŒì¼ í˜•ì‹ ë¯¸ì§€ì› (ì •ì±…)",
      "category": "íŒŒì¼ í˜•ì‹",
      "enabled": true
    },
    {
      "id": "encrypted_file",
      "pattern": "Password.*required|encrypted|Workbook is encrypted",
      "reason": "ì•”í˜¸í™”ëœ íŒŒì¼ (ì •ì±…ìƒ ì²˜ë¦¬ ë¶ˆê°€)",
      "category": "ì•”í˜¸í™”",
      "enabled": true
    },
    {
      "id": "file_size_limit",
      "pattern": "File size exceeds.*100MB",
      "reason": "íŒŒì¼ í¬ê¸° ì œí•œ ì´ˆê³¼ (ì •ì±…)",
      "category": "íŒŒì¼ í¬ê¸°",
      "enabled": true
    },
    {
      "id": "empty_file",
      "pattern": "empty file|no data|0 bytes",
      "reason": "ë¹ˆ íŒŒì¼ (ë°ì´í„° ì—†ìŒ)",
      "category": "ë¹ˆ íŒŒì¼",
      "enabled": true
    }
  ],

  "noise_rules": {
    "same_day_success_threshold": 0.8,
    "description": "ê°™ì€ ë‚  ê°™ì€ configë¡œ 80% ì´ìƒ ì„±ê³µ ì‹œ Noise"
  }
}
```

---

### 3. ê³ ê°ì‚¬ë³„ ì»¤ìŠ¤í…€ ì„¤ì • (`config/customers/c0159c00.json`)

```json
{
  "customer_code": "c0159c00",
  "customer_name": "GCë…¹ì‹­ì",

  "custom_error_types": {
    "enabled": false,
    "additional_types": []
  },

  "custom_exclude_rules": {
    "enabled": true,
    "rules": [
      {
        "id": "gc_specific_format",
        "pattern": "ì‹œí—˜ì„±ì ì„œ.*not found",
        "reason": "GCë…¹ì‹­ì - ì‹œí—˜ì„±ì ì„œëŠ” ë³„ë„ ì²˜ë¦¬ (ì •ì±…)",
        "category": "ê³ ê°ì‚¬ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤",
        "enabled": true
      }
    ]
  },

  "llm_prompt_override": {
    "enabled": true,
    "additional_context": "GCë…¹ì‹­ìëŠ” ì œì•½íšŒì‚¬ë¡œ, 'ì‹œí—˜ì„±ì ì„œ', 'GST', 'MSDS' ê°™ì€ ì œì•½ ì „ë¬¸ ìš©ì–´ê°€ ìì£¼ ë‚˜ì˜µë‹ˆë‹¤."
  },

  "consecutive_fail_threshold_override": {
    "enabled": false,
    "value": 5
  },

  "notification_settings": {
    "enabled": false,
    "custom_webhook": null,
    "mentions": ["@data-team"]
  }
}
```

---

### 4. ê³ ê°ì‚¬ë³„ ì œì™¸ ê·œì¹™ (`config/exclude_rules/customer_exclude/c0159c00.json`)

```json
{
  "customer_code": "c0159c00",
  "additional_exclusions": [
    {
      "id": "gc_test_report",
      "pattern": "ì‹œí—˜ì„±ì ì„œ.*Sheet.*not found",
      "reason": "ì‹œí—˜ì„±ì ì„œëŠ” ë³„ë„ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìˆìŒ",
      "category": "GC íŠ¹ìˆ˜ ì¼€ì´ìŠ¤",
      "enabled": true
    },
    {
      "id": "gc_legacy_format",
      "pattern": "xls file.*97-2003",
      "reason": "GCë…¹ì‹­ìëŠ” êµ¬ ë²„ì „ ì—‘ì…€ í—ˆìš© (ì •ì±…)",
      "category": "ê³ ê°ì‚¬ ì •ì±…",
      "enabled": true
    }
  ]
}
```

---

### 5. ì„¤ì • ë¡œë” (`classifier.py`)

```python
import json
import re
from pathlib import Path
from typing import Dict, List, Optional

class ConfigLoader:
    def __init__(self, config_dir: str = "./config"):
        self.config_dir = Path(config_dir)
        self.common_config = self._load_common_config()
        self.common_exclude = self._load_common_exclude()
        self.customer_configs = {}
        self.customer_excludes = {}

    def _load_common_config(self) -> dict:
        with open(self.config_dir / "common_config.json") as f:
            return json.load(f)

    def _load_common_exclude(self) -> dict:
        with open(self.config_dir / "exclude_rules" / "common_exclude.json") as f:
            return json.load(f)

    def get_customer_config(self, customer_code: str) -> dict:
        """ê³ ê°ì‚¬ë³„ ì„¤ì • ë¡œë“œ (ì—†ìœ¼ë©´ ê³µí†µ ì„¤ì •)"""
        if customer_code in self.customer_configs:
            return self.customer_configs[customer_code]

        customer_file = self.config_dir / "customers" / f"{customer_code}.json"
        if customer_file.exists():
            with open(customer_file) as f:
                config = json.load(f)
                self.customer_configs[customer_code] = config
                return config

        return {}  # ì»¤ìŠ¤í…€ ì„¤ì • ì—†ìŒ

    def get_exclude_rules(self, customer_code: str) -> dict:
        """ê³ ê°ì‚¬ë³„ ì œì™¸ ê·œì¹™ (ê³µí†µ + ê³ ê°ì‚¬ë³„ ë³‘í•©)"""
        # ê³µí†µ ê·œì¹™
        rules = self.common_exclude.copy()

        # ê³ ê°ì‚¬ë³„ ê·œì¹™ ì¶”ê°€
        customer_exclude_file = self.config_dir / "exclude_rules" / "customer_exclude" / f"{customer_code}.json"
        if customer_exclude_file.exists():
            with open(customer_exclude_file) as f:
                customer_rules = json.load(f)
                # ê³µí†µ + ê³ ê°ì‚¬ë³„ ë³‘í•©
                if "additional_exclusions" in customer_rules:
                    rules["policy_exclusions"].extend(customer_rules["additional_exclusions"])

        # ê³ ê°ì‚¬ ì»¤ìŠ¤í…€ ì„¤ì •ì—ì„œë„ ì¶”ê°€
        customer_config = self.get_customer_config(customer_code)
        if customer_config.get("custom_exclude_rules", {}).get("enabled"):
            rules["policy_exclusions"].extend(
                customer_config["custom_exclude_rules"]["rules"]
            )

        return rules


class ErrorClassifier:
    def __init__(self, config_loader: ConfigLoader):
        self.config_loader = config_loader
        self.common_config = config_loader.common_config

    def classify(self, error_message: str, customer_code: str) -> dict:
        """
        ì—ëŸ¬ ë¶„ë¥˜ (ê·œì¹™ + LLM)
        ê³ ê°ì‚¬ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì§€ì›
        """
        # 1. ê·œì¹™ ê¸°ë°˜ ì‹œë„
        rule_result = self._classify_by_rule(error_message)
        if rule_result:
            return rule_result

        # 2. LLM ì‹œë„ (ê³ ê°ì‚¬ë³„ í”„ë¡¬í”„íŠ¸)
        llm_result = self._classify_by_llm(error_message, customer_code)
        return llm_result

    def _classify_by_rule(self, error_message: str) -> Optional[dict]:
        """ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜"""
        error_patterns = {
            "í—¤ë” ì—ëŸ¬": [
                r"Header .* not found",
                r"Cannot find header",
                r"Missing column",
            ],
            "ì‹œíŠ¸ ì—ëŸ¬": [
                r"Sheet .* not found",
                r"Worksheet .* does not exist",
            ],
            # ... ìƒëµ
        }

        for error_type, patterns in error_patterns.items():
            for pattern in patterns:
                if re.search(pattern, error_message, re.IGNORECASE):
                    return {
                        "type": error_type,
                        "method": "rule",
                        "confidence": "high"
                    }
        return None

    def _classify_by_llm(self, error_message: str, customer_code: str) -> dict:
        """LLM ê¸°ë°˜ ë¶„ë¥˜ (ê³ ê°ì‚¬ë³„ í”„ë¡¬í”„íŠ¸)"""
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
        base_prompt = f"""
ë‹¤ìŒ ì—‘ì…€ ë³€í™˜ ì—ëŸ¬ë¥¼ ë¶„ì„í•˜ê³  ë¶„ë¥˜í•˜ì„¸ìš”.

ì—ëŸ¬ ë©”ì‹œì§€: {error_message}

ì—ëŸ¬ ìœ í˜•: {', '.join(self.common_config['error_types'])}
"""

        # ê³ ê°ì‚¬ë³„ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
        customer_config = self.config_loader.get_customer_config(customer_code)
        if customer_config.get("llm_prompt_override", {}).get("enabled"):
            additional_context = customer_config["llm_prompt_override"]["additional_context"]
            base_prompt += f"\n\n[ê³ ê°ì‚¬ ì»¨í…ìŠ¤íŠ¸]\n{additional_context}\n"

        # LLM í˜¸ì¶œ
        response = self._call_llm(base_prompt)
        return {
            "type": response["type"],
            "method": "llm",
            "confidence": "medium",
            "reason": response.get("reason")
        }


class FailureFilter:
    def __init__(self, config_loader: ConfigLoader):
        self.config_loader = config_loader

    def filter_failures(self, failures: List[dict]) -> dict:
        """
        ì‹¤íŒ¨ ê±´ í•„í„°ë§
        - ì •ì±…ìƒ ì œì™¸
        - Noise
        - ë¶„ì„ ëŒ€ìƒ
        """
        result = {
            "to_analyze": [],
            "excluded_policy": [],
            "excluded_noise": []
        }

        for failure in failures:
            customer_code = failure["customer_code"]
            error_message = failure["error_message"]

            # ê³ ê°ì‚¬ë³„ ì œì™¸ ê·œì¹™
            exclude_rules = self.config_loader.get_exclude_rules(customer_code)

            # 1. ì •ì±…ìƒ ì œì™¸ ì²´í¬
            if self._is_excluded_by_policy(error_message, exclude_rules):
                result["excluded_policy"].append(failure)
                continue

            # 2. Noise ì²´í¬
            if self._is_noise(failure, exclude_rules):
                result["excluded_noise"].append(failure)
                continue

            # 3. ë¶„ì„ ëŒ€ìƒ
            result["to_analyze"].append(failure)

        return result

    def _is_excluded_by_policy(self, error_message: str, exclude_rules: dict) -> bool:
        """ì •ì±…ìƒ ì œì™¸ ì—¬ë¶€"""
        for rule in exclude_rules["policy_exclusions"]:
            if not rule.get("enabled", True):
                continue

            if re.search(rule["pattern"], error_message, re.IGNORECASE):
                return True
        return False

    def _is_noise(self, failure: dict, exclude_rules: dict) -> bool:
        """Noise ì—¬ë¶€ (ê°™ì€ ë‚  ë‹¤ë¥¸ íŒŒì¼ ì„±ê³µ)"""
        noise_rules = exclude_rules.get("noise_rules", {})
        threshold = noise_rules.get("same_day_success_threshold", 0.8)

        # ê°™ì€ ë‚  ì„±ê³µë¥  ì²´í¬ (BigQueryì—ì„œ ë¯¸ë¦¬ ê³„ì‚°í•´ì„œ ë„˜ê¹€)
        same_day_success_rate = failure.get("same_day_success_rate", 0)
        return same_day_success_rate >= threshold
```

---

### 6. ë©”ì¸ ë¡œì§ (`main.py`)

```python
def generate_converter_report(target_date: str):
    """
    Converter ì‹¤íŒ¨ ë¦¬í¬íŠ¸ ìƒì„±
    """
    # ì„¤ì • ë¡œë“œ
    config_loader = ConfigLoader("./config")
    classifier = ErrorClassifier(config_loader)
    filter_engine = FailureFilter(config_loader)

    # 1. BigQueryì—ì„œ ì‹¤íŒ¨ ê±´ ì¡°íšŒ
    all_failures = fetch_failures_from_bq(target_date)

    # 2. í•„í„°ë§ (ì •ì±… ì œì™¸, Noise)
    filtered = filter_engine.filter_failures(all_failures)

    # 3. ë¶„ì„ ëŒ€ìƒë§Œ ì—ëŸ¬ ë¶„ë¥˜
    to_analyze = filtered["to_analyze"]
    classified = []

    for failure in to_analyze:
        classification = classifier.classify(
            failure["error_message"],
            failure["customer_code"]
        )
        classified.append({**failure, **classification})

    # 4. ê³ ê°ì‚¬ë³„ ê·¸ë£¹í•‘
    by_customer = group_by_customer_and_source(classified)

    # 5. ë©”ì‹œì§€ í¬ë§·íŒ…
    message = format_message(
        by_customer=by_customer,
        excluded_policy=filtered["excluded_policy"],
        excluded_noise=filtered["excluded_noise"],
        config=config_loader.common_config
    )

    # 6. Teams ì „ì†¡
    send_to_teams(message)
```

---

## ì»¤ìŠ¤í„°ë§ˆì´ì§• ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ìƒˆ ê³ ê°ì‚¬ ì¶”ê°€ (ê¸°ë³¸ ì„¤ì •)
```bash
# ì•„ë¬´ ì„¤ì • ì•ˆ í•´ë„ ë¨
# ê³µí†µ ì„¤ì • ìë™ ì ìš©
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: íŠ¹ì • ê³ ê°ì‚¬ë§Œ xlsb í—ˆìš©
```json
// config/exclude_rules/customer_exclude/c8cd3500.json (í•œíˆ¬)
{
  "customer_code": "c8cd3500",
  "additional_exclusions": [],
  "rule_overrides": [
    {
      "id": "xlsb_not_supported",
      "enabled": false  // ì´ ê³ ê°ì‚¬ëŠ” xlsb ì œì™¸ ì•ˆ í•¨
    }
  ]
}
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ê³ ê°ì‚¬ íŠ¹ìˆ˜ ì—ëŸ¬ íŒ¨í„´
```json
// config/customers/c0159c00.json (GCë…¹ì‹­ì)
{
  "custom_exclude_rules": {
    "enabled": true,
    "rules": [
      {
        "pattern": "ì‹œí—˜ì„±ì ì„œ.*Sheet",
        "reason": "ì‹œí—˜ì„±ì ì„œëŠ” ë³„ë„ ì²˜ë¦¬",
        "enabled": true
      }
    ]
  },
  "llm_prompt_override": {
    "enabled": true,
    "additional_context": "ì œì•½íšŒì‚¬ íŠ¹ìˆ˜ ìš©ì–´: GST, MSDS, ì‹œí—˜ì„±ì ì„œ ë“±"
  }
}
```

### ì‹œë‚˜ë¦¬ì˜¤ 4: ì—°ì† ì‹¤íŒ¨ ì„ê³„ê°’ ë³€ê²½
```json
// config/customers/c7005b01.json (ê³ í”¼ì)
{
  "consecutive_fail_threshold_override": {
    "enabled": true,
    "value": 5  // 5ì¼ ì—°ì† ì‹¤íŒ¨ë¶€í„° ì•Œë¦¼
  }
}
```

---

## GCS íŒŒì¼ ì ‘ê·¼ ë° ë¶„ì„

### GCS íŒŒì¼ ê²½ë¡œ êµ¬ì¡°

ì‹¤ì œ Excel íŒŒì¼ì€ `gcs_path`ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤:

**BigQuery `convert_job_history`ì˜ gcs_path**:
- í˜•ì‹: `{source_type}/{source_id}/crawl/{file_id}`
- ì˜ˆ: `email/s12bf560/crawl/fcf7a5ae477c`

**ì‹¤ì œ GCS ìœ„ì¹˜**:
- ë²„í‚·: `hyperlounge-{customer_code}`
- ê²½ë¡œ: gcs_path ê·¸ëŒ€ë¡œ
- ì˜ˆ: `gs://hyperlounge-c8cd3500/email/s12bf560/crawl/fcf7a5ae477c`

**Convert Config ìœ„ì¹˜**:
- ë²„í‚·: `hyperlounge-migrator`
- ê²½ë¡œ: `convert_configs/{customer_code}/{config_id}.json`
- ì˜ˆ: `gs://hyperlounge-migrator/convert_configs/c8cd3500/eml_rv56_021.json`

---

### íŒŒì¼ ì½ê¸° ì˜ˆì‹œ ì½”ë“œ

```python
from google.cloud import storage
import io
import pandas as pd
import json

def get_excel_from_gcs(customer_code: str, gcs_path: str):
    """
    gcs_pathì—ì„œ ì‹¤ì œ Excel íŒŒì¼ ì½ê¸°

    Args:
        customer_code: ê³ ê°ì‚¬ ì½”ë“œ (ì˜ˆ: "c8cd3500")
        gcs_path: BigQueryì˜ gcs_path ê°’ (ì˜ˆ: "email/s12bf560/crawl/fcf7a5ae477c")

    Returns:
        pd.ExcelFile: Excel íŒŒì¼ ê°ì²´
    """
    storage_client = storage.Client(project='hyperlounge-dev')
    bucket_name = f"hyperlounge-{customer_code}"

    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(gcs_path)

    if not blob.exists():
        raise FileNotFoundError(f"File not found: gs://{bucket_name}/{gcs_path}")

    # Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
    excel_data = blob.download_as_bytes()
    excel_file = pd.ExcelFile(io.BytesIO(excel_data))

    return excel_file

def get_convert_config(customer_code: str, config_id: str):
    """
    Convert config JSON ì½ê¸°

    Args:
        customer_code: ê³ ê°ì‚¬ ì½”ë“œ
        config_id: config ID (ì˜ˆ: "eml_rv56_021")

    Returns:
        dict: Convert config JSON
    """
    storage_client = storage.Client(project='hyperlounge-dev')
    bucket = storage_client.bucket("hyperlounge-migrator")
    blob = bucket.blob(f"convert_configs/{customer_code}/{config_id}.json")

    if not blob.exists():
        raise FileNotFoundError(f"Config not found: {config_id}")

    config = json.loads(blob.download_as_text())
    return config

# ì‚¬ìš© ì˜ˆì‹œ
excel_file = get_excel_from_gcs("c8cd3500", "email/s12bf560/crawl/fcf7a5ae477c")
print(f"ì‹œíŠ¸ ëª©ë¡: {excel_file.sheet_names}")

config = get_convert_config("c8cd3500", "eml_rv56_021")
print(f"ì°¾ìœ¼ë ¤ëŠ” ì‹œíŠ¸: {list(config['convert'].keys())}")
```

---

### ì‹¤ì œ ë¶„ì„ ì˜ˆì‹œ

2025-11-07 trinityspa (c8cd3500) email ì‹¤íŒ¨ ì¼€ì´ìŠ¤:

**ì—ëŸ¬ ì •ë³´** (BigQuery):
```
gcs_path: email/s12bf560/crawl/fcf7a5ae477c
convert_config_id: eml_rv56_021
error_message: not found matched sheet from ['Sheet1']
```

**ì‹¤ì œ Excel í™•ì¸**:
```python
excel_file = get_excel_from_gcs("c8cd3500", "email/s12bf560/crawl/fcf7a5ae477c")
# ê²°ê³¼: sheet_names = ['Sheet1']
#       ì»¬ëŸ¼: ['ê°€ë™ì¼', 'ë‹´ë‹¹ì', 'ê°€ë™ë¥ ']
#       í–‰ ìˆ˜: 2
```

**Convert Config í™•ì¸**:
```python
config = get_convert_config("c8cd3500", "eml_rv56_021")
# ê²°ê³¼: convert.keys() = ['í‹°ì¼“íŒ…\\(.*\\)\\s*$']  # ì •ê·œì‹ íŒ¨í„´
```

**ì—ëŸ¬ ì›ì¸ ë¶„ì„**:
- Configê°€ ì°¾ìœ¼ë ¤ëŠ” ì‹œíŠ¸: `'í‹°ì¼“íŒ…\(.*\)\s*$'` (ì •ê·œì‹ - "í‹°ì¼“íŒ…(ì–´ì©Œê³ )" í˜•íƒœ)
- ì‹¤ì œ Excelì˜ ì‹œíŠ¸: `['Sheet1']`
- **ê²°ë¡ **: ì‹œíŠ¸ ì´ë¦„ ë¶ˆì¼ì¹˜ â†’ "not found matched sheet" ì—ëŸ¬ ë°œìƒ

**í•´ê²° ë°©ì•ˆ**:
1. Convert config ìˆ˜ì • (ì‹œíŠ¸ëª… íŒ¨í„´ ì¡°ì •)
2. Excel íŒŒì¼ ì¬ìš”ì²­ (ì˜¬ë°”ë¥¸ ì‹œíŠ¸ëª… í¬í•¨)
3. ì†ŒìŠ¤ íŒŒì¼ ìë™ ë³€í™˜ ê·œì¹™ ì¶”ê°€

---

### LLM ë¶„ì„ ì‹œ í¬í•¨ ì •ë³´

#### Phase 1: Basic (Error Messageë§Œ)
```python
classification = classify_error(error_message)
# ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ë¶€ë¶„ ë¶„ë¥˜ ê°€ëŠ¥
```

#### Phase 2: Enhanced (+ Convert Config)
```python
context = {
    "error_message": "not found matched sheet from ['Sheet1']",
    "convert_config": {
        "ì°¾ìœ¼ë ¤ëŠ”_ì‹œíŠ¸": "í‹°ì¼“íŒ…\\(.*\\)\\s*$"
    }
}
# â†’ LLMì´ "ì‹œíŠ¸ëª… íŒ¨í„´ ë¶ˆì¼ì¹˜" ë” ì •í™•í•˜ê²Œ íŒŒì•…
```

#### Phase 3: Deep Analysis (+ Actual Excel)
```python
context = {
    "error_message": "not found matched sheet from ['Sheet1']",
    "convert_config": {
        "ì°¾ìœ¼ë ¤ëŠ”_ì‹œíŠ¸": "í‹°ì¼“íŒ…\\(.*\\)\\s*$"
    },
    "actual_excel": {
        "ì‹œíŠ¸_ëª©ë¡": ["Sheet1"],
        "Sheet1_ì»¬ëŸ¼": ["ê°€ë™ì¼", "ë‹´ë‹¹ì", "ê°€ë™ë¥ "],
        "Sheet1_í–‰ìˆ˜": 2,
        "ì²«_3í–‰_ë¯¸ë¦¬ë³´ê¸°": [...]
    }
}
# â†’ LLMì´ ì •í™•í•œ ì›ì¸ + í•´ê²°ë°©ì•ˆ ì œì‹œ ê°€ëŠ¥
```

**ê¶Œì¥ ì ‘ê·¼**:
- ê¸°ë³¸: Phase 1 (ê·œì¹™ ê¸°ë°˜, ë¬´ë£Œ, ë¹ ë¦„)
- í•„ìš” ì‹œ: Phase 2 (Config í¬í•¨, ì¤‘ìš”í•œ ì—ëŸ¬ë§Œ)
- íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: Phase 3 (Excelê¹Œì§€ ì½ê¸°, ìˆ˜ë™ ë¶„ì„ í•„ìš” ì‹œ)

---

### ê¶Œí•œ ìš”êµ¬ì‚¬í•­

**í•„ìš”í•œ IAM ê¶Œí•œ**:
- `storage.objects.get` (Storage Object Viewer)
- `storage.objects.list` (í•„ìš” ì‹œ)

**ì ìš© ë²”ìœ„**:
- `hyperlounge-{customer_code}` ë²„í‚·ë“¤ (Excel íŒŒì¼)
- `hyperlounge-migrator` ë²„í‚· (Convert configs)

---

## êµ¬í˜„ ì „ëµ ë° ë¡œë“œë§µ

### ë‹¨ê³„ë³„ ì ‘ê·¼ ë°©ì‹

ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì€ **ì ì§„ì ìœ¼ë¡œ ê°œì„ **í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.
- ì²˜ìŒë¶€í„° ì™„ë²½í•œ ì‹œìŠ¤í…œë³´ë‹¤ëŠ” **ì‘ë™í•˜ëŠ” MVP**ë¥¼ ë¨¼ì € ë§Œë“¤ê³ 
- **ì‹¤ì œ ìš´ì˜ ë°ì´í„°**ë¥¼ ë³´ë©´ì„œ ê°œì„ ì  íŒŒì•…
- ì§„ì§œ í•„ìš”í•œ ê¸°ëŠ¥ë§Œ ì¶”ê°€ (ì˜¤ë²„ì—”ì§€ë‹ˆì–´ë§ ë°©ì§€)

---

### Phase 1: MVP - ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ (1-3ì¼)

**ëª©í‘œ**: ë§¤ì¼ ì•„ì¹¨ Teamsë¡œ Converter ì‹¤íŒ¨ ë¦¬í¬íŠ¸ ë°›ê¸°

**í•µì‹¬ ê¸°ëŠ¥**:
```
âœ… BigQueryì—ì„œ ì–´ì œ ì‹¤íŒ¨ ê±´ ì¡°íšŒ
âœ… íŒŒì¼ ë ˆë²¨ ê·¸ë£¹í•‘ (gcs_path ê¸°ì¤€)
âœ… ì—ëŸ¬ ë¶„ë¥˜ (ê·œì¹™ ê¸°ë°˜ë§Œ, LLM ì—†ì´)
âœ… Teams ë©”ì‹œì§€ í¬ë§·íŒ…
âœ… RPA/Board vs NonRPA êµ¬ë¶„
```

**ì˜ë„ì ìœ¼ë¡œ ì œì™¸**:
```
âŒ LLM ì—ëŸ¬ ë¶„ì„ â†’ ê·œì¹™ë§Œìœ¼ë¡œ ì‹œì‘
âŒ Excel íŒŒì¼ ì½ê¸° â†’ error_messageë§Œ ì‚¬ìš©
âŒ Convert config ë¶„ì„ â†’ ë‚˜ì¤‘ì— í•„ìš”í•˜ë©´
âŒ Noise í•„í„°ë§ â†’ ì¼ë‹¨ ëª¨ë“  ì‹¤íŒ¨ í‘œì‹œ
âŒ DAG ìƒíƒœ ì²´í¬ â†’ ë‹¨ìˆœíˆ ì–´ì œ ë‚ ì§œë§Œ
```

**ì‹¤íŒ¨ ê±´ìˆ˜ ë§ìœ¼ë©´?**
- ìƒìœ„ Nê°œ ê³ ê°ì‚¬ë§Œ í‘œì‹œ (ì˜ˆ: 10ê°œ)
- ë‚˜ë¨¸ì§€ëŠ” "ê¸°íƒ€ Xê°œ ê³ ê°ì‚¬" ìš”ì•½
- BigQuery ë§í¬ë¡œ ì „ì²´ ë³´ê¸°

**êµ¬ì¡°**:
```
converter_monitor/
â”œâ”€ main.py              # ë©”ì¸ ë¡œì§
â”œâ”€ queries.py           # BigQuery ì¿¼ë¦¬ë“¤
â”œâ”€ classifier.py        # ì—ëŸ¬ ë¶„ë¥˜ (ê·œì¹™ë§Œ)
â”œâ”€ formatter.py         # Teams ë©”ì‹œì§€ í¬ë§·
â”œâ”€ config.py            # ì„¤ì •ê°’
â””â”€ requirements.txt
```

**ë°°í¬**: Cloud Scheduler + Cloud Function (ê°„ë‹¨)

---

### Phase 2: ìš´ì˜ í”¼ë“œë°± ë°˜ì˜ (1-2ì£¼ ìš´ì˜ í›„)

**Phase 1 ìš´ì˜í•˜ë©´ì„œ ë°œê²¬ë  ë¬¸ì œë“¤**:

1. **ì‹¤íŒ¨ ê±´ìˆ˜ í†µê³„ íŒŒì•…**
   ```
   ë¬¸ì œ: ë§¤ì¼ ì‹¤íŒ¨ ê±´ì´ ë„ˆë¬´ ë§ìŒ (100ê±´+)
   ëŒ€ì‘:
   - Noise í•„í„°ë§ ì¶”ê°€ (ê°™ì€ ë‚  ì„±ê³µë¥  ì²´í¬)
   - ì •ì±…ìƒ ì œì™¸ ê·œì¹™ (xlsb, ì•”í˜¸í™”) ì ìš©
   - ìš°ì„ ìˆœìœ„ ë¡œì§ (ì—°ì† ì‹¤íŒ¨ > ì¼íšŒì„± ì‹¤íŒ¨)
   ```

2. **ì—ëŸ¬ íŒ¨í„´ ë¶„ì„**
   ```
   ë¬¸ì œ: ê°™ì€ ì—ëŸ¬ê°€ ë°˜ë³µë¨
   ëŒ€ì‘:
   - ìì£¼ ë‚˜ì˜¤ëŠ” ì—ëŸ¬ â†’ ê·œì¹™ì— ì¶”ê°€
   - ë¶„ë¥˜ ì•ˆ ë˜ëŠ” ì—ëŸ¬ â†’ LLM ë„ì… ê²€í† 
   ```

3. **ë©”ì‹œì§€ ê°€ë…ì„±**
   ```
   ë¬¸ì œ: í…Œì´ë¸”ì´ ë„ˆë¬´ ê¹€
   ëŒ€ì‘:
   - ê³ ê°ì‚¬ í‘œì‹œ ê°œìˆ˜ ì¡°ì •
   - ìš”ì•½ ë ˆë²¨ ë³€ê²½
   - ë§í¬ ì¶”ê°€ë¡œ ìƒì„¸ëŠ” BigQuery/Grafana
   ```

4. **ì—°ì† ì‹¤íŒ¨ ê°ì§€**
   ```
   ë¬¸ì œ: ê°™ì€ ì—ëŸ¬ê°€ ë©°ì¹ ì§¸ ë°˜ë³µ
   ëŒ€ì‘:
   - DAG ìƒíƒœ ì²´í¬ ì¶”ê°€
   - ì—°ì† ì‹¤íŒ¨ ì¼ìˆ˜ í‘œì‹œ
   - 3ì¼ ì´ìƒ ì‹¤íŒ¨ ì‹œ ê°•ì¡°
   ```

**ê°œì„  í•­ëª© ìš°ì„ ìˆœìœ„** (ì‹¤ì œ ë°ì´í„° ë³´ê³  ê²°ì •):
- [ ] Noise í•„í„°ë§
- [ ] ì •ì±…ìƒ ì œì™¸ ê·œì¹™
- [ ] ì—°ì† ì‹¤íŒ¨ ê°ì§€
- [ ] LLM ì—ëŸ¬ ë¶„ë¥˜
- [ ] DAG ìƒíƒœ ì²´í¬

---

### Phase 3: Convert Config ë¶„ì„ ë„êµ¬ (í•„ìš”ì„± í™•ì¸ í›„)

**ë°°ê²½**:
- Convert config JSONì€ ë³µì¡í•˜ê³  ë¶„ì„ì´ ì–´ë ¤ì›€
- ì‹¤íŒ¨ ì›ì¸ íŒŒì•…ì„ ìœ„í•´ **config vs ì‹¤ì œ Excel ë¹„êµ** í•„ìš”
- ìˆ˜ë™ìœ¼ë¡œ í•˜ê¸°ì—” ì‹œê°„ì´ ë„ˆë¬´ ë§ì´ ê±¸ë¦¼

**Convert Configì˜ ë³µì¡ì„±**:
```json
// collector/c0159c00/eml_gi00_011.json ì˜ˆì‹œ
{
  "convert": {
    "ì•½íš¨": {  // ì‹œíŠ¸ëª… íŒ¨í„´ (ì •ê·œì‹ ê°€ëŠ¥)
      "TB01": {
        "header": true,
        "cols": "A:D",
        "rows": {
          "start": {
            "header_coordinate": "A",
            "expr": "êµ¬ë¶„",  // ì •ê·œì‹ìœ¼ë¡œ í—¤ë” ì°¾ê¸°
            "match": 1,
            "offset": 0
          },
          "end": {
            "header_name": "êµ¬ë¶„",
            "empty": true
          }
        }
      }
    }
  }
}
```

**ë¬¸ì œì **:
1. ì‹œíŠ¸ëª…ì´ ì •ê·œì‹ì´ë¼ ì–´ë–¤ ì´ë¦„ì„ ì°¾ëŠ”ì§€ ë¶ˆëª…í™•
2. í—¤ë” ì°¾ê¸° ë¡œì§ì´ ë³µì¡ (coordinate, expr, offset...)
3. ì‹¤íŒ¨ ì‹œ "ì–´ë””ê°€ ë¬¸ì œì¸ì§€" íŒŒì•… ì–´ë ¤ì›€
4. ê³¼ê±° ì„±ê³µ ì¼€ì´ìŠ¤ì™€ ë¹„êµí•˜ê³  ì‹¶ì€ë° ë°©ë²•ì´ ì—†ìŒ

---

### Convert Config Analyzer ì„¤ê³„

#### ëª©í‘œ
- Config JSONì„ **ì‹œê°í™”**í•˜ì—¬ ì´í•´í•˜ê¸° ì‰½ê²Œ
- ì‹¤íŒ¨ Excel vs ì„±ê³µ Excel **ì–‘ìª½ ë¹„êµ**
- **ìë™ìœ¼ë¡œ í•´ê²°ë°©ì•ˆ ì œì‹œ**

#### í•µì‹¬ ê¸°ëŠ¥

##### 1. Config ì‹œê°í™” (Human-Readable)

**ì…ë ¥**: `eml_rv56_021.json`
```json
{
  "convert": {
    "í‹°ì¼“íŒ…\\(.*\\)\\s*$": {
      "TB01": { ... }
    }
  }
}
```

**ì¶œë ¥**:
```
ğŸ“„ Convert Config: eml_rv56_021

ğŸ” ì‹œíŠ¸ ë§¤ì¹­ ê·œì¹™:
  íŒ¨í„´: 'í‹°ì¼“íŒ…\(.*\)\s*$' (ì •ê·œì‹)

  ì„¤ëª…:
  - "í‹°ì¼“íŒ…"ìœ¼ë¡œ ì‹œì‘
  - ê´„í˜¸() ì•ˆì— ì„ì˜ì˜ í…ìŠ¤íŠ¸ (.*)
  - ê³µë°±(\s*) í›„ ì¤„ ë($)

  ë§¤ì¹­ ì˜ˆì‹œ:
    âœ… "í‹°ì¼“íŒ…(11ì›”)"
    âœ… "í‹°ì¼“íŒ…(ë§¤ì¶œ) "
    âœ… "í‹°ì¼“íŒ…(ì¼ì¼ì§‘ê³„)  "
    âŒ "Sheet1"
    âŒ "í‹°ì¼“íŒ…"
    âŒ "í‹°ì¼“íŒ…ë°ì´í„°"

ğŸ“Š í…Œì´ë¸” ì¶”ì¶œ ê·œì¹™ (TB01):
  ì»¬ëŸ¼ ë²”ìœ„: A:D

  í—¤ë” ì°¾ê¸°:
    - A ì»¬ëŸ¼ì—ì„œ "êµ¬ë¶„" í…ìŠ¤íŠ¸ ê²€ìƒ‰
    - 1ë²ˆì§¸ ë§¤ì¹­ë˜ëŠ” í–‰
    - offset 0 (ë°”ë¡œ ê·¸ í–‰ë¶€í„°)

  ë°ì´í„° ë²”ìœ„:
    - ì‹œì‘: í—¤ë” í–‰
    - ë: "êµ¬ë¶„" ì»¬ëŸ¼ì´ ë¹„ì–´ìˆëŠ” í–‰ (ë¯¸í¬í•¨)
```

##### 2. ì‹¤íŒ¨ vs ì„±ê³µ Excel ë¹„êµ ëŒ€ì‹œë³´ë“œ

**ì›¹ UI ë ˆì´ì•„ì›ƒ**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Converter ì‹¤íŒ¨ ë¶„ì„: c8cd3500 - eml_rv56_021                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  âŒ ì‹¤íŒ¨ ì¼€ì´ìŠ¤          â”‚  âœ… ì„±ê³µ ì¼€ì´ìŠ¤ (ê³¼ê±°)   â”‚         â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
â”‚  â”‚  ğŸ“… 2025-11-07          â”‚  ğŸ“… 2025-10-15          â”‚         â”‚
â”‚  â”‚  gcs_path:              â”‚  gcs_path:              â”‚         â”‚
â”‚  â”‚  email/.../fcf7a5ae477c â”‚  email/.../a1b2c3d4     â”‚         â”‚
â”‚  â”‚                         â”‚                         â”‚         â”‚
â”‚  â”‚  ğŸ“Š Excel ì •ë³´:          â”‚  ğŸ“Š Excel ì •ë³´:          â”‚         â”‚
â”‚  â”‚  â”œâ”€ ì‹œíŠ¸ ëª©ë¡:           â”‚  â”œâ”€ ì‹œíŠ¸ ëª©ë¡:           â”‚         â”‚
â”‚  â”‚  â”‚  â€¢ Sheet1           â”‚  â”‚  â€¢ í‹°ì¼“íŒ…(11ì›”)      â”‚         â”‚
â”‚  â”‚  â”‚                     â”‚  â”‚                     â”‚         â”‚
â”‚  â”‚  â”œâ”€ [Sheet1] êµ¬ì¡°:      â”‚  â”œâ”€ [í‹°ì¼“íŒ…(11ì›”)] êµ¬ì¡°: â”‚         â”‚
â”‚  â”‚  â”‚  ì»¬ëŸ¼: ê°€ë™ì¼,       â”‚  â”‚  ì»¬ëŸ¼: ë‚ ì§œ, ë‹´ë‹¹ì, â”‚         â”‚
â”‚  â”‚  â”‚        ë‹´ë‹¹ì,       â”‚  â”‚        ë§¤ì¶œ, ë¹„ê³     â”‚         â”‚
â”‚  â”‚  â”‚        ê°€ë™ë¥         â”‚  â”‚                     â”‚         â”‚
â”‚  â”‚  â”‚  í–‰ ìˆ˜: 2           â”‚  â”‚  í–‰ ìˆ˜: 145         â”‚         â”‚
â”‚  â”‚  â”‚                     â”‚  â”‚                     â”‚         â”‚
â”‚  â”‚  â”‚  ë¯¸ë¦¬ë³´ê¸°:           â”‚  â”‚  ë¯¸ë¦¬ë³´ê¸°:           â”‚         â”‚
â”‚  â”‚  â”‚  [í…Œì´ë¸” í‘œì‹œ]       â”‚  â”‚  [í…Œì´ë¸” í‘œì‹œ]       â”‚         â”‚
â”‚  â”‚                         â”‚                         â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                 â”‚
â”‚  ğŸ“„ Convert Config (eml_rv56_021):                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  ì‹œíŠ¸ íŒ¨í„´: 'í‹°ì¼“íŒ…\(.*\)\s*$'                   â”‚           â”‚
â”‚  â”‚                                                 â”‚           â”‚
â”‚  â”‚  âœ… ì„±ê³µ ì¼€ì´ìŠ¤ ë§¤ì¹­:                            â”‚           â”‚
â”‚  â”‚     "í‹°ì¼“íŒ…(11ì›”)" â† íŒ¨í„´ ì¼ì¹˜                   â”‚           â”‚
â”‚  â”‚                                                 â”‚           â”‚
â”‚  â”‚  âŒ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë§¤ì¹­:                            â”‚           â”‚
â”‚  â”‚     "Sheet1" â† íŒ¨í„´ ë¶ˆì¼ì¹˜                       â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                 â”‚
â”‚  âš ï¸  ë¶ˆì¼ì¹˜ ì›ì¸ ë¶„ì„:                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  ë¬¸ì œ: ì‹œíŠ¸ ì´ë¦„ì´ íŒ¨í„´ê³¼ ë§ì§€ ì•ŠìŒ              â”‚           â”‚
â”‚  â”‚                                                 â”‚           â”‚
â”‚  â”‚  ConfigëŠ” "í‹°ì¼“íŒ…(...)" í˜•íƒœì˜ ì‹œíŠ¸ë¥¼ ì°¾ì§€ë§Œ     â”‚           â”‚
â”‚  â”‚  ì‹¤ì œ Excelì—ëŠ” "Sheet1"ë§Œ ì¡´ì¬                  â”‚           â”‚
â”‚  â”‚                                                 â”‚           â”‚
â”‚  â”‚  ê³¼ê±° ì„±ê³µ ì¼€ì´ìŠ¤ì—ì„œëŠ” "í‹°ì¼“íŒ…(11ì›”)" ì‹œíŠ¸ê°€     â”‚           â”‚
â”‚  â”‚  ì¡´ì¬í•˜ì—¬ ì •ìƒ ì²˜ë¦¬ë¨                            â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                 â”‚
â”‚  ğŸ’¡ ì¶”ì²œ í•´ê²°ë°©ì•ˆ:                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  1. Convert Config ìˆ˜ì • (ê¶Œì¥)                   â”‚           â”‚
â”‚  â”‚     ì‹œíŠ¸ íŒ¨í„´ì— "Sheet1" ì¶”ê°€:                   â”‚           â”‚
â”‚  â”‚     {                                           â”‚           â”‚
â”‚  â”‚       "convert": {                              â”‚           â”‚
â”‚  â”‚         "í‹°ì¼“íŒ…\\(.*\\)\\s*$": { ... },           â”‚           â”‚
â”‚  â”‚         "Sheet1": { ... }  â† ì¶”ê°€               â”‚           â”‚
â”‚  â”‚       }                                         â”‚           â”‚
â”‚  â”‚     }                                           â”‚           â”‚
â”‚  â”‚     [Config ìˆ˜ì •í•˜ê¸°] [í…ŒìŠ¤íŠ¸]                   â”‚           â”‚
â”‚  â”‚                                                 â”‚           â”‚
â”‚  â”‚  2. Excel íŒŒì¼ ì¬ìš”ì²­                            â”‚           â”‚
â”‚  â”‚     ê³ ê°ì‚¬ì— "í‹°ì¼“íŒ…(ì›”)" í˜•íƒœë¡œ ì‹œíŠ¸ëª… ë³€ê²½ ìš”ì²­ â”‚           â”‚
â”‚  â”‚                                                 â”‚           â”‚
â”‚  â”‚  3. ì†ŒìŠ¤ ìë™ ë³€í™˜ ê·œì¹™ ì¶”ê°€                      â”‚           â”‚
â”‚  â”‚     "Sheet1" â†’ "í‹°ì¼“íŒ…(í˜„ì¬ì›”)" ìë™ ë³€í™˜        â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### 3. Config Editor & Tester

**ê¸°ëŠ¥**:
- Config JSON ì§ì ‘ í¸ì§‘ (ì›¹ ì—ë””í„°)
- ì‹¤ì‹œê°„ ë¬¸ë²• ê²€ì¦
- í…ŒìŠ¤íŠ¸ Excel ì—…ë¡œë“œí•´ì„œ ë¯¸ë¦¬ ê²€ì¦
- ë³€ê²½ì‚¬í•­ Git ì»¤ë°‹/PR ìë™ ìƒì„±

**ì›Œí¬í”Œë¡œìš°**:
```
1. ì‹¤íŒ¨ ì¼€ì´ìŠ¤ í™•ì¸
   â†“
2. ë¹„êµ ëŒ€ì‹œë³´ë“œì—ì„œ ì›ì¸ íŒŒì•…
   â†“
3. Config ìˆ˜ì •ì•ˆ í™•ì¸
   â†“
4. ì›¹ ì—ë””í„°ì—ì„œ ìˆ˜ì •
   â†“
5. í…ŒìŠ¤íŠ¸ Excelë¡œ ê²€ì¦
   â†“
6. í†µê³¼í•˜ë©´ ìë™ìœ¼ë¡œ PR ìƒì„±
   â†“
7. ë¦¬ë·° í›„ ë¨¸ì§€
```

---

#### ê¸°ìˆ  ìŠ¤íƒ ì œì•ˆ

##### ì›¹ ëŒ€ì‹œë³´ë“œ
```python
# Option 1: Streamlit (ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…)
streamlit run app.py
# ì¥ì : ë¹ ë¥´ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŒ
# ë‹¨ì : ì»¤ìŠ¤í„°ë§ˆì´ì§• ì œí•œ

# Option 2: Flask + React (ë³¸ê²© ê°œë°œ)
# ì¥ì : ììœ ë¡œìš´ UI, í™•ì¥ì„±
# ë‹¨ì : ê°œë°œ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼

# ì¶”ì²œ: Streamlitìœ¼ë¡œ ì‹œì‘ â†’ í•„ìš”í•˜ë©´ Flask ì „í™˜
```

##### Excel ë¹„êµ ë¡œì§
```python
from google.cloud import storage, bigquery
import pandas as pd
import json
import re
from difflib import SequenceMatcher

class ConvertConfigAnalyzer:
    """Convert Configì™€ Excel íŒŒì¼ ë¶„ì„"""

    def __init__(self):
        self.storage_client = storage.Client()
        self.bq_client = bigquery.Client()

    def analyze_failure(self, customer_code, config_id, failed_gcs_path):
        """
        ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„

        1. Failed Excel ì½ê¸°
        2. Convert Config ì½ê¸°
        3. ê³¼ê±° ì„±ê³µ ì¼€ì´ìŠ¤ ì°¾ê¸°
        4. ë¹„êµ ë¶„ì„
        5. í•´ê²°ë°©ì•ˆ ì œì‹œ
        """
        # 1. ì‹¤íŒ¨ Excel
        failed_excel = self.get_excel(customer_code, failed_gcs_path)

        # 2. Config
        config = self.get_config(customer_code, config_id)

        # 3. ê³¼ê±° ì„±ê³µ ì¼€ì´ìŠ¤ (BigQuery ì¡°íšŒ)
        success_case = self.find_last_success(customer_code, config_id)
        success_excel = self.get_excel(customer_code, success_case['gcs_path'])

        # 4. ë¹„êµ
        comparison = self.compare_excels(
            failed_excel,
            success_excel,
            config
        )

        # 5. í•´ê²°ë°©ì•ˆ
        solutions = self.suggest_solutions(comparison)

        return {
            'failed': failed_excel,
            'success': success_excel,
            'config': config,
            'comparison': comparison,
            'solutions': solutions
        }

    def compare_excels(self, failed, success, config):
        """Excel íŒŒì¼ ë¹„êµ"""
        sheet_pattern = list(config['convert'].keys())[0]

        return {
            'sheet_names': {
                'failed': failed.sheet_names,
                'success': success.sheet_names,
                'pattern': sheet_pattern,
                'failed_matched': self.match_sheet(failed.sheet_names, sheet_pattern),
                'success_matched': self.match_sheet(success.sheet_names, sheet_pattern)
            },
            'structure': {
                # ì»¬ëŸ¼, í–‰ ìˆ˜ ë¹„êµ
            },
            'content_sample': {
                # ì²« Ní–‰ ë¹„êµ
            }
        }

    def match_sheet(self, sheet_names, pattern):
        """ì‹œíŠ¸ëª… íŒ¨í„´ ë§¤ì¹­ ê²€ì¦"""
        for sheet in sheet_names:
            if re.match(pattern, sheet):
                return sheet
        return None

    def suggest_solutions(self, comparison):
        """ìë™ í•´ê²°ë°©ì•ˆ ì œì‹œ"""
        solutions = []

        # ì‹œíŠ¸ ë¶ˆì¼ì¹˜
        if not comparison['sheet_names']['failed_matched']:
            solutions.append({
                'type': 'config_update',
                'priority': 'high',
                'description': 'Configì— ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì‹œíŠ¸ëª… ì¶”ê°€',
                'code': self.generate_config_patch(comparison)
            })

            solutions.append({
                'type': 'request_fix',
                'priority': 'medium',
                'description': 'ê³ ê°ì‚¬ì— ì˜¬ë°”ë¥¸ ì‹œíŠ¸ëª… ìš”ì²­'
            })

        return solutions
```

##### Config ì‹œê°í™”
```python
class ConfigVisualizer:
    """Config JSONì„ Human-Readableë¡œ ë³€í™˜"""

    def visualize(self, config):
        """Configë¥¼ ì„¤ëª… í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        output = []

        for sheet_pattern, tables in config['convert'].items():
            output.append(f"ğŸ” ì‹œíŠ¸ ë§¤ì¹­ ê·œì¹™:")
            output.append(f"  íŒ¨í„´: '{sheet_pattern}'")

            # ì •ê·œì‹ ì„¤ëª…
            if self.is_regex(sheet_pattern):
                explanation = self.explain_regex(sheet_pattern)
                output.append(f"\n  ì„¤ëª…:")
                output.extend([f"  - {line}" for line in explanation])

                # ë§¤ì¹­ ì˜ˆì‹œ
                examples = self.generate_examples(sheet_pattern)
                output.append(f"\n  ë§¤ì¹­ ì˜ˆì‹œ:")
                for ex, matches in examples:
                    symbol = "âœ…" if matches else "âŒ"
                    output.append(f"    {symbol} \"{ex}\"")

            # í…Œì´ë¸” ê·œì¹™
            for table_name, table_config in tables.items():
                output.append(f"\nğŸ“Š í…Œì´ë¸”: {table_name}")
                # ... (ìƒëµ)

        return "\n".join(output)

    def explain_regex(self, pattern):
        """ì •ê·œì‹ì„ ìì—°ì–´ë¡œ ì„¤ëª…"""
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì„¤ëª… ìƒì„±
        explanations = []

        if pattern.startswith("^"):
            explanations.append("ì¤„ ì‹œì‘ë¶€í„°")
        if pattern.endswith("$"):
            explanations.append("ì¤„ ëê¹Œì§€")
        if r"\(" in pattern:
            explanations.append("ê´„í˜¸() í¬í•¨")
        if ".*" in pattern:
            explanations.append("ì„ì˜ì˜ í…ìŠ¤íŠ¸")
        if r"\s*" in pattern:
            explanations.append("ê³µë°± í—ˆìš©")

        return explanations
```

---

#### êµ¬í˜„ ìš°ì„ ìˆœìœ„

**ì§€ê¸ˆ ë‹¹ì¥ í•„ìš”í•œê°€?**
- Phase 1, 2 ìš´ì˜í•´ë³´ê³  **ì‹¤ì œë¡œ Config ë¶„ì„ì´ ìì£¼ í•„ìš”í•œì§€** í™•ì¸
- ë§Œì•½ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ê°€ ëŒ€ë¶€ë¶„ "ì‹œíŠ¸ëª… ë¶ˆì¼ì¹˜" ê°™ì€ ë‹¨ìˆœ ë¬¸ì œë©´ â†’ ê¼­ í•„ìš”
- ë³µì¡í•œ ì—ëŸ¬ê°€ ë§ì§€ ì•Šìœ¼ë©´ â†’ ë‚˜ì¤‘ì—

**êµ¬í˜„í•œë‹¤ë©´ ìˆœì„œ**:
1. **Config ì‹œê°í™”** (ê°€ì¥ ì‰½ê³  ìœ ìš©)
2. **ì‹¤íŒ¨ vs ì„±ê³µ ë¹„êµ** (í•µì‹¬ ê¸°ëŠ¥)
3. **Config Editor** (ì—¬ìœ  ìˆìœ¼ë©´)

---

### Phase 4: ê³ ë„í™” (ì„ íƒì‚¬í•­)

**í•„ìš”ì„±ì´ ì…ì¦ëœ í›„ì—ë§Œ êµ¬í˜„**:

- [ ] LLM ê¸°ë°˜ ì—ëŸ¬ ì›ì¸ ë¶„ì„ (Phase 2 + Excel ì½ê¸°)
- [ ] Convert Config ìë™ ìƒì„± (Excel ì—…ë¡œë“œ â†’ Config ì¶”ì²œ)
- [ ] ì‹¤ì‹œê°„ ì•Œë¦¼ (Slack/Teamsì— ì¦‰ì‹œ ì•Œë¦¼)
- [ ] Grafana ëŒ€ì‹œë³´ë“œ í†µí•©
- [ ] ê³¼ê±° íŠ¸ë Œë“œ ë¶„ì„ (ì—ëŸ¬ ì¦ê°€ ì¶”ì„¸ ê°ì§€)
- [ ] ìë™ ë³µêµ¬ ì‹œë„ (Config ìë™ ìˆ˜ì • PR)

---

## êµ¬í˜„ ì‹œ ì£¼ì˜ì‚¬í•­

### 1. ì‹¤íŒ¨ ê±´ìˆ˜ê°€ ë„ˆë¬´ ë§ì„ ê²½ìš°

**ì˜ˆìƒ ì‹œë‚˜ë¦¬ì˜¤**:
```
ë§¤ì¼ 500ê±´ ì‹¤íŒ¨ â†’ Teams ë©”ì‹œì§€ ë„ˆë¬´ ê¹€
```

**ëŒ€ì‘ ì „ëµ** (ìš°ì„ ìˆœìœ„ ìˆœ):

#### Level 1: ê¸°ë³¸ í•„í„°ë§ (Phase 1ë¶€í„°)
```python
# ìƒìœ„ Nê°œë§Œ í‘œì‹œ
MAX_CUSTOMERS = 10

# ë‚˜ë¨¸ì§€ëŠ” ìš”ì•½
if len(customers) > MAX_CUSTOMERS:
    shown = customers[:MAX_CUSTOMERS]
    hidden_count = len(customers) - MAX_CUSTOMERS
    message += f"\nâš ï¸ ê¸°íƒ€ {hidden_count}ê°œ ê³ ê°ì‚¬ (BigQueryì—ì„œ í™•ì¸)"
```

#### Level 2: ì •ì±…ìƒ ì œì™¸ (Phase 2)
```python
# ìë™ ì œì™¸
POLICY_EXCLUSIONS = [
    r"xlsb.*not supported",
    r"encrypted",
    r"File size exceeds"
]

# ë©”ì‹œì§€ì—ì„œ ë¶„ë¦¬
excluded_count = ...
message += f"\nğŸ“‹ ì •ì±…ìƒ ì œì™¸: {excluded_count}ê±´ (xlsb, ì•”í˜¸í™” ë“±)"
```

#### Level 3: Noise í•„í„°ë§ (Phase 2)
```python
# ê°™ì€ ë‚  ì„±ê³µë¥  80% ì´ìƒì´ë©´ Noise
def is_noise(failure_record):
    same_day_success_rate = calculate_success_rate(
        failure_record['customer_code'],
        failure_record['convert_config_id'],
        failure_record['date']
    )
    return same_day_success_rate >= 0.8

# NoiseëŠ” ë³„ë„ ì„¹ì…˜
message += f"\nğŸ”‡ Noise: {noise_count}ê±´ (ê°™ì€ ë‚  ëŒ€ë¶€ë¶„ ì„±ê³µ)"
```

#### Level 4: ìš°ì„ ìˆœìœ„ ì •ë ¬ (Phase 2)
```python
# ì¤‘ìš”ë„ ìˆœ ì •ë ¬
def calculate_priority(failure):
    score = 0

    # ì—°ì† ì‹¤íŒ¨ì¼ìˆ˜ ë†’ìœ¼ë©´ ìš°ì„ 
    score += failure['consecutive_days'] * 10

    # RPA/Boardê°€ NonRPAë³´ë‹¤ ìš°ì„ 
    if failure['source_type'] in ['rpa', 'board']:
        score += 5

    # ì‹¤íŒ¨ í…Œì´ë¸” ìˆ˜ ë§ìœ¼ë©´ ìš°ì„ 
    score += failure['table_count']

    return score

customers.sort(key=calculate_priority, reverse=True)
```

### 2. Config íŒŒì¼ ë¶„ì„ ë³µì¡ë„

**ë¬¸ì œì **:
- Config JSONì´ ë³µì¡í•¨ (ì •ê·œì‹, ì¤‘ì²© êµ¬ì¡°)
- ëª¨ë“  ì¼€ì´ìŠ¤ë¥¼ ìë™ ë¶„ì„í•˜ê¸° ì–´ë ¤ì›€

**ëŒ€ì‘**:
```python
# ì™„ë²½í•œ ìë™ ë¶„ì„ X
# â†’ ì¼ë‹¨ "ì‹œíŠ¸ëª… ë¶ˆì¼ì¹˜" ê°™ì€ ëª…í™•í•œ ì¼€ì´ìŠ¤ë§Œ ì²˜ë¦¬

def analyze_config_error(error_message, config):
    """ê°„ë‹¨í•œ ì¼€ì´ìŠ¤ë§Œ ìë™ ë¶„ì„"""

    # ì‹œíŠ¸ ì—ëŸ¬ë§Œ ì²˜ë¦¬
    if "not found matched sheet" in error_message:
        return analyze_sheet_mismatch(error_message, config)

    # í—¤ë” ì—ëŸ¬ëŠ” ë³µì¡í•´ì„œ ì¼ë‹¨ íŒ¨ìŠ¤
    elif "row not found" in error_message:
        return {
            'analysis': 'manual_required',
            'message': 'í—¤ë” ì—ëŸ¬ëŠ” ìˆ˜ë™ ë¶„ì„ í•„ìš”'
        }

    # ë‚˜ë¨¸ì§€ë„ íŒ¨ìŠ¤
    else:
        return {
            'analysis': 'unknown',
            'message': 'Config Analyzer í•„ìš”'
        }
```

### 3. ê³¼ê±° ì„±ê³µ ì¼€ì´ìŠ¤ ì°¾ê¸°

**ë¬¸ì œ**:
- ê°™ì€ configë¡œ ì„±ê³µí•œ ì¼€ì´ìŠ¤ê°€ ì—†ì„ ìˆ˜ë„ ìˆìŒ
- ë„ˆë¬´ ì˜¤ë˜ëœ ì„±ê³µ ì¼€ì´ìŠ¤ëŠ” ì˜ë¯¸ ì—†ìŒ

**ëŒ€ì‘**:
```python
def find_last_success(customer_code, config_id, max_days=30):
    """
    ìµœê·¼ Nì¼ ì´ë‚´ ì„±ê³µ ì¼€ì´ìŠ¤ ì°¾ê¸°

    ì—†ìœ¼ë©´ None ë°˜í™˜
    """
    query = f"""
    SELECT gcs_path, created_at
    FROM `convert_job_history`
    WHERE customer_code = '{customer_code}'
      AND convert_config_id = '{config_id}'
      AND status = 'success'
      AND DATE(created_at) >= DATE_SUB(CURRENT_DATE(), INTERVAL {max_days} DAY)
    ORDER BY created_at DESC
    LIMIT 1
    """

    result = bq_client.query(query).result()

    if result.total_rows == 0:
        return None  # ì„±ê³µ ì¼€ì´ìŠ¤ ì—†ìŒ

    return next(result)
```

**UI ì²˜ë¦¬**:
```
âœ… ì„±ê³µ ì¼€ì´ìŠ¤ (ê³¼ê±°)
  âš ï¸ ìµœê·¼ 30ì¼ ë‚´ ì„±ê³µ ê¸°ë¡ ì—†ìŒ

  ì´ ConfigëŠ” ê³„ì† ì‹¤íŒ¨ ì¤‘ì´ê±°ë‚˜
  ìƒˆë¡œ ì¶”ê°€ëœ Configì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```

---

## ì„¤ê³„ ì›ì¹™

### 1. ì ì§„ì  ê°œì„  (Incremental Improvement)
- ì™„ë²½í•œ ì‹œìŠ¤í…œë³´ë‹¤ **ì‘ë™í•˜ëŠ” ì‹œìŠ¤í…œ**
- ì‹¤ì œ ë°ì´í„° ë³´ê³  ê°œì„ 
- ë¶ˆí•„ìš”í•œ ê¸°ëŠ¥ ë§Œë“¤ì§€ ì•Šê¸°

### 2. ì‹¤ìš©ì£¼ì˜ (Pragmatism)
- LLM: í•„ìš”í•˜ë©´ ì“°ë˜, ê·œì¹™ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥í•˜ë©´ ê·œì¹™ ì‚¬ìš©
- Config ë¶„ì„: 100% ìë™í™” ë¶ˆê°€ëŠ¥ â†’ ëª…í™•í•œ ì¼€ì´ìŠ¤ë§Œ
- ëŒ€ì‹œë³´ë“œ: Streamlitìœ¼ë¡œ ë¹ ë¥´ê²Œ ì‹œì‘

### 3. í™•ì¥ ê°€ëŠ¥ì„± (Scalability)
- ë‚˜ì¤‘ì— ê¸°ëŠ¥ ì¶”ê°€í•˜ê¸° ì‰½ê²Œ ëª¨ë“ˆí™”
- Config ê¸°ë°˜ (í•˜ë“œì½”ë”© ìµœì†Œí™”)
- ê³ ê°ì‚¬ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì§€ì›

### 4. ìš´ì˜ ì¹œí™”ì  (Ops-Friendly)
- ì—ëŸ¬ ë‚˜ë„ ì¼ë‹¨ ë©”ì‹œì§€ëŠ” ë³´ëƒ„ (Fail-safe)
- ë¡œê·¸ ì˜ ë‚¨ê¸°ê¸° (ë””ë²„ê¹… ìš©ì´)
- BigQuery ë§í¬ë¡œ ì›ë³¸ ë°ì´í„° ì ‘ê·¼ ì‰½ê²Œ

---

## ë‹¤ìŒ ë…¼ì˜ í¬ì¸íŠ¸

### ê¸°ìˆ  ê²°ì • í•„ìš”

1. **Phase 1 ë°°í¬ ë°©ì‹**
   - Cloud Function? Cloud Run?
   - ì‹¤í–‰ ì£¼ê¸°: ë§¤ì¼ ì•„ì¹¨ ëª‡ ì‹œ?

2. **ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ**
   - ìµœëŒ€ í‘œì‹œ ê³ ê°ì‚¬ ìˆ˜: 10ê°œ? 20ê°œ?
   - í…Œì´ë¸” ìµœëŒ€ í–‰ ìˆ˜ëŠ”?

3. **BigQuery ë§í¬**
   - Looker Studio? ì§ì ‘ BigQuery ì½˜ì†”?
   - ë¯¸ë¦¬ ë§Œë“¤ì–´ë‘˜ ì¿¼ë¦¬ëŠ”?

4. **Noise íŒì • ê¸°ì¤€** (Phase 2)
   - ê°™ì€ ë‚  ì„±ê³µë¥ : 80%? 50%?
   - ê³ ê°ì‚¬ë³„ë¡œ ë‹¤ë¥´ê²Œ ì„¤ì •?

5. **ì—°ì† ì‹¤íŒ¨ ê¸°ì¤€** (Phase 2)
   - RPA/Board: ë©°ì¹ ë¶€í„° ê°•ì¡°?
   - DAG ìƒíƒœ ì²´í¬ í•„ìš”?

### êµ¬í˜„ ìš°ì„ ìˆœìœ„ í™•ì¸

- [ ] Phase 1 MVP ë¨¼ì € êµ¬í˜„? âœ…
- [ ] Config Analyzer í•„ìš”ì„± ìˆëŠ”ì§€ í™•ì¸? ğŸ¤”
- [ ] íŒŒì¼ëŸ¿ ê³ ê°ì‚¬ ì„ ì •?

### Convert Config Analyzer ê´€ë ¨

1. **ì •ë§ í•„ìš”í•œê°€?**
   - Phase 1, 2 ìš´ì˜ í›„ íŒë‹¨?
   - ì§€ê¸ˆ ë°”ë¡œ í•„ìš”?

2. **í•„ìš”í•˜ë‹¤ë©´ ë²”ìœ„ëŠ”?**
   - ì‹œê°í™”ë§Œ?
   - Excel ë¹„êµê¹Œì§€?
   - Editorê¹Œì§€?

3. **ê¸°ìˆ  ìŠ¤íƒ**
   - Streamlit? Flask?
   - ë°°í¬ëŠ” ì–´ë–»ê²Œ?

---

## Phase 1 êµ¬í˜„ ì•„í‚¤í…ì²˜

### ê¸°ìˆ  ìŠ¤íƒ (Tech Stack)

#### ì–¸ì–´ ë° ëŸ°íƒ€ì„
- **Python 3.9** âœ…
  - airflow_dag_monitorì™€ ë™ì¼
  - GCP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì •ì  ì§€ì›
  - íƒ€ì… íŒíŒ… ì§€ì› (Python 3.9+)

#### GCP ì„œë¹„ìŠ¤
- **Cloud Run Job** âœ…
  - ë°°ì¹˜ ì‘ì—…ì— ìµœì í™”
  - ì‹¤í–‰ ì‹œê°„ ì œí•œ ì—†ìŒ (ìµœëŒ€ 60ë¶„)
  - ë¦¬ì†ŒìŠ¤ ìœ ì—°í•˜ê²Œ ì¡°ì • ê°€ëŠ¥
  - ë¡œì»¬ Docker í…ŒìŠ¤íŠ¸ ê°€ëŠ¥

- **Cloud Scheduler** âœ…
  - Cron ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§
  - Cloud Run Job íŠ¸ë¦¬ê±°
  - Timezone ì§€ì› (Asia/Seoul)

- **BigQuery** âœ…
  - `convert_job_history` í…Œì´ë¸” ì¡°íšŒ
  - ë¹ ë¥¸ ë¶„ì„ ì¿¼ë¦¬

- **Firestore** âœ…
  - ê³ ê°ì‚¬ ì •ë³´ (customer names)
  - env í•„í„°ë§ (ops/dev)

- **Cloud Logging** âœ…
  - êµ¬ì¡°í™”ëœ ë¡œê·¸
  - ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

- **Secret Manager** âœ… (ê¶Œì¥)
  - Teams Webhook URL ë³´ì•ˆ ì €ì¥

#### Python ë¼ì´ë¸ŒëŸ¬ë¦¬

**í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬**:
```python
# GCP í´ë¼ì´ì–¸íŠ¸
google-cloud-bigquery==3.11.0      # BigQuery ì¡°íšŒ
google-cloud-firestore==2.11.0     # Firestore ì¡°íšŒ

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
requests==2.31.0                   # Teams webhook í˜¸ì¶œ
pytz==2023.3                       # Timezone (KST)
```

**ì„ íƒ ë¼ì´ë¸ŒëŸ¬ë¦¬** (Phase 2+):
```python
# LLM ì—ëŸ¬ ë¶„ì„ (Phase 2)
openai==1.3.0                      # OpenAI API
# or
google-cloud-aiplatform==1.38.0    # Vertex AI

# Excel íŒŒì¼ ì½ê¸° (Phase 3)
pandas==2.1.0                      # Excel íŒŒì‹±
openpyxl==3.1.2                    # xlsx ì§€ì›
google-cloud-storage==2.10.0       # GCS íŒŒì¼ ì½ê¸°
```

#### ì»¨í…Œì´ë„ˆ
- **Docker** âœ…
  - Python 3.9-slim ë² ì´ìŠ¤ ì´ë¯¸ì§€
  - Multi-stage build ë¶ˆí•„ìš” (ê°„ë‹¨í•œ êµ¬ì¡°)

- **GCR (Google Container Registry)** âœ…
  - Docker ì´ë¯¸ì§€ ì €ì¥ì†Œ
  - Cloud Run Jobê³¼ í†µí•©

#### ë©”ì‹œì§•
- **Microsoft Teams Webhook** âœ…
  - Incoming Webhook ì»¤ë„¥í„°
  - JSON payload ì „ì†¡

#### ê°œë°œ ë„êµ¬
- **Git** âœ…
  - ë²„ì „ ê´€ë¦¬
  - ì½”ë“œ ë¦¬ë·°

- **VS Code** (ê¶Œì¥)
  - Python í™•ì¥
  - Docker í™•ì¥

---

### ê¸°ìˆ  ì„ íƒ ê·¼ê±°

#### 1. ì™œ Python 3.9?

**ì„ íƒ ì´ìœ **:
- airflow_dag_monitorì™€ ë™ì¼ ë²„ì „ (ì¼ê´€ì„±)
- GCP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì •ì  ì§€ì›
- íƒ€ì… íŒíŒ… ì§€ì›ìœ¼ë¡œ ì½”ë“œ í’ˆì§ˆ í–¥ìƒ

**ëŒ€ì•ˆ ê³ ë ¤**:
- âŒ Python 3.11+: GCP ì¼ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì§€ì›
- âŒ Python 3.7: ë„ˆë¬´ ì˜¤ë˜ë¨, EOL ì„ë°•
- âœ… Python 3.9: ì•ˆì •ì„±ê³¼ ìµœì‹  ê¸°ëŠ¥ ê· í˜•

---

#### 2. ì™œ Cloud Run Job?

**ì„ íƒ ì´ìœ **:
- âœ… **ì¼ê´€ì„±**: airflow_dag_monitorì™€ ë™ì¼ íŒ¨í„´
- âœ… **ì‹¤í–‰ ì‹œê°„**: ìµœëŒ€ 60ë¶„ (Cloud Functionì€ 9ë¶„)
- âœ… **ë¦¬ì†ŒìŠ¤**: ë©”ëª¨ë¦¬/CPU ìœ ì—°í•˜ê²Œ ì¡°ì •
- âœ… **ë””ë²„ê¹…**: ë¡œì»¬ Docker í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
- âœ… **ë¹„ìš©**: ì‹¤í–‰ ì‹œê°„ë§Œí¼ë§Œ ê³¼ê¸ˆ

**ëŒ€ì•ˆ ë¹„êµ**:

| íŠ¹ì„± | Cloud Function | Cloud Run Job | GKE CronJob |
|------|---------------|---------------|-------------|
| ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ | 9ë¶„ | 60ë¶„ | ì œí•œ ì—†ìŒ |
| ì„¤ì • ë³µì¡ë„ | ê°„ë‹¨ | ê°„ë‹¨ | ë³µì¡ |
| ë¡œì»¬ í…ŒìŠ¤íŠ¸ | ì–´ë ¤ì›€ | ì‰¬ì›€ | ì‰¬ì›€ |
| ë¹„ìš© | ì €ë ´ | ì¤‘ê°„ | ë¹„ìŒˆ |
| ìœ ì§€ë³´ìˆ˜ | ì‰¬ì›€ | ì‰¬ì›€ | ì–´ë ¤ì›€ |

**ê²°ë¡ **: Cloud Run Jobì´ ìµœì  âœ…

---

#### 3. ì™œ Cloud Scheduler?

**ì„ íƒ ì´ìœ **:
- âœ… GCP ë„¤ì´í‹°ë¸Œ (í†µí•© ìš©ì´)
- âœ… Cron í‘œí˜„ì‹ ì§€ì›
- âœ… Timezone ì§€ì› (Asia/Seoul)
- âœ… Cloud Run Job ì§ì ‘ íŠ¸ë¦¬ê±°
- âœ… ë¬´ë£Œ (ì›” 3ê°œê¹Œì§€)

**ëŒ€ì•ˆ ë¹„êµ**:

| ì„œë¹„ìŠ¤ | ì¥ì  | ë‹¨ì  |
|--------|------|------|
| Cloud Scheduler | GCP í†µí•©, ê°„ë‹¨ | N/A |
| Airflow | ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° | ì˜¤ë²„í‚¬, ê´€ë¦¬ ë³µì¡ |
| Cron (VM) | ìœ ì—° | VM ê´€ë¦¬ í•„ìš” |

**ê²°ë¡ **: Cloud Schedulerê°€ ìµœì  âœ…

---

#### 4. ì™œ BigQuery?

**ì„ íƒ ì´ìœ **:
- âœ… ì´ë¯¸ `convert_job_history` í…Œì´ë¸” ì¡´ì¬
- âœ… ë¹ ë¥¸ ë¶„ì„ ì¿¼ë¦¬ (íŒŒí‹°ì…˜ í…Œì´ë¸”)
- âœ… SQL ì¹œìˆ™í•¨
- âœ… ì„œë²„ë¦¬ìŠ¤ (ê´€ë¦¬ ë¶ˆí•„ìš”)

**ì‚¬ìš© íŒ¨í„´**:
```sql
-- ë‹¨ì¼ ë‚ ì§œ ì¡°íšŒ (ë§¤ìš° ë¹ ë¦„)
WHERE DATE(created_at, 'Asia/Seoul') = '2025-11-07'
  AND status = 'fail'
  AND env != 'dev'
```

---

#### 5. ì™œ Firestore?

**ì„ íƒ ì´ìœ **:
- âœ… ì´ë¯¸ ê³ ê°ì‚¬ ì •ë³´ ì €ì¥ ì¤‘
- âœ… NoSQLë¡œ ìœ ì—°í•œ ìŠ¤í‚¤ë§ˆ
- âœ… env í•„í„°ë§ í•„ìš” (BigQueryì—ë„ ìˆì§€ë§Œ customer_name ë§¤í•‘ í•„ìš”)

**ì‚¬ìš© íŒ¨í„´**:
```python
# env != 'dev' ê³ ê°ì‚¬ í•„í„°ë§
# customer_code â†’ customer_name ë§¤í•‘
```

**ëŒ€ì•ˆ ê³ ë ¤**:
- BigQueryë§Œ ì‚¬ìš©?
  - ì¥ì : ë‹¨ì¼ ë°ì´í„° ì†ŒìŠ¤
  - ë‹¨ì : customer_nameì´ ì¤‘ë³µ ì €ì¥ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ
  - ê²°ë¡ : Firestoreë¥¼ source of truthë¡œ ì‚¬ìš©

---

#### 6. ì™œ Teams Webhook?

**ì„ íƒ ì´ìœ **:
- âœ… ì´ë¯¸ airflow_dag_monitorì—ì„œ ì‚¬ìš© ì¤‘
- âœ… ì„¤ì • ê°„ë‹¨ (Incoming Webhookë§Œ)
- âœ… ì½”ë“œ ë‹¨ìˆœ (HTTP POSTë§Œ)

**ì‚¬ìš© íŒ¨í„´**:
```python
import requests

payload = {"text": message}
requests.post(WEBHOOK_URL, json=payload)
```

**ëŒ€ì•ˆ ê³ ë ¤**:
- Slack: ì¶”ê°€ í†µí•© í•„ìš”
- Email: í¬ë§·íŒ… ì œí•œì 
- PubSub: ì˜¤ë²„í‚¬

---

#### 7. ì™œ Docker?

**ì„ íƒ ì´ìœ **:
- âœ… Cloud Run Job ìš”êµ¬ì‚¬í•­
- âœ… ë¡œì»¬ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
- âœ… ì˜ì¡´ì„± ê²©ë¦¬
- âœ… ì¬í˜„ ê°€ëŠ¥í•œ í™˜ê²½

**ë² ì´ìŠ¤ ì´ë¯¸ì§€**:
```dockerfile
FROM python:3.9-slim  # âœ… ì‘ì€ í¬ê¸°, ë¹ ë¥¸ ë¹Œë“œ
```

**ëŒ€ì•ˆ**:
- `python:3.9`: ë„ˆë¬´ í¼ (900MB)
- `python:3.9-alpine`: ì¼ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¹Œë“œ ë¬¸ì œ
- `python:3.9-slim`: ì ì ˆí•œ í¬ê¸° (150MB) âœ…

---

#### 8. Phase 2+ ê¸°ìˆ  ìŠ¤íƒ

**LLM ì—ëŸ¬ ë¶„ì„** (Phase 2):

**Option A: OpenAI API** (ê¶Œì¥)
```python
import openai

# ì¥ì :
# - API ê°„ë‹¨
# - GPT-4o-mini ì €ë ´ ($0.15/1M tokens)
# - ë¹ ë¥¸ ì‘ë‹µ

# ë‹¨ì :
# - ì™¸ë¶€ ì„œë¹„ìŠ¤ (GCP ë°–)
# - ë°ì´í„° ì™¸ë¶€ ì „ì†¡
```

**Option B: Vertex AI (PaLM/Gemini)**
```python
from google.cloud import aiplatform

# ì¥ì :
# - GCP ë„¤ì´í‹°ë¸Œ
# - ë°ì´í„°ê°€ GCP ë‚´ë¶€ì—ë§Œ
# - í†µí•© ê²°ì œ

# ë‹¨ì :
# - API ë³µì¡
# - ë¹„ìš©ì´ ë” ë†’ì„ ìˆ˜ ìˆìŒ
# - ì‘ë‹µ ì†ë„ ëŠë¦´ ìˆ˜ ìˆìŒ
```

**ì¶”ì²œ**: OpenAI API (ê°„ë‹¨, ì €ë ´, ë¹ ë¦„) âœ…

---

**Excel íŒŒì¼ ë¶„ì„** (Phase 3):

```python
# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬
google-cloud-storage==2.10.0   # GCS íŒŒì¼ ì½ê¸°
pandas==2.1.0                  # Excel íŒŒì‹±
openpyxl==3.1.2                # xlsx ì§€ì›
xlrd==2.0.1                    # xls ì§€ì› (í•„ìš”ì‹œ)
```

**ì‚¬ìš© íŒ¨í„´**:
```python
from google.cloud import storage
import pandas as pd
import io

# GCSì—ì„œ Excel ì½ê¸°
storage_client = storage.Client()
bucket = storage_client.bucket("hyperlounge-c8cd3500")
blob = bucket.blob("email/s12bf560/crawl/fcf7a5ae477c")

excel_data = blob.download_as_bytes()
excel_file = pd.ExcelFile(io.BytesIO(excel_data))

# ì‹œíŠ¸ ëª©ë¡
print(excel_file.sheet_names)

# ì‹œíŠ¸ ì½ê¸°
df = pd.read_excel(excel_file, sheet_name="Sheet1")
```

---

**Convert Config Analyzer ì›¹ ëŒ€ì‹œë³´ë“œ** (Phase 3):

**Option A: Streamlit** (ê¶Œì¥ - ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…)
```python
streamlit==1.28.0

# ì¥ì :
# - ë¹ ë¥¸ ê°œë°œ (Pythonë§Œìœ¼ë¡œ UI)
# - ë°ì´í„° ì‹œê°í™” ê¸°ë³¸ ì œê³µ
# - ë°°í¬ ê°„ë‹¨

# ë‹¨ì :
# - ì»¤ìŠ¤í„°ë§ˆì´ì§• ì œí•œ
# - ì„±ëŠ¥ ì œí•œ (ëŒ€ìš©ëŸ‰ ë°ì´í„°)
```

**Option B: Flask + React** (ë³¸ê²© ê°œë°œ)
```python
flask==3.0.0
flask-cors==4.0.0

# + React í”„ë¡ íŠ¸ì—”ë“œ

# ì¥ì :
# - ì™„ì „í•œ ì»¤ìŠ¤í„°ë§ˆì´ì§•
# - í”„ë¡œë•ì…˜ ë ˆë²¨ ì„±ëŠ¥
# - í™•ì¥ì„±

# ë‹¨ì :
# - ê°œë°œ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼
# - í”„ë¡ íŠ¸ì—”ë“œ/ë°±ì—”ë“œ ë¶„ë¦¬ í•„ìš”
```

**ì¶”ì²œ ìˆœì„œ**:
1. Streamlitìœ¼ë¡œ í”„ë¡œí† íƒ€ì… âœ…
2. í•„ìš”í•˜ë©´ Flask + React ì „í™˜

---

### ì˜ì¡´ì„± ê´€ë¦¬

#### requirements.txt (Phase 1)

```txt
# GCP í´ë¼ì´ì–¸íŠ¸
google-cloud-bigquery==3.11.0
google-cloud-firestore==2.11.0

# HTTP ìš”ì²­
requests==2.31.0

# Timezone
pytz==2023.3

# ë¡œê¹… (ê¸°ë³¸ ì œê³µì´ì§€ë§Œ ëª…ì‹œ)
# logging (built-in)
```

#### requirements-dev.txt (ê°œë°œìš©)

```txt
# í…ŒìŠ¤íŠ¸
pytest==7.4.0
pytest-cov==4.1.0
pytest-mock==3.11.1

# ì½”ë“œ í’ˆì§ˆ
black==23.7.0          # í¬ë§·íŒ…
flake8==6.1.0          # ë¦°íŒ…
mypy==1.5.0            # íƒ€ì… ì²´í‚¹

# ë¡œì»¬ í…ŒìŠ¤íŠ¸
python-dotenv==1.0.0   # .env íŒŒì¼ ì§€ì›
```

---

### ë²„ì „ ê´€ë¦¬ ì „ëµ

**Python ë¼ì´ë¸ŒëŸ¬ë¦¬**:
- ë©”ì´ì €.ë§ˆì´ë„ˆ ë²„ì „ ê³ ì • (ì˜ˆ: `3.11.0`)
- ë³´ì•ˆ íŒ¨ì¹˜ëŠ” ìˆ˜ë™ ì—…ë°ì´íŠ¸
- Dependabot ì‚¬ìš© ì•ˆ í•¨ (ìˆ˜ë™ ê´€ë¦¬)

**Docker ë² ì´ìŠ¤ ì´ë¯¸ì§€**:
- `python:3.9-slim` (íƒœê·¸ ê³ ì • ì•ˆ í•¨)
- ë§¤ ë¹Œë“œë§ˆë‹¤ ìµœì‹  íŒ¨ì¹˜ ë°˜ì˜
- íŠ¹ì • ë²„ì „ ê³ ì •ì€ ì¬í˜„ì„± í•„ìš” ì‹œë§Œ

**ì´ìœ **:
- ìë™ ì—…ë°ì´íŠ¸ë³´ë‹¤ ì•ˆì •ì„± ìš°ì„ 
- í…ŒìŠ¤íŠ¸ í›„ ìˆ˜ë™ ì—…ë°ì´íŠ¸
- í”„ë¡œë•ì…˜ ì˜ˆìƒì¹˜ ëª»í•œ ë³€ê²½ ë°©ì§€

---

### ê°œë°œ í™˜ê²½ ì„¤ì •

#### ë¡œì»¬ ê°œë°œ

```bash
# 1. Python ê°€ìƒí™˜ê²½
python3.9 -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate     # Windows

# 2. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
pip install -r requirements-dev.txt  # ê°œë°œìš©

# 3. í™˜ê²½ë³€ìˆ˜ ì„¤ì • (.env)
cat > .env << EOF
GCP_PROJECT_ID=hyperlounge-dev
TEAMS_WEBHOOK_URL=https://...
LOG_LEVEL=DEBUG
EOF

# 4. ì‹¤í–‰
python -m converter_failure_monitor.main
```

#### Docker ê°œë°œ

```bash
# 1. ë¹Œë“œ
docker build -t converter-failure-monitor:dev .

# 2. ì‹¤í–‰ (í™˜ê²½ë³€ìˆ˜ ì£¼ì…)
docker run \
  --env-file .env \
  -v ~/.config/gcloud:/root/.config/gcloud \
  converter-failure-monitor:dev

# 3. ì¸í„°ë™í‹°ë¸Œ ë””ë²„ê¹…
docker run -it \
  --env-file .env \
  --entrypoint /bin/bash \
  converter-failure-monitor:dev
```

---

### ì½”ë“œ í’ˆì§ˆ ë„êµ¬

#### Black (ì½”ë“œ í¬ë§·íŒ…)

```bash
# ìë™ í¬ë§·íŒ…
black converter_failure_monitor/

# ì„¤ì • (pyproject.toml)
[tool.black]
line-length = 120
target-version = ['py39']
```

#### Flake8 (ë¦°íŒ…)

```bash
# ë¦°íŒ… ì²´í¬
flake8 converter_failure_monitor/

# ì„¤ì • (.flake8)
[flake8]
max-line-length = 120
exclude = __pycache__,.venv
ignore = E203,W503  # Black í˜¸í™˜
```

#### MyPy (íƒ€ì… ì²´í‚¹)

```bash
# íƒ€ì… ì²´í¬
mypy converter_failure_monitor/

# ì„¤ì • (pyproject.toml)
[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
```

**ì ìš© ì—¬ë¶€**: Phase 1ì€ ìŠ¤í‚µ, Phase 2ë¶€í„° ì ìš© ê³ ë ¤ âš ï¸

---

### CI/CD (ì„ íƒì‚¬í•­)

#### GitHub Actions (Phase 2+)

```yaml
# .github/workflows/deploy.yml
name: Deploy Converter Monitor

on:
  push:
    branches: [main]
    paths:
      - 'converter_failure_monitor/**'

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Cloud SDK
        uses: google-github-actions/setup-gcloud@v1

      - name: Deploy
        run: |
          cd converter_failure_monitor
          ./deploy.sh
```

**Phase 1**: ìˆ˜ë™ ë°°í¬ë¡œ ì‹œì‘ âœ…
**Phase 2+**: ìë™í™” ê³ ë ¤

---

### ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cloud Scheduler                          â”‚
â”‚              (ë§¤ì¼ ì•„ì¹¨ 8ì‹œ KST ì‹¤í–‰)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Trigger
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Cloud Run Job                             â”‚
â”‚              (converter_failure_monitor)                    â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  main.py                                            â”‚   â”‚
â”‚  â”‚  - ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜                               â”‚   â”‚
â”‚  â”‚  - ì „ì²´ íë¦„ ì œì–´                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚            â”‚                                                â”‚
â”‚            â”œâ”€â–º clients/                                     â”‚
â”‚            â”‚   â”œâ”€ bigquery_client.py (BigQuery ì¡°íšŒ)       â”‚
â”‚            â”‚   â””â”€ firestore_client.py (ê³ ê°ì‚¬ ì •ë³´)        â”‚
â”‚            â”‚                                                â”‚
â”‚            â”œâ”€â–º analyzer/                                    â”‚
â”‚            â”‚   â”œâ”€ classifier.py (ì—ëŸ¬ ë¶„ë¥˜)                â”‚
â”‚            â”‚   â”œâ”€ filter.py (í•„í„°ë§ ë¡œì§)                  â”‚
â”‚            â”‚   â””â”€ aggregator.py (ì§‘ê³„)                     â”‚
â”‚            â”‚                                                â”‚
â”‚            â”œâ”€â–º formatter/                                   â”‚
â”‚            â”‚   â””â”€ teams_formatter.py (Teams ë©”ì‹œì§€)        â”‚
â”‚            â”‚                                                â”‚
â”‚            â””â”€â–º config/                                      â”‚
â”‚                â”œâ”€ constants.py (ìƒìˆ˜)                       â”‚
â”‚                â””â”€ error_patterns.py (ì—ëŸ¬ ê·œì¹™)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”œâ”€â–º BigQuery (convert_job_history)
                     â”œâ”€â–º Firestore (customer info)
                     â””â”€â–º Teams Webhook
```

---

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
converter_failure_monitor/
â”œâ”€ Dockerfile                    # Cloud Run ë°°í¬ìš©
â”œâ”€ requirements.txt              # Python ì˜ì¡´ì„±
â”œâ”€ deploy.sh                     # ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€ README.md                     # ì„¤ëª…ì„œ
â”‚
â”œâ”€ main.py                       # ì§„ì…ì  (Cloud Runì´ ì‹¤í–‰)
â”‚
â”œâ”€ clients/                      # ì™¸ë¶€ ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ bigquery_client.py         # BigQuery ì¡°íšŒ
â”‚  â””â”€ firestore_client.py        # Firestore ì¡°íšŒ (ê³ ê°ì‚¬ ì •ë³´)
â”‚
â”œâ”€ analyzer/                     # ë¶„ì„ ë¡œì§
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ classifier.py              # ì—ëŸ¬ ë¶„ë¥˜ (ê·œì¹™ ê¸°ë°˜)
â”‚  â”œâ”€ filter.py                  # í•„í„°ë§ (ì •ì±… ì œì™¸, Noise - Phase 2)
â”‚  â””â”€ aggregator.py              # ì§‘ê³„ (íŒŒì¼ ë ˆë²¨ ê·¸ë£¹í•‘)
â”‚
â”œâ”€ formatter/                    # ë©”ì‹œì§€ í¬ë§·íŒ…
â”‚  â”œâ”€ __init__.py
â”‚  â””â”€ teams_formatter.py         # Teams ë©”ì‹œì§€ ìƒì„±
â”‚
â”œâ”€ config/                       # ì„¤ì •
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ constants.py               # ìƒìˆ˜ (WEBHOOK_URL ë“±)
â”‚  â””â”€ error_patterns.py          # ì—ëŸ¬ ë¶„ë¥˜ ê·œì¹™
â”‚
â””â”€ utils/                        # ìœ í‹¸ë¦¬í‹°
   â”œâ”€ __init__.py
   â””â”€ logger.py                  # ë¡œê¹… ì„¤ì •
```

---

### ì°¸ì¡° ì•„í‚¤í…ì²˜: airflow_dag_monitor

**ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ë™ì¼í•œ íŒ¨í„´ ì‚¬ìš©**:
- âœ… Cloud Run Job (not Cloud Function)
- âœ… Dockerfile ê¸°ë°˜ ë°°í¬
- âœ… clients/ ë””ë ‰í† ë¦¬ íŒ¨í„´
- âœ… utils/ ë””ë ‰í† ë¦¬ íŒ¨í„´
- âœ… constants.pyë¡œ í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬

**airflow_dag_monitor êµ¬ì¡°**:
```
airflow_dag_monitor/
â”œâ”€ Dockerfile
â”œâ”€ requirements.txt
â”œâ”€ deploy.sh
â”œâ”€ main.py
â”œâ”€ clients/
â”‚  â”œâ”€ airflow_client.py
â”‚  â””â”€ bigquery_client.py
â”œâ”€ utils/
â”‚  â””â”€ formatter.py
â””â”€ constants.py
```

**converter_failure_monitorë„ ë™ì¼ íŒ¨í„´ ì ìš©**:
- Cloud Run Jobìœ¼ë¡œ ë°°í¬
- ë™ì¼í•œ ë””ë ‰í† ë¦¬ êµ¬ì¡°
- ë™ì¼í•œ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ íŒ¨í„´

---

### ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì„¤ê³„

#### 1. main.py (ì§„ì…ì )

```python
# converter_failure_monitor/main.py

import os
import logging
from datetime import datetime, timedelta
import pytz

from .clients.bigquery_client import BigQueryClient
from .clients.firestore_client import FirestoreClient
from .analyzer.classifier import ErrorClassifier
from .analyzer.aggregator import FailureAggregator
from .formatter.teams_formatter import TeamsFormatter
from .config.constants import WEBHOOK_URL, PROJECT_ID

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def run_monitor():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ Converter Failure Monitor Started")

    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    bq_client = BigQueryClient(project_id=PROJECT_ID)
    fs_client = FirestoreClient()

    # KST ê¸°ì¤€ ì–´ì œ ë‚ ì§œ ê³„ì‚°
    KST = pytz.timezone('Asia/Seoul')
    now = datetime.now(KST)

    # ì˜¤ì „ 9ì‹œ ê·œì¹™ (airflow_dag_monitorì™€ ë™ì¼)
    if now.hour < 9:
        target_date = (now - timedelta(days=1)).date()
    else:
        target_date = now.date() - timedelta(days=1)

    logger.info(f"Target date: {target_date}")

    # 1. BigQueryì—ì„œ ì‹¤íŒ¨ ê±´ ì¡°íšŒ
    logger.info("Step 1: Fetching failures from BigQuery...")
    failures = bq_client.get_failures(target_date)
    logger.info(f"Found {len(failures)} failure records")

    if not failures:
        send_success_message(target_date)
        return

    # 2. Firestoreì—ì„œ ê³ ê°ì‚¬ ì •ë³´ (env != 'dev')
    logger.info("Step 2: Fetching customer info from Firestore...")
    active_customers = fs_client.get_active_customers()  # env != 'dev'
    customer_names = fs_client.get_customer_names()

    # 3. í•„í„°ë§ (ìš´ì˜ ê³ ê°ì‚¬ë§Œ)
    failures = [f for f in failures if f['customer_code'] in active_customers]
    logger.info(f"After filtering (env != 'dev'): {len(failures)} failures")

    # 4. ì§‘ê³„ (íŒŒì¼ ë ˆë²¨ ê·¸ë£¹í•‘)
    logger.info("Step 3: Aggregating by file...")
    aggregator = FailureAggregator()
    aggregated = aggregator.aggregate_by_file(failures)

    # 5. ì—ëŸ¬ ë¶„ë¥˜ (ê·œì¹™ ê¸°ë°˜)
    logger.info("Step 4: Classifying errors...")
    classifier = ErrorClassifier()
    for item in aggregated:
        item['error_type'] = classifier.classify(item['error_message'])

    # 6. ì†ŒìŠ¤ íƒ€ì…ë³„ ê·¸ë£¹í•‘ (RPA/Board vs NonRPA)
    rpa_board_failures = [f for f in aggregated if f['source_type'] in ['rpa', 'board']]
    nonrpa_failures = [f for f in aggregated if f['source_type'] in ['pc', 'email', 'shared_drive']]

    # 7. Teams ë©”ì‹œì§€ ìƒì„±
    logger.info("Step 5: Formatting Teams message...")
    formatter = TeamsFormatter(customer_names)
    message = formatter.create_message(
        target_date=target_date,
        rpa_board_failures=rpa_board_failures,
        nonrpa_failures=nonrpa_failures,
        total_count=len(failures)
    )

    # 8. Teams ì „ì†¡
    logger.info("Step 6: Sending to Teams...")
    send_to_teams(message)

    logger.info("âœ… Converter Failure Monitor Completed")


def send_to_teams(message):
    """Teams webhookìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡"""
    import requests

    payload = {"text": message}
    response = requests.post(WEBHOOK_URL, json=payload)

    if response.status_code == 200:
        logger.info("âœ… Message sent to Teams successfully")
    else:
        logger.error(f"âŒ Failed to send message: {response.status_code}")


def send_success_message(target_date):
    """ì‹¤íŒ¨ ê±´ì´ ì—†ì„ ë•Œ ë©”ì‹œì§€"""
    message = f"""
ğŸ“Š Converter ì‹¤íŒ¨ ë¦¬í¬íŠ¸ - {target_date}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‰ ì‹¤íŒ¨ ê±´ì´ ì—†ìŠµë‹ˆë‹¤!

ì „ì²´: ëª¨ë‘ ì„±ê³µ âœ…
"""
    send_to_teams(message)


if __name__ == "__main__":
    run_monitor()
```

---

#### 2. clients/bigquery_client.py

```python
# converter_failure_monitor/clients/bigquery_client.py

from google.cloud import bigquery
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class BigQueryClient:
    """BigQuery ì¡°íšŒ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, project_id):
        self.client = bigquery.Client(project=project_id)
        self.project_id = project_id

    def get_failures(self, target_date):
        """
        íŠ¹ì • ë‚ ì§œì˜ ì‹¤íŒ¨ ê±´ ì¡°íšŒ

        Returns:
            List[dict]: ì‹¤íŒ¨ ê¸°ë¡ë“¤
        """
        query = f"""
        SELECT
            customer_code,
            customer_name,
            source_type,
            source_id,
            convert_config_id,
            gcs_path,
            error_message,
            created_at,
            env
        FROM
            `{self.project_id}.dashboard.convert_job_history`
        WHERE
            DATE(created_at, 'Asia/Seoul') = '{target_date}'
            AND status = 'fail'
            AND env != 'dev'  -- ìš´ì˜ í™˜ê²½ë§Œ
        ORDER BY
            customer_code, source_type, gcs_path
        """

        logger.info(f"Querying BigQuery for date: {target_date}")

        try:
            query_job = self.client.query(query)
            results = query_job.result()

            failures = []
            for row in results:
                failures.append({
                    'customer_code': row.customer_code,
                    'customer_name': row.customer_name,
                    'source_type': row.source_type,
                    'source_id': row.source_id,
                    'convert_config_id': row.convert_config_id,
                    'gcs_path': row.gcs_path,
                    'error_message': row.error_message,
                    'created_at': row.created_at,
                    'env': row.env
                })

            return failures

        except Exception as e:
            logger.error(f"Error querying BigQuery: {e}")
            raise
```

---

#### 3. clients/firestore_client.py

```python
# converter_failure_monitor/clients/firestore_client.py

from google.cloud import firestore
import logging

logger = logging.getLogger(__name__)


class FirestoreClient:
    """Firestore ì¡°íšŒ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.db = firestore.Client()
        self.company_collection = self.db.collection("company").document("version").collection("v1.0")

    def get_active_customers(self):
        """
        ìš´ì˜ ì¤‘ì¸ ê³ ê°ì‚¬ ì½”ë“œ ëª©ë¡ (env != 'dev')

        Returns:
            Set[str]: ê³ ê°ì‚¬ ì½”ë“œ ì§‘í•©
        """
        logger.info("Fetching active customers from Firestore...")

        active_customers = set()

        try:
            customer_docs = self.company_collection.list_documents()

            for customer_doc in customer_docs:
                customer_code = customer_doc.id

                # source_metasì—ì„œ env í™•ì¸
                source_metas_ref = customer_doc.collection("source_metas")
                for source_meta_doc in source_metas_ref.stream():
                    env = source_meta_doc.to_dict().get("env", "dev")

                    if env != "dev":  # ops, test ë“± í¬í•¨
                        active_customers.add(customer_code)
                        break  # í•˜ë‚˜ë¼ë„ ë°œê²¬ë˜ë©´ ë‹¤ìŒ ê³ ê°ì‚¬ë¡œ

            logger.info(f"Found {len(active_customers)} active customers")
            return active_customers

        except Exception as e:
            logger.error(f"Error fetching active customers: {e}")
            raise

    def get_customer_names(self):
        """
        ê³ ê°ì‚¬ ì½”ë“œ â†’ ì´ë¦„ ë§¤í•‘

        Returns:
            Dict[str, str]: {customer_code: customer_name}
        """
        logger.info("Fetching customer names from Firestore...")

        customer_names = {}

        try:
            customer_docs = self.company_collection.stream()

            for customer_doc in customer_docs:
                customer_code = customer_doc.id
                customer_data = customer_doc.to_dict()
                customer_name = customer_data.get("name", "Unknown")
                customer_names[customer_code] = customer_name

            logger.info(f"Found {len(customer_names)} customer names")
            return customer_names

        except Exception as e:
            logger.error(f"Error fetching customer names: {e}")
            raise
```

---

#### 4. analyzer/classifier.py

```python
# converter_failure_monitor/analyzer/classifier.py

import re
import logging
from ..config.error_patterns import ERROR_PATTERNS

logger = logging.getLogger(__name__)


class ErrorClassifier:
    """ì—ëŸ¬ ë¶„ë¥˜ê¸° (ê·œì¹™ ê¸°ë°˜)"""

    def __init__(self):
        self.patterns = ERROR_PATTERNS

    def classify(self, error_message):
        """
        ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë¶„ë¥˜

        Args:
            error_message (str): ì—ëŸ¬ ë©”ì‹œì§€

        Returns:
            str: ì—ëŸ¬ ìœ í˜• ("í—¤ë” ì—ëŸ¬", "ì‹œíŠ¸ ì—ëŸ¬", etc.)
        """
        if not error_message:
            return "ê¸°íƒ€"

        # ê·œì¹™ ê¸°ë°˜ ë§¤ì¹­
        for error_type, patterns in self.patterns.items():
            for pattern in patterns:
                if re.search(pattern, error_message, re.IGNORECASE):
                    logger.debug(f"Classified as '{error_type}': {error_message[:100]}")
                    return error_type

        # ë§¤ì¹­ ì•ˆ ë˜ë©´ "ê¸°íƒ€"
        logger.debug(f"Unclassified (ê¸°íƒ€): {error_message[:100]}")
        return "ê¸°íƒ€"
```

---

#### 5. analyzer/aggregator.py

```python
# converter_failure_monitor/analyzer/aggregator.py

import logging
from collections import defaultdict

logger = logging.getLogger(__name__)


class FailureAggregator:
    """ì‹¤íŒ¨ ê±´ ì§‘ê³„"""

    def aggregate_by_file(self, failures):
        """
        íŒŒì¼ ë ˆë²¨ë¡œ ê·¸ë£¹í•‘ (gcs_path ê¸°ì¤€)

        í•˜ë‚˜ì˜ íŒŒì¼ì´ ì—¬ëŸ¬ í…Œì´ë¸” ì‹¤íŒ¨ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
        gcs_pathë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í•‘

        Args:
            failures (List[dict]): ì‹¤íŒ¨ ê¸°ë¡ë“¤

        Returns:
            List[dict]: íŒŒì¼ ë ˆë²¨ë¡œ ì§‘ê³„ëœ ê²°ê³¼
        """
        logger.info("Aggregating failures by file (gcs_path)...")

        # gcs_pathë¥¼ í‚¤ë¡œ ê·¸ë£¹í•‘
        grouped = defaultdict(lambda: {
            'customer_code': None,
            'customer_name': None,
            'source_type': None,
            'source_id': None,
            'gcs_path': None,
            'error_message': None,
            'failed_configs': [],
            'table_count': 0
        })

        for failure in failures:
            key = (
                failure['customer_code'],
                failure['source_type'],
                failure['gcs_path']
            )

            item = grouped[key]

            # ì²« ë²ˆì§¸ ë ˆì½”ë“œë¡œ ê¸°ë³¸ ì •ë³´ ì„¤ì •
            if item['customer_code'] is None:
                item['customer_code'] = failure['customer_code']
                item['customer_name'] = failure['customer_name']
                item['source_type'] = failure['source_type']
                item['source_id'] = failure['source_id']
                item['gcs_path'] = failure['gcs_path']
                item['error_message'] = failure['error_message']  # ëŒ€í‘œ ì—ëŸ¬

            # ì‹¤íŒ¨í•œ config ì¶”ê°€
            if failure['convert_config_id'] not in item['failed_configs']:
                item['failed_configs'].append(failure['convert_config_id'])

            item['table_count'] += 1

        # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        result = list(grouped.values())

        logger.info(f"Aggregated {len(failures)} records into {len(result)} files")
        return result
```

---

#### 6. formatter/teams_formatter.py

```python
# converter_failure_monitor/formatter/teams_formatter.py

import logging
from collections import defaultdict

logger = logging.getLogger(__name__)


class TeamsFormatter:
    """Teams ë©”ì‹œì§€ í¬ë§·í„°"""

    def __init__(self, customer_names):
        """
        Args:
            customer_names (dict): {customer_code: customer_name}
        """
        self.customer_names = customer_names

    def create_message(self, target_date, rpa_board_failures, nonrpa_failures, total_count):
        """
        Teams ë©”ì‹œì§€ ìƒì„±

        Args:
            target_date: ëŒ€ìƒ ë‚ ì§œ
            rpa_board_failures: RPA/Board ì‹¤íŒ¨ ëª©ë¡
            nonrpa_failures: NonRPA ì‹¤íŒ¨ ëª©ë¡
            total_count: ì „ì²´ ì‹¤íŒ¨ ê±´ìˆ˜ (í…Œì´ë¸” ë ˆë²¨)
        """
        # í—¤ë”
        message = f"""
ğŸ“Š Converter ì‹¤íŒ¨ ë¦¬í¬íŠ¸ - {target_date}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì „ì²´: {total_count}ê±´ (í…Œì´ë¸” ë ˆë²¨)
ì‹¤íŒ¨ íŒŒì¼: {len(rpa_board_failures) + len(nonrpa_failures)}ê°œ
"""

        # RPA/Board ì„¹ì…˜
        if rpa_board_failures:
            message += self._format_rpa_board_section(rpa_board_failures)
        else:
            message += "\nğŸ”´ [RPA/Board] ì‹¤íŒ¨ ì—†ìŒ\n"

        # NonRPA ì„¹ì…˜
        if nonrpa_failures:
            message += self._format_nonrpa_section(nonrpa_failures)
        else:
            message += "\nâš ï¸ [NonRPA] ì‹¤íŒ¨ ì—†ìŒ\n"

        # í‘¸í„°
        message += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”— ìƒì„¸ ë³´ê¸°: [BigQuery] | [Grafana]
"""

        return message

    def _format_rpa_board_section(self, failures):
        """RPA/Board ì„¹ì…˜ í¬ë§·íŒ…"""
        # ê³ ê°ì‚¬ë³„ ê·¸ë£¹í•‘
        by_customer = defaultdict(list)
        for f in failures:
            by_customer[f['customer_code']].append(f)

        # ìµœëŒ€ 10ê°œ ê³ ê°ì‚¬ë§Œ í‘œì‹œ
        MAX_CUSTOMERS = 10
        shown_customers = list(by_customer.keys())[:MAX_CUSTOMERS]

        message = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”´ [RPA/Board] ì¼ë³„ ìˆ˜ì§‘ ì‹¤íŒ¨ - {len(shown_customers)}ê°œ ê³ ê°ì‚¬
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

        # ê³ ê°ì‚¬ë³„ ì¶œë ¥
        for customer_code in shown_customers:
            customer_failures = by_customer[customer_code]
            customer_name = self.customer_names.get(customer_code, customer_code)

            # ì—ëŸ¬ ìœ í˜•ë³„ ì§‘ê³„
            error_type_count = defaultdict(int)
            for f in customer_failures:
                error_type = f.get('error_type', 'ê¸°íƒ€')
                error_type_count[error_type] += f['table_count']

            # í¬ë§·íŒ…
            error_summary = " | ".join([f"{k}: {v}ê±´" for k, v in error_type_count.items()])

            message += f"{customer_name} [{customer_code}]\n"
            message += f"  íŒŒì¼: {len(customer_failures)}ê°œ, {error_summary}\n"

        # ë‚˜ë¨¸ì§€ ê³ ê°ì‚¬
        if len(by_customer) > MAX_CUSTOMERS:
            hidden_count = len(by_customer) - MAX_CUSTOMERS
            message += f"\nâš ï¸ ê¸°íƒ€ {hidden_count}ê°œ ê³ ê°ì‚¬ (BigQueryì—ì„œ í™•ì¸)\n"

        return message

    def _format_nonrpa_section(self, failures):
        """NonRPA ì„¹ì…˜ í¬ë§·íŒ…"""
        # ê³ ê°ì‚¬ë³„ ê·¸ë£¹í•‘
        by_customer = defaultdict(list)
        for f in failures:
            by_customer[f['customer_code']].append(f)

        # ìµœëŒ€ 10ê°œ ê³ ê°ì‚¬ë§Œ í‘œì‹œ
        MAX_CUSTOMERS = 10
        shown_customers = list(by_customer.keys())[:MAX_CUSTOMERS]

        message = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ [NonRPA] ê³ ê°ì‚¬ ì—…ë¡œë“œ ì‹¤íŒ¨ - {len(shown_customers)}ê°œ ê³ ê°ì‚¬
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

        # ê³ ê°ì‚¬ë³„ ì¶œë ¥
        for customer_code in shown_customers:
            customer_failures = by_customer[customer_code]
            customer_name = self.customer_names.get(customer_code, customer_code)

            # ì†ŒìŠ¤ íƒ€ì…ë³„ ê·¸ë£¹í•‘
            by_source_type = defaultdict(list)
            for f in customer_failures:
                by_source_type[f['source_type']].append(f)

            message += f"{customer_name} [{customer_code}]\n"
            for source_type, failures in by_source_type.items():
                message += f"  {source_type}: {len(failures)}ê°œ íŒŒì¼\n"

        # ë‚˜ë¨¸ì§€ ê³ ê°ì‚¬
        if len(by_customer) > MAX_CUSTOMERS:
            hidden_count = len(by_customer) - MAX_CUSTOMERS
            message += f"\nâš ï¸ ê¸°íƒ€ {hidden_count}ê°œ ê³ ê°ì‚¬ (BigQueryì—ì„œ í™•ì¸)\n"

        return message
```

---

#### 7. config/error_patterns.py

```python
# converter_failure_monitor/config/error_patterns.py

# ì—ëŸ¬ ë¶„ë¥˜ ê·œì¹™ (ì •ê·œì‹)
ERROR_PATTERNS = {
    "ì‹œíŠ¸ ì—ëŸ¬": [
        r"not found matched sheet",
        r"Sheet .* not found",
        r"Worksheet .* does not exist",
        r"No sheet named",
    ],
    "í—¤ë” ì—ëŸ¬": [
        r"row not found",
        r"Header .* not found",
        r"Cannot find header",
        r"Missing column",
        r"header_coordinate",
    ],
    "ì»¬ëŸ¼ ë²”ìœ„ ì—ëŸ¬": [
        r"usecols.*out of bounds",
        r"invalid column range",
    ],
    "íŒŒì¼ ì†ìƒ": [
        r"corrupt",
        r"damaged",
        r"cannot.*read.*file",
        r"empty.*sheet",
    ],
    "Timeout": [
        r"[Tt]imeout",
        r"exceed.*time limit",
    ],
}
```

---

#### 8. config/constants.py

```python
# converter_failure_monitor/config/constants.py

import os

# GCP í”„ë¡œì íŠ¸ ID
PROJECT_ID = os.getenv("GCP_PROJECT_ID", "hyperlounge-dev")

# Teams Webhook URL
WEBHOOK_URL = os.getenv("TEAMS_WEBHOOK_URL", "")

# BigQuery í…Œì´ë¸”
BQ_TABLE = f"{PROJECT_ID}.dashboard.convert_job_history"

# Firestore ì»¬ë ‰ì…˜ ê²½ë¡œ
FIRESTORE_COLLECTION = "company/version/v1.0"

# ë¡œê¹… ë ˆë²¨
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
```

---

#### 9. Dockerfile

```dockerfile
# converter_failure_monitor/Dockerfile

# airflow_dag_monitorì™€ ë™ì¼í•œ íŒ¨í„´
FROM python:3.9-slim

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì†ŒìŠ¤ ì½”ë“œ ë³µì‚¬
COPY . /app/converter_failure_monitor/

# ì‹¤í–‰
CMD ["python", "-m", "converter_failure_monitor.main"]
```

---

#### 10. requirements.txt

```
google-cloud-bigquery==3.11.0
google-cloud-firestore==2.11.0
requests==2.31.0
pytz==2023.3
```

---

#### 11. deploy.sh

```bash
#!/bin/bash
# converter_failure_monitor/deploy.sh

# airflow_dag_monitor/deploy.sh ì°¸ê³ 

set -e

PROJECT_ID="hyperlounge-dev"
REGION="asia-northeast3"
JOB_NAME="converter-failure-monitor"
IMAGE_NAME="gcr.io/${PROJECT_ID}/${JOB_NAME}"

echo "ğŸš€ Deploying Converter Failure Monitor..."

# 1. Docker ì´ë¯¸ì§€ ë¹Œë“œ
echo "ğŸ“¦ Building Docker image..."
docker build -t ${IMAGE_NAME}:latest .

# 2. GCRì— í‘¸ì‹œ
echo "ğŸ“¤ Pushing to GCR..."
docker push ${IMAGE_NAME}:latest

# 3. Cloud Run Job ìƒì„±/ì—…ë°ì´íŠ¸
echo "â˜ï¸  Deploying to Cloud Run Job..."
gcloud run jobs deploy ${JOB_NAME} \
  --image=${IMAGE_NAME}:latest \
  --region=${REGION} \
  --project=${PROJECT_ID} \
  --set-env-vars="GCP_PROJECT_ID=${PROJECT_ID}" \
  --set-env-vars="TEAMS_WEBHOOK_URL=${TEAMS_WEBHOOK_URL}" \
  --memory=512Mi \
  --cpu=1 \
  --max-retries=0 \
  --task-timeout=10m

# 4. Cloud Scheduler ìƒì„± (ë§¤ì¼ ì•„ì¹¨ 8ì‹œ KST)
echo "â° Setting up Cloud Scheduler..."
gcloud scheduler jobs create http ${JOB_NAME}-scheduler \
  --location=${REGION} \
  --schedule="0 8 * * *" \
  --time-zone="Asia/Seoul" \
  --uri="https://${REGION}-run.googleapis.com/apis/run.googleapis.com/v1/namespaces/${PROJECT_ID}/jobs/${JOB_NAME}:run" \
  --http-method=POST \
  --oauth-service-account-email="${PROJECT_ID}@appspot.gserviceaccount.com" \
  || echo "Scheduler already exists"

echo "âœ… Deployment complete!"
```

---

### ë°°í¬ ë°©ì‹

#### ì™œ Cloud Run Jobì¸ê°€?

**Cloud Function ëŒ€ì‹  Cloud Run Job ì„ íƒ ì´ìœ **:

1. **ì¼ê´€ì„±** âœ…
   - airflow_dag_monitorì™€ ë™ì¼í•œ íŒ¨í„´
   - ê¸°ì¡´ ë…¸í•˜ìš° ì¬ì‚¬ìš©
   - ìœ ì§€ë³´ìˆ˜ ìš©ì´

2. **ì‹¤í–‰ ì‹œê°„** âœ…
   - Cloud Function: ìµœëŒ€ 9ë¶„
   - Cloud Run Job: ìµœëŒ€ 60ë¶„
   - Converter ë¶„ì„ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ

3. **ë¦¬ì†ŒìŠ¤** âœ…
   - Cloud Function: ë©”ëª¨ë¦¬ ì œí•œ
   - Cloud Run Job: ìœ ì—°í•œ ë¦¬ì†ŒìŠ¤ í• ë‹¹

4. **ë””ë²„ê¹…** âœ…
   - ë¡œì»¬ì—ì„œ Dockerë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
   - ë¡œê·¸ í™•ì¸ ì‰¬ì›€

**ì‹¤í–‰ íë¦„**:
```
Cloud Scheduler (ë§¤ì¼ 8ì‹œ)
  â†“
Cloud Run Job ì‹¤í–‰
  â†“
main.py ì‹¤í–‰
  â†“
Teams ë©”ì‹œì§€ ì „ì†¡
  â†“
Job ì¢…ë£Œ
```

---

### í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬

**í•„ìš”í•œ í™˜ê²½ë³€ìˆ˜**:
```bash
# deploy.shì—ì„œ ì„¤ì •
GCP_PROJECT_ID=hyperlounge-dev
TEAMS_WEBHOOK_URL=https://...
LOG_LEVEL=INFO  # (ì„ íƒ)
```

**Secret Manager ì‚¬ìš©** (ê¶Œì¥):
```bash
# Webhook URLì„ Secretìœ¼ë¡œ ì €ì¥
gcloud secrets create teams-webhook-url --data-file=-

# Cloud Run Jobì—ì„œ ì‚¬ìš©
gcloud run jobs deploy converter-failure-monitor \
  --set-secrets="TEAMS_WEBHOOK_URL=teams-webhook-url:latest"
```

---

### ë¡œì»¬ í…ŒìŠ¤íŠ¸

```bash
# 1. Docker ë¹Œë“œ
docker build -t converter-failure-monitor .

# 2. ë¡œì»¬ ì‹¤í–‰ (í™˜ê²½ë³€ìˆ˜ ì£¼ì…)
docker run \
  -e GCP_PROJECT_ID=hyperlounge-dev \
  -e TEAMS_WEBHOOK_URL=https://... \
  -v ~/.config/gcloud:/root/.config/gcloud \
  converter-failure-monitor

# 3. Python ì§ì ‘ ì‹¤í–‰ (ê°œë°œ ì¤‘)
export GCP_PROJECT_ID=hyperlounge-dev
export TEAMS_WEBHOOK_URL=https://...
python -m converter_failure_monitor.main
```

---

### ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

**Cloud Logging ì¿¼ë¦¬**:
```
resource.type="cloud_run_job"
resource.labels.job_name="converter-failure-monitor"
severity>=WARNING
```

**ì£¼ìš” ë¡œê·¸**:
- âœ… ì‹œì‘/ì¢…ë£Œ ë¡œê·¸
- âœ… ê° ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™©
- âœ… ì—ëŸ¬ ë°œìƒ ì‹œ ìƒì„¸ ì •ë³´
- âœ… Teams ì „ì†¡ ì„±ê³µ/ì‹¤íŒ¨

**ì•Œë¦¼ ì„¤ì •** (ì„ íƒ):
```bash
# Job ì‹¤íŒ¨ ì‹œ ì•Œë¦¼
gcloud logging metrics create converter-monitor-failures \
  --description="Converter monitor job failures" \
  --log-filter='resource.type="cloud_run_job"
    resource.labels.job_name="converter-failure-monitor"
    severity="ERROR"'
```

---

### Phase 2 í™•ì¥ ê³ ë ¤ì‚¬í•­

**í˜„ì¬ ì•„í‚¤í…ì²˜ì—ì„œ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥**:

1. **Noise í•„í„°ë§**
   - `analyzer/filter.py`ì— ì¶”ê°€
   - BigQueryì—ì„œ ê°™ì€ ë‚  ì„±ê³µë¥  ì¡°íšŒ

2. **LLM ì—ëŸ¬ ë¶„ì„**
   - `analyzer/classifier.py`ì— fallback ë¡œì§ ì¶”ê°€
   - Vertex AI ë˜ëŠ” OpenAI API í˜¸ì¶œ

3. **DAG ìƒíƒœ ì²´í¬**
   - `clients/airflow_client.py` ì¶”ê°€
   - airflow_dag_monitorì˜ ë¡œì§ ì¬ì‚¬ìš©

4. **ì •ì±…ìƒ ì œì™¸**
   - `config/error_patterns.py`ì— ê·œì¹™ ì¶”ê°€
   - `analyzer/filter.py`ì—ì„œ ì²˜ë¦¬

**í™•ì¥ì„± ìˆëŠ” êµ¬ì¡°**:
- ëª¨ë“ˆí™”ë˜ì–´ ìˆì–´ ê¸°ëŠ¥ ì¶”ê°€ ì‰¬ì›€
- airflow_dag_monitor íŒ¨í„´ê³¼ ë™ì¼í•˜ì—¬ í•™ìŠµ ê³¡ì„  ì—†ìŒ
- í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ êµ¬ì¡°

---
