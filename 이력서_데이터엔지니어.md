# 박준현 | Data Engineer

> **Email**: [이메일]
> **GitHub**: [GitHub URL]
> **LinkedIn**: [LinkedIn URL]

---

## Summary

**3년차 플랫폼/데이터 엔지니어**로, Airflow 기반 대규모 데이터 파이프라인 운영과 80개 이상의 Python 자동화 서비스 개발 경험을 보유하고 있습니다. 22개 고객사의 ETL 파이프라인을 설계/운영하며 데이터 품질 관리 시스템을 직접 구축했습니다.

**핵심 역량:**
- Airflow 기반 대규모 파이프라인 운영 (22개 고객사 DAG)
- 데이터 파이프라인 아키텍처 설계 및 최적화 (성능 60% 개선)
- 클라우드 데이터 플랫폼 구축/운영 (GCP)
- Python 기반 자동화 시스템 개발 (80+ 서비스)

---

## Technical Skills

| 분류 | 기술 |
|------|------|
| **Languages** | Python (주력), SQL, Shell Script |
| **Data Pipeline** | Apache Airflow, ETL/ELT |
| **Cloud (GCP)** | Cloud Run, Cloud Composer, BigQuery, Cloud Functions, GCS, Pub/Sub |
| **Database** | BigQuery, PostgreSQL, Firestore |
| **Infrastructure** | Docker, Cloud Build, IAM |
| **Monitoring** | Cloud Logging, Grafana, Custom Monitoring Tools |
| **API** | Flask, FastAPI, REST API |
| **학습 중** | Apache Kafka, Apache Spark |

---

## Work Experience

### 하이퍼라운지 | 플랫폼 엔지니어 (데이터 파이프라인 총괄)
**2023.XX ~ 현재** | 서울

#### 데이터 파이프라인 아키텍처 설계 및 운영

**Airflow 기반 대규모 파이프라인 운영**
- **22개 고객사**의 데이터 파이프라인을 Apache Airflow로 설계/운영
- DAG 의존성 관리, 스케줄링 최적화, 백필(Backfill) 처리
- 일일 **수백만 건** 데이터 처리

**Unified Converter 아키텍처 개선**
- 3개의 분산된 Cloud Function → 1개 Cloud Run으로 통합
- **성능 60% 개선**, API 호출 67% 감소, 장애 포인트 67% 감소
- Airflow Task 수 75% 감소 (6-8개 → 2-3개)

**E2E 데이터 플로우 설계**
```
Crawler → FileID Mapping → Filter → Convert → Tag → Tag-Prep → Load → Transform → Sync
```

#### 모니터링 시스템 구축

- **4개의 자체 모니터링 시스템** 설계 및 개발
  - `airflow_dag_monitor`: DAG 실행 상태 실시간 추적
  - `history_checker`: 파이프라인 이력 관리
  - `converter_failure_monitor`: 변환 실패 즉시 알림
  - `sli_slo`: 서비스 수준 지표 관리
- 장애 대응 시간 **50% 단축**

#### 클라우드 인프라 운영 (GCP)

- Cloud Run 기반 서비스 배포 및 자동 스케일링
- Cloud Composer (Managed Airflow) 환경 운영
- BigQuery 데이터 웨어하우스 설계 및 최적화
- Cloud Build CI/CD 파이프라인 구축

#### Python 자동화 서비스 개발

- **80개 이상**의 마이크로서비스 설계/개발/운영
- 데이터 수집, 변환, 태깅, 동기화 등 전체 스택 담당
- 일일 수작업 **6시간 절감** 효과

---

## Projects

### realtime-crypto-pipeline (개인 프로젝트)
**2026.01 ~ 진행 중** | [GitHub Link]

실시간 암호화폐 거래 데이터 수집 → 처리 → 분석 파이프라인

**기술 스택:** Python, Apache Kafka, Apache Spark (학습 중), PostgreSQL, Docker

**주요 구현:**
- Binance WebSocket API 실시간 데이터 수집 (5개 심볼)
- Kafka Producer 구현 (aiokafka, 배치 최적화)
- Airflow DAG 기반 데이터 품질 관리
- Grafana 실시간 대시보드

**목표:**
- 처리량: 10,000 msg/sec
- 지연: < 100ms

---

## Key Achievements

| 성과 | 상세 | 임팩트 |
|------|------|--------|
| **파이프라인 통합** | 3개 서비스 → 1개 통합 | 성능 60%↑, 유지보수 비용↓ |
| **대규모 운영** | 22개 고객사 DAG | 일일 수백만 건 처리 |
| **자동화** | 80+ Python 서비스 | 일일 6시간 절감 |
| **모니터링** | 4개 시스템 구축 | 장애 대응 50% 단축 |

---

## Education

### [대학교] | [전공]
**20XX ~ 20XX**

---

## Certifications (선택)

- [자격증이 있다면 추가]

---

## Why Me?

### 1. Airflow 실전 경험
단순 사용이 아닌, **22개 고객사 규모**의 복잡한 DAG를 설계하고 운영한 경험이 있습니다. 의존성 관리, 백필 처리, 모니터링까지 전체 사이클을 경험했습니다.

### 2. 데이터 파이프라인 아키텍처
분산된 시스템을 통합하여 **성능 60% 개선**한 경험이 있습니다. 단순 개발이 아닌 아키텍처 관점에서 시스템을 바라봅니다.

### 3. 문제 해결 능력
4개의 모니터링 시스템을 직접 설계/개발하여 **장애 대응 시간을 50% 단축**했습니다. 문제를 발견하고 시스템으로 해결합니다.

### 4. 빠른 학습 능력
현재 Kafka + Spark를 개인 프로젝트로 학습 중입니다. 새로운 기술도 빠르게 습득하여 적용할 수 있습니다.

---

*마지막 업데이트: 2026-01-24*
