# 지원 준비 체크리스트

> **목표**: 토스, 당근페이 우선 지원 + Spark 학습 후 추가 지원
> **작성일**: 2026-01-24

---

## 1. 즉시 할 일 (이번 주)

### 1.1 이력서 업데이트
- [ ] **Airflow 경험 강조**
  ```
  Before: "Airflow DAG 운영"
  After:  "22개 고객사 데이터 파이프라인을 Airflow로 운영,
           DAG 모니터링 시스템 직접 개발로 장애 대응 시간 50% 단축"
  ```

- [ ] **데이터 파이프라인 성과 정량화**
  ```
  - Unified Converter: 3개 서비스 → 1개 통합, 성능 60% 개선
  - 22개 고객사 DAG 운영, 일일 수백만 건 처리
  - 80+ Python 자동화 서비스 개발/운영
  ```

- [ ] **기술 스택 정리**
  ```
  주력: Python, SQL, Airflow, GCP (Cloud Run, BigQuery, Composer)
  사용: Docker, PostgreSQL, Flask/FastAPI
  학습중: Kafka, Spark (개인 프로젝트)
  ```

### 1.2 포트폴리오 정리
- [ ] realtime-crypto-pipeline README 업데이트
  - 아키텍처 다이어그램
  - 기술 스택 명시
  - Phase 3 진행 상황

- [ ] GitHub 프로필 정리
  - 핀 고정 리포지토리 선택
  - README 업데이트

### 1.3 Kafka 테스트 완료
- [ ] docker-compose up 실행
- [ ] Kafka UI에서 메시지 확인
- [ ] Producer 테스트 완료

---

## 2. 회사별 지원서 준비

### 2.1 토스 (1순위) ⭐

**자기소개서 핵심 포인트:**
```
1. [Airflow 전문성]
   "22개 고객사의 데이터 파이프라인을 Airflow로 운영하며,
   DAG 모니터링 시스템을 직접 개발했습니다.
   토스의 요구사항인 'Airflow 사용 가능 수준'을 넘어
   복잡한 의존성 관리와 백필 처리까지 경험했습니다."

2. [데이터 일관성]
   "3개의 분산된 Cloud Function을 1개의 Cloud Run으로 통합하여
   데이터 일관성을 확보하고 성능을 60% 개선했습니다.
   토스에서 강조하는 '일관된 데이터 제공'에 부합합니다."

3. [대용량 처리 의지]
   "현재 일일 수백만 건을 처리하고 있지만,
   토스의 100TB/일 규모에 도전하고 싶습니다.
   개인 프로젝트로 Kafka + Spark를 학습하며 준비 중입니다."
```

**예상 면접 질문:**
- [ ] Airflow 운영 경험? → 22개 DAG, 백필, 모니터링
- [ ] 대용량 처리 경험? → BigQuery + 학습 의지
- [ ] 왜 토스? → 금융 데이터 규모, 기술 도전

**체크리스트:**
- [ ] 토스 기술 블로그 읽기 (toss.tech)
- [ ] 금융 데이터 특성 학습
- [ ] 지원서 작성
- [ ] 제출

---

### 2.2 당근페이 (2순위) ⭐

**자기소개서 핵심 포인트:**
```
1. [Airflow + 클라우드]
   "Airflow 22개 DAG + GCP 전반 운영 경험이
   당근페이의 AWS 기반 데이터 플랫폼과 잘 맞습니다.
   클라우드 개념은 동일하므로 AWS 전환은 빠르게 가능합니다."

2. [Kafka 학습 중]
   "현재 개인 프로젝트에서 Kafka Producer를 구현 완료했고,
   Spark Streaming Consumer를 개발 중입니다.
   입사 전까지 Kafka 역량을 충분히 갖출 수 있습니다."

3. [파이프라인 설계]
   "Crawler → Convert → Tag → Load → Sync 전체 플로우를
   설계하고 운영한 경험이 있습니다.
   E2E 데이터 파이프라인 이해도가 높습니다."
```

**체크리스트:**
- [ ] 당근 기술 블로그 읽기
- [ ] AWS 기초 학습 (S3, Kinesis)
- [ ] 지원서 작성
- [ ] 제출

---

### 2.3 카카오뱅크 (3순위)

**Spark 학습 후 지원**

**체크리스트:**
- [ ] Spark 기초 완료 (2주)
- [ ] realtime-crypto-pipeline Spark 연동
- [ ] 지원서 작성
- [ ] 제출

---

## 3. 기술 준비 (병행)

### 3.1 이번 주
- [ ] Kafka 환경 테스트
- [ ] PySpark 로컬 설치
- [ ] Spark DataFrame 기초

### 3.2 다음 주
- [ ] Kafka → Spark Streaming 연동
- [ ] 실시간 처리 구현

### 3.3 3주차
- [ ] 1분/5분 캔들 생성
- [ ] 처리량 측정 (면접용 숫자)

### 3.4 4주차
- [ ] README 정리
- [ ] 면접 답변 준비

---

## 4. 면접 준비

### 4.1 공통 예상 질문

**기술 질문:**
| 질문 | 답변 키워드 |
|------|------------|
| Airflow 경험? | 22개 DAG, 모니터링, 백필 |
| 대용량 처리? | BigQuery, ETL, 파티셔닝 |
| Spark 경험? | 학습 중 + 개인 프로젝트 |
| 데이터 품질 관리? | 모니터링 4개, failure_monitor |

**행동 질문:**
| 질문 | 답변 키워드 |
|------|------------|
| 어려웠던 문제? | Unified Converter 통합 |
| 팀 협업? | 22개 고객사 DAG 조율 |
| 실패 경험? | 초기 설계 실패 → 리팩토링 |

### 4.2 회사별 질문

**토스:**
- 금융 데이터 특성 이해?
- 대규모 트래픽 처리 방법?
- 왜 토스인가?

**당근페이:**
- AWS vs GCP 차이?
- 핀테크 관심도?
- Kafka 학습 현황?

---

## 5. 일정 계획

### Week 1 (지금)
```
월: 이력서 업데이트
화: 포트폴리오 정리, Kafka 테스트
수: 토스 지원서 작성
목: 당근페이 지원서 작성
금: 지원 제출, Spark 학습 시작
```

### Week 2
```
월-수: Spark + Kafka 연동
목-금: 카카오뱅크 지원 준비
```

### Week 3
```
Spark 실습 + 추가 회사 지원
```

### Week 4
```
면접 준비 + 처리량 측정
```

---

## 6. 진행 상황 트래킹

| 회사 | 지원서 | 제출 | 서류 | 면접 |
|------|:------:|:----:|:----:|:----:|
| 토스 | [ ] | [ ] | - | - |
| 당근페이 | [ ] | [ ] | - | - |
| 카카오뱅크 | [ ] | [ ] | - | - |
| 현대오토에버 | [ ] | [ ] | - | - |
| 카카오엔터 | [ ] | [ ] | - | - |
| 배달의민족 | [ ] | [ ] | - | - |

---

## 7. 핵심 메시지 (모든 지원서 공통)

> **"Airflow 22개 DAG 운영 + 80개 Python 자동화 서비스 개발 경험을 바탕으로,
> 대규모 데이터 파이프라인을 안정적으로 운영할 수 있습니다.
> Spark는 현재 개인 프로젝트로 학습 중이며, 빠르게 역량을 갖출 수 있습니다."**

---

*마지막 업데이트: 2026-01-24*
