#!/usr/bin/env python3
"""
ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ - ì‚¬ëŒì¸, ì¡ì½”ë¦¬ì•„, ì›í‹°ë“œì—ì„œ ë°ì´í„° ì—”ì§€ë‹ˆì–´/ë°±ì—”ë“œ ê³µê³  ìˆ˜ì§‘
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
import json
from collections import Counter
from datetime import datetime
from typing import List, Dict, Optional
import warnings
warnings.filterwarnings('ignore')

# ê¸°ìˆ ìŠ¤íƒ í‚¤ì›Œë“œ ì •ì˜
TECH_KEYWORDS = {
    # ì–¸ì–´
    'Python': ['python', 'íŒŒì´ì¬'],
    'Java': ['java', 'ìë°”'],
    'Kotlin': ['kotlin', 'ì½”í‹€ë¦°'],
    'Go': ['golang', 'goì–¸ì–´'],
    'Scala': ['scala', 'ìŠ¤ì¹¼ë¼'],
    'TypeScript': ['typescript', 'ts'],
    'JavaScript': ['javascript', 'js'],
    'SQL': ['sql'],

    # í”„ë ˆì„ì›Œí¬
    'Spring': ['spring', 'spring boot', 'springboot', 'ìŠ¤í”„ë§'],
    'Django': ['django', 'ì¥ê³ '],
    'FastAPI': ['fastapi'],
    'Flask': ['flask'],
    'Express': ['express', 'express.js'],
    'NestJS': ['nestjs', 'nest.js'],

    # ë°ì´í„° ì²˜ë¦¬
    'Spark': ['spark', 'pyspark', 'ìŠ¤íŒŒí¬'],
    'Hadoop': ['hadoop', 'í•˜ë‘¡'],
    'Airflow': ['airflow', 'apache airflow'],
    'Kafka': ['kafka', 'ì¹´í”„ì¹´'],
    'Flink': ['flink'],
    'Presto': ['presto', 'trino'],

    # ë°ì´í„°ë² ì´ìŠ¤
    'MySQL': ['mysql'],
    'PostgreSQL': ['postgresql', 'postgres'],
    'MongoDB': ['mongodb', 'ëª½ê³ db'],
    'Redis': ['redis', 'ë ˆë””ìŠ¤'],
    'Elasticsearch': ['elasticsearch', 'elastic', 'es'],
    'DynamoDB': ['dynamodb'],
    'Redshift': ['redshift'],
    'BigQuery': ['bigquery'],

    # í´ë¼ìš°ë“œ
    'AWS': ['aws', 'amazon web services', 'ec2', 's3', 'lambda'],
    'GCP': ['gcp', 'google cloud', 'gce'],
    'Azure': ['azure', 'ì• ì €'],

    # ì»¨í…Œì´ë„ˆ/ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
    'Docker': ['docker', 'ë„ì»¤'],
    'Kubernetes': ['kubernetes', 'k8s', 'ì¿ ë²„ë„¤í‹°ìŠ¤'],

    # CI/CD
    'Jenkins': ['jenkins', 'ì  í‚¨ìŠ¤'],
    'GitHub Actions': ['github actions'],
    'ArgoCD': ['argocd', 'argo cd'],

    # ê¸°íƒ€
    'Linux': ['linux', 'ë¦¬ëˆ…ìŠ¤'],
    'Terraform': ['terraform', 'í…Œë¼í¼'],
    'Git': ['git', 'ê¹ƒ'],
    'REST API': ['rest api', 'restful', 'api'],
    'GraphQL': ['graphql'],
    'gRPC': ['grpc'],
    'MSA': ['msa', 'microservice', 'ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤'],
}

class JobCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        self.jobs = []

    def crawl_saramin(self, keyword: str, pages: int = 5) -> List[Dict]:
        """ì‚¬ëŒì¸ ì±„ìš©ê³µê³  í¬ë¡¤ë§"""
        print(f"\n[ì‚¬ëŒì¸] '{keyword}' ê²€ìƒ‰ ì¤‘...")
        jobs = []

        for page in range(1, pages + 1):
            url = f"https://www.saramin.co.kr/zf_user/search/recruit?searchType=search&searchword={keyword}&recruitPage={page}"

            try:
                response = requests.get(url, headers=self.headers, timeout=10)
                if response.status_code != 200:
                    print(f"  í˜ì´ì§€ {page}: ì‘ë‹µ ì˜¤ë¥˜ ({response.status_code})")
                    continue

                soup = BeautifulSoup(response.text, 'html.parser')
                job_items = soup.select('.item_recruit')

                for item in job_items:
                    try:
                        title_elem = item.select_one('.job_tit a')
                        company_elem = item.select_one('.corp_name a')
                        conditions = item.select('.job_condition span')
                        sector = item.select_one('.job_sector')

                        if not title_elem:
                            continue

                        job = {
                            'source': 'ì‚¬ëŒì¸',
                            'title': title_elem.get_text(strip=True),
                            'company': company_elem.get_text(strip=True) if company_elem else '',
                            'link': 'https://www.saramin.co.kr' + title_elem.get('href', ''),
                            'conditions': [c.get_text(strip=True) for c in conditions],
                            'sector': sector.get_text(strip=True) if sector else '',
                            'raw_text': item.get_text(' ', strip=True)
                        }
                        jobs.append(job)
                    except Exception as e:
                        continue

                print(f"  í˜ì´ì§€ {page}: {len(job_items)}ê°œ ê³µê³  ìˆ˜ì§‘")
                time.sleep(1)  # ìš”ì²­ ê°„ê²©

            except Exception as e:
                print(f"  í˜ì´ì§€ {page}: ì˜¤ë¥˜ ë°œìƒ - {e}")
                continue

        print(f"[ì‚¬ëŒì¸] ì´ {len(jobs)}ê°œ ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ")
        return jobs

    def crawl_jobkorea(self, keyword: str, pages: int = 5) -> List[Dict]:
        """ì¡ì½”ë¦¬ì•„ ì±„ìš©ê³µê³  í¬ë¡¤ë§"""
        print(f"\n[ì¡ì½”ë¦¬ì•„] '{keyword}' ê²€ìƒ‰ ì¤‘...")
        jobs = []

        for page in range(1, pages + 1):
            url = f"https://www.jobkorea.co.kr/Search/?stext={keyword}&tabType=recruit&Page_No={page}"

            try:
                response = requests.get(url, headers=self.headers, timeout=10)
                if response.status_code != 200:
                    print(f"  í˜ì´ì§€ {page}: ì‘ë‹µ ì˜¤ë¥˜ ({response.status_code})")
                    continue

                soup = BeautifulSoup(response.text, 'html.parser')
                job_items = soup.select('.list-default .list-post')

                for item in job_items:
                    try:
                        title_elem = item.select_one('.post-list-info a.title')
                        company_elem = item.select_one('.post-list-corp a.name')
                        options = item.select('.option span')

                        if not title_elem:
                            continue

                        job = {
                            'source': 'ì¡ì½”ë¦¬ì•„',
                            'title': title_elem.get_text(strip=True),
                            'company': company_elem.get_text(strip=True) if company_elem else '',
                            'link': 'https://www.jobkorea.co.kr' + title_elem.get('href', ''),
                            'conditions': [o.get_text(strip=True) for o in options],
                            'sector': '',
                            'raw_text': item.get_text(' ', strip=True)
                        }
                        jobs.append(job)
                    except Exception as e:
                        continue

                print(f"  í˜ì´ì§€ {page}: {len(job_items)}ê°œ ê³µê³  ìˆ˜ì§‘")
                time.sleep(1)

            except Exception as e:
                print(f"  í˜ì´ì§€ {page}: ì˜¤ë¥˜ ë°œìƒ - {e}")
                continue

        print(f"[ì¡ì½”ë¦¬ì•„] ì´ {len(jobs)}ê°œ ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ")
        return jobs

    def crawl_wanted(self, keyword: str, limit: int = 50) -> List[Dict]:
        """ì›í‹°ë“œ ì±„ìš©ê³µê³  í¬ë¡¤ë§ (API ë°©ì‹)"""
        print(f"\n[ì›í‹°ë“œ] '{keyword}' ê²€ìƒ‰ ì¤‘...")
        jobs = []

        # ì›í‹°ë“œëŠ” APIë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
        url = f"https://www.wanted.co.kr/api/v4/jobs?country=kr&job_sort=company.response_rate_order&years=-1&locations=all&query={keyword}&limit={limit}"

        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                for item in data.get('data', []):
                    job = {
                        'source': 'ì›í‹°ë“œ',
                        'title': item.get('position', ''),
                        'company': item.get('company', {}).get('name', ''),
                        'link': f"https://www.wanted.co.kr/wd/{item.get('id', '')}",
                        'conditions': [],
                        'sector': '',
                        'raw_text': f"{item.get('position', '')} {item.get('company', {}).get('name', '')}"
                    }
                    jobs.append(job)
                print(f"[ì›í‹°ë“œ] ì´ {len(jobs)}ê°œ ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                print(f"[ì›í‹°ë“œ] API ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
        except Exception as e:
            print(f"[ì›í‹°ë“œ] ì˜¤ë¥˜ ë°œìƒ: {e}")

        return jobs

    def extract_tech_stack(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ"""
        found_techs = []
        text_lower = text.lower()

        for tech, keywords in TECH_KEYWORDS.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    found_techs.append(tech)
                    break

        return list(set(found_techs))

    def analyze_jobs(self, jobs: List[Dict]) -> Dict:
        """ìˆ˜ì§‘ëœ ê³µê³  ë¶„ì„"""
        print("\n" + "="*60)
        print("ğŸ“Š ì±„ìš©ê³µê³  ë¶„ì„ ê²°ê³¼")
        print("="*60)

        # ê¸°ìˆ ìŠ¤íƒ ë¹ˆë„ ë¶„ì„
        all_techs = []
        for job in jobs:
            techs = self.extract_tech_stack(job['raw_text'])
            job['tech_stack'] = techs
            all_techs.extend(techs)

        tech_counter = Counter(all_techs)

        # ê²°ê³¼ ì •ë¦¬
        result = {
            'total_jobs': len(jobs),
            'by_source': Counter(job['source'] for job in jobs),
            'tech_frequency': dict(tech_counter.most_common(30)),
            'top_companies': Counter(job['company'] for job in jobs if job['company']).most_common(20),
        }

        # ì¶œë ¥
        print(f"\nì´ ìˆ˜ì§‘ ê³µê³ : {result['total_jobs']}ê°œ")
        print(f"\nì¶œì²˜ë³„:")
        for source, count in result['by_source'].items():
            print(f"  - {source}: {count}ê°œ")

        print(f"\nğŸ”§ ê¸°ìˆ ìŠ¤íƒ Top 20:")
        for i, (tech, count) in enumerate(tech_counter.most_common(20), 1):
            percentage = (count / len(jobs)) * 100
            bar = 'â–ˆ' * int(percentage / 2)
            print(f"  {i:2}. {tech:15} | {bar:25} {count:3}ê°œ ({percentage:.1f}%)")

        print(f"\nğŸ¢ ì±„ìš© í™œë°œí•œ íšŒì‚¬ Top 10:")
        for company, count in result['top_companies'][:10]:
            print(f"  - {company}: {count}ê°œ")

        return result

    def save_results(self, jobs: List[Dict], result: Dict, output_dir: str = '.'):
        """ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # CSV ì €ì¥
        df = pd.DataFrame(jobs)
        csv_path = f"{output_dir}/jobs_{timestamp}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“ CSV ì €ì¥: {csv_path}")

        # JSON ì €ì¥
        json_path = f"{output_dir}/analysis_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': timestamp,
                'total_jobs': result['total_jobs'],
                'by_source': dict(result['by_source']),
                'tech_frequency': result['tech_frequency'],
                'top_companies': result['top_companies'],
            }, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ ë¶„ì„ ê²°ê³¼ ì €ì¥: {json_path}")

        # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        md_path = f"{output_dir}/report_{timestamp}.md"
        self._generate_report(result, jobs, md_path)
        print(f"ğŸ“ ë¦¬í¬íŠ¸ ì €ì¥: {md_path}")

    def _generate_report(self, result: Dict, jobs: List[Dict], path: str):
        """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        with open(path, 'w', encoding='utf-8') as f:
            f.write(f"# ì±„ìš©ê³µê³  ë¶„ì„ ë¦¬í¬íŠ¸\n\n")
            f.write(f"ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"## ê°œìš”\n\n")
            f.write(f"- ì´ ë¶„ì„ ê³µê³ : {result['total_jobs']}ê°œ\n")
            for source, count in result['by_source'].items():
                f.write(f"- {source}: {count}ê°œ\n")

            f.write(f"\n## ê¸°ìˆ ìŠ¤íƒ ë¶„ì„\n\n")
            f.write(f"| ìˆœìœ„ | ê¸°ìˆ  | ë“±ì¥ íšŸìˆ˜ | ë¹„ìœ¨ |\n")
            f.write(f"|------|------|----------|------|\n")
            for i, (tech, count) in enumerate(result['tech_frequency'].items(), 1):
                percentage = (count / result['total_jobs']) * 100
                f.write(f"| {i} | {tech} | {count} | {percentage:.1f}% |\n")

            f.write(f"\n## ì±„ìš© í™œë°œí•œ íšŒì‚¬\n\n")
            for company, count in result['top_companies'][:15]:
                f.write(f"- {company}: {count}ê°œ\n")


def main():
    crawler = JobCrawler()
    all_jobs = []

    # ê²€ìƒ‰ í‚¤ì›Œë“œ
    keywords = ['ë°ì´í„° ì—”ì§€ë‹ˆì–´', 'ë°±ì—”ë“œ ê°œë°œì', 'backend developer', 'data engineer']

    print("="*60)
    print("ğŸ” ì±„ìš©ê³µê³  í¬ë¡¤ë§ ì‹œì‘")
    print("="*60)

    for keyword in keywords:
        # ì‚¬ëŒì¸ í¬ë¡¤ë§
        saramin_jobs = crawler.crawl_saramin(keyword, pages=3)
        all_jobs.extend(saramin_jobs)

        # ì¡ì½”ë¦¬ì•„ í¬ë¡¤ë§
        jobkorea_jobs = crawler.crawl_jobkorea(keyword, pages=3)
        all_jobs.extend(jobkorea_jobs)

        # ì›í‹°ë“œ í¬ë¡¤ë§
        wanted_jobs = crawler.crawl_wanted(keyword, limit=30)
        all_jobs.extend(wanted_jobs)

        time.sleep(2)  # í‚¤ì›Œë“œ ê°„ ê°„ê²©

    # ì¤‘ë³µ ì œê±° (íšŒì‚¬ëª…+ì œëª© ê¸°ì¤€)
    seen = set()
    unique_jobs = []
    for job in all_jobs:
        key = (job['company'], job['title'])
        if key not in seen:
            seen.add(key)
            unique_jobs.append(job)

    print(f"\nì¤‘ë³µ ì œê±° í›„: {len(unique_jobs)}ê°œ (ì›ë³¸: {len(all_jobs)}ê°œ)")

    # ë¶„ì„
    result = crawler.analyze_jobs(unique_jobs)

    # ì €ì¥
    crawler.save_results(unique_jobs, result, output_dir='/home/junhyun/job_crawler')

    print("\n" + "="*60)
    print("âœ… í¬ë¡¤ë§ ë° ë¶„ì„ ì™„ë£Œ!")
    print("="*60)


if __name__ == '__main__':
    main()
