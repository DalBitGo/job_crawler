#!/usr/bin/env python3
"""
ì±„ìš©ê³µê³  ìƒì„¸ í¬ë¡¤ëŸ¬ - ì‚¬ëŒì¸, ì›í‹°ë“œì—ì„œ ìƒì„¸ ê³µê³  ë‚´ìš©ê¹Œì§€ ìˆ˜ì§‘
ìê²©ìš”ê±´, ìš°ëŒ€ì‚¬í•­, ê¸°ìˆ ìŠ¤íƒ, ì—°ë´‰ ì •ë³´ ë“± í¬í•¨
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
import json
from collections import Counter
from datetime import datetime
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
warnings.filterwarnings('ignore')

# ê¸°ìˆ ìŠ¤íƒ í‚¤ì›Œë“œ ì •ì˜ (ë” ìƒì„¸í•˜ê²Œ)
TECH_KEYWORDS = {
    # ì–¸ì–´
    'Python': ['python', 'íŒŒì´ì¬'],
    'Java': ['java', 'ìë°”'],
    'Kotlin': ['kotlin', 'ì½”í‹€ë¦°'],
    'Go': ['golang', 'goì–¸ì–´', 'go '],
    'Scala': ['scala', 'ìŠ¤ì¹¼ë¼'],
    'TypeScript': ['typescript', 'ts'],
    'JavaScript': ['javascript', 'js', 'node.js', 'nodejs'],
    'SQL': ['sql'],
    'C++': ['c++', 'cpp'],
    'Rust': ['rust', 'ëŸ¬ìŠ¤íŠ¸'],
    'Ruby': ['ruby', 'ë£¨ë¹„'],
    'PHP': ['php'],

    # ë°±ì—”ë“œ í”„ë ˆì„ì›Œí¬
    'Spring': ['spring', 'spring boot', 'springboot', 'ìŠ¤í”„ë§'],
    'Spring Boot': ['spring boot', 'springboot', 'ìŠ¤í”„ë§ë¶€íŠ¸', 'ìŠ¤í”„ë§ ë¶€íŠ¸'],
    'Django': ['django', 'ì¥ê³ '],
    'FastAPI': ['fastapi', 'fast api'],
    'Flask': ['flask', 'í”Œë¼ìŠ¤í¬'],
    'Express': ['express', 'express.js'],
    'NestJS': ['nestjs', 'nest.js'],
    'JPA': ['jpa', 'hibernate', 'í•˜ì´ë²„ë„¤ì´íŠ¸'],
    'MyBatis': ['mybatis', 'ë§ˆì´ë°”í‹°ìŠ¤'],

    # ë°ì´í„° ì²˜ë¦¬
    'Spark': ['spark', 'pyspark', 'ìŠ¤íŒŒí¬', 'apache spark'],
    'Hadoop': ['hadoop', 'í•˜ë‘¡'],
    'Airflow': ['airflow', 'apache airflow', 'ì—ì–´í”Œë¡œìš°'],
    'Kafka': ['kafka', 'ì¹´í”„ì¹´', 'apache kafka'],
    'Flink': ['flink', 'í”Œë§í¬'],
    'Presto': ['presto', 'trino'],
    'Hive': ['hive', 'í•˜ì´ë¸Œ'],
    'dbt': ['dbt', 'data build tool'],
    'ETL': ['etl', 'elt'],
    'Data Pipeline': ['data pipeline', 'ë°ì´í„° íŒŒì´í”„ë¼ì¸'],

    # ë°ì´í„°ë² ì´ìŠ¤
    'MySQL': ['mysql', 'mariadb'],
    'PostgreSQL': ['postgresql', 'postgres', 'psql'],
    'Oracle': ['oracle', 'ì˜¤ë¼í´'],
    'MongoDB': ['mongodb', 'ëª½ê³ db', 'ëª½ê³ ë””ë¹„'],
    'Redis': ['redis', 'ë ˆë””ìŠ¤'],
    'Elasticsearch': ['elasticsearch', 'elastic search', 'elk'],
    'DynamoDB': ['dynamodb'],
    'Redshift': ['redshift', 'ë ˆë“œì‹œí”„íŠ¸'],
    'BigQuery': ['bigquery', 'ë¹…ì¿¼ë¦¬'],
    'Snowflake': ['snowflake', 'ìŠ¤ë…¸ìš°í”Œë ˆì´í¬'],
    'Cassandra': ['cassandra', 'ì¹´ì‚°ë“œë¼'],
    'ClickHouse': ['clickhouse', 'í´ë¦­í•˜ìš°ìŠ¤'],

    # í´ë¼ìš°ë“œ
    'AWS': ['aws', 'amazon web services', 'ec2', 's3', 'lambda', 'ecs', 'eks', 'rds', 'emr', 'athena', 'glue'],
    'GCP': ['gcp', 'google cloud', 'gce', 'bigquery', 'dataflow'],
    'Azure': ['azure', 'ì• ì €', 'microsoft azure'],
    'NCP': ['ncp', 'naver cloud', 'ë„¤ì´ë²„ í´ë¼ìš°ë“œ'],

    # ì»¨í…Œì´ë„ˆ/ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
    'Docker': ['docker', 'ë„ì»¤'],
    'Kubernetes': ['kubernetes', 'k8s', 'ì¿ ë²„ë„¤í‹°ìŠ¤'],
    'ECS': ['ecs', 'fargate'],
    'EKS': ['eks'],

    # CI/CD & DevOps
    'Jenkins': ['jenkins', 'ì  í‚¨ìŠ¤'],
    'GitHub Actions': ['github actions', 'github action'],
    'GitLab CI': ['gitlab ci', 'gitlab-ci'],
    'ArgoCD': ['argocd', 'argo cd', 'argo'],
    'Terraform': ['terraform', 'í…Œë¼í¼'],
    'Ansible': ['ansible', 'ì•¤ì„œë¸”'],
    'Helm': ['helm', 'í—¬ë¦„'],

    # ëª¨ë‹ˆí„°ë§/ë¡œê¹…
    'Prometheus': ['prometheus', 'í”„ë¡œë©”í…Œìš°ìŠ¤'],
    'Grafana': ['grafana', 'ê·¸ë¼íŒŒë‚˜'],
    'Datadog': ['datadog', 'ë°ì´í„°ë…'],
    'ELK Stack': ['elk', 'logstash', 'kibana'],

    # ê¸°íƒ€
    'Linux': ['linux', 'ë¦¬ëˆ…ìŠ¤', 'ubuntu', 'centos'],
    'Git': ['git', 'ê¹ƒ', 'github', 'gitlab'],
    'REST API': ['rest api', 'restful', 'rest'],
    'GraphQL': ['graphql', 'ê·¸ë˜í”„íì—˜'],
    'gRPC': ['grpc'],
    'MSA': ['msa', 'microservice', 'ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤'],
    'Message Queue': ['message queue', 'mq', 'rabbitmq', 'sqs'],
    'CI/CD': ['ci/cd', 'cicd', 'ci cd', 'ì§€ì†ì  í†µí•©'],
    'Agile': ['agile', 'ì• ìì¼', 'scrum', 'ìŠ¤í¬ëŸ¼'],
    'TDD': ['tdd', 'test driven', 'í…ŒìŠ¤íŠ¸ ì£¼ë„'],
}

# ìê²©ìš”ê±´ ê´€ë ¨ í‚¤ì›Œë“œ
QUALIFICATION_PATTERNS = {
    'experience': [
        r'(\d+)\s*ë…„\s*ì´ìƒ',
        r'ê²½ë ¥\s*(\d+)\s*ë…„',
        r'(\d+)\s*~\s*(\d+)\s*ë…„',
        r'(\d+)ë…„ì°¨',
    ],
    'education': [
        r'(ëŒ€ì¡¸|í•™ì‚¬|ì„ì‚¬|ë°•ì‚¬|ì´ˆëŒ€ì¡¸|ê³ ì¡¸)',
        r'(ì»´í“¨í„°|ì „ì‚°|ì •ë³´|ì†Œí”„íŠ¸ì›¨ì–´|IT)\s*(ê³µí•™|ê³¼í•™|í•™ê³¼)',
    ],
}


class DetailedJobCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def crawl_saramin_list(self, keyword: str, pages: int = 5) -> List[Dict]:
        """ì‚¬ëŒì¸ ì±„ìš©ê³µê³  ëª©ë¡ í¬ë¡¤ë§"""
        print(f"\n[ì‚¬ëŒì¸] '{keyword}' ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
        jobs = []

        for page in range(1, pages + 1):
            url = f"https://www.saramin.co.kr/zf_user/search/recruit?searchType=search&searchword={keyword}&recruitPage={page}"

            try:
                response = self.session.get(url, timeout=10)
                if response.status_code != 200:
                    continue

                soup = BeautifulSoup(response.text, 'html.parser')
                job_items = soup.select('.item_recruit')

                for item in job_items:
                    try:
                        title_elem = item.select_one('.job_tit a')
                        company_elem = item.select_one('.corp_name a')
                        conditions = item.select('.job_condition span')

                        if not title_elem:
                            continue

                        href = title_elem.get('href', '')
                        # rec_idx ì¶”ì¶œ
                        rec_idx_match = re.search(r'rec_idx=(\d+)', href)
                        if not rec_idx_match:
                            continue

                        job = {
                            'source': 'ì‚¬ëŒì¸',
                            'title': title_elem.get_text(strip=True),
                            'company': company_elem.get_text(strip=True) if company_elem else '',
                            'rec_idx': rec_idx_match.group(1),
                            'link': f"https://www.saramin.co.kr/zf_user/jobs/relay/view?rec_idx={rec_idx_match.group(1)}",
                            'conditions': [c.get_text(strip=True) for c in conditions],
                        }
                        jobs.append(job)
                    except Exception:
                        continue

                print(f"  í˜ì´ì§€ {page}: {len(job_items)}ê°œ")
                time.sleep(0.5)

            except Exception as e:
                print(f"  í˜ì´ì§€ {page}: ì˜¤ë¥˜ - {e}")
                continue

        print(f"[ì‚¬ëŒì¸] ëª©ë¡ {len(jobs)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return jobs

    def get_saramin_detail(self, job: Dict) -> Dict:
        """ì‚¬ëŒì¸ ìƒì„¸ ê³µê³  í¬ë¡¤ë§"""
        try:
            url = job['link']
            response = self.session.get(url, timeout=10)
            if response.status_code != 200:
                return job

            soup = BeautifulSoup(response.text, 'html.parser')

            # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            detail = {
                'qualifications': '',      # ìê²©ìš”ê±´
                'preferred': '',           # ìš°ëŒ€ì‚¬í•­
                'responsibilities': '',    # ë‹´ë‹¹ì—…ë¬´
                'benefits': '',            # ë³µë¦¬í›„ìƒ
                'salary': '',              # ì—°ë´‰
                'detail_tech_stack': [],   # ìƒì„¸ ê¸°ìˆ ìŠ¤íƒ
                'experience_years': '',    # ê²½ë ¥ ì—°ì°¨
                'education': '',           # í•™ë ¥
                'full_description': '',    # ì „ì²´ ì„¤ëª…
            }

            # ì±„ìš© ìƒì„¸ ì„¹ì…˜ ì°¾ê¸°
            detail_sections = soup.select('.jv_cont')

            for section in detail_sections:
                header = section.select_one('.jv_header, .tit_cont')
                content = section.select_one('.jv_detail, .cont')

                if not header or not content:
                    continue

                header_text = header.get_text(strip=True)
                content_text = content.get_text('\n', strip=True)

                if any(k in header_text for k in ['ìê²©ìš”ê±´', 'ìê²© ìš”ê±´', 'í•„ìˆ˜', 'ì§€ì›ìê²©']):
                    detail['qualifications'] = content_text
                elif any(k in header_text for k in ['ìš°ëŒ€', 'ì„ í˜¸', 'ê°€ì‚°ì ']):
                    detail['preferred'] = content_text
                elif any(k in header_text for k in ['ë‹´ë‹¹ì—…ë¬´', 'ì—…ë¬´ë‚´ìš©', 'ì£¼ìš”ì—…ë¬´', 'ë‹´ë‹¹ ì—…ë¬´']):
                    detail['responsibilities'] = content_text
                elif any(k in header_text for k in ['ë³µë¦¬í›„ìƒ', 'í˜œíƒ', 'ë³µì§€']):
                    detail['benefits'] = content_text

            # ì—°ë´‰ ì •ë³´
            salary_elem = soup.select_one('.salary')
            if salary_elem:
                detail['salary'] = salary_elem.get_text(strip=True)

            # ê²½ë ¥/í•™ë ¥ ì •ë³´
            career_elem = soup.select_one('.career')
            if career_elem:
                detail['experience_years'] = career_elem.get_text(strip=True)

            edu_elem = soup.select_one('.education')
            if edu_elem:
                detail['education'] = edu_elem.get_text(strip=True)

            # ì „ì²´ ì„¤ëª… (ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œìš©)
            job_detail = soup.select_one('.jv_detail, .job_detail, .wrap_jv_cont')
            if job_detail:
                detail['full_description'] = job_detail.get_text('\n', strip=True)

            # ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ
            full_text = f"{detail['qualifications']} {detail['preferred']} {detail['responsibilities']} {detail['full_description']}"
            detail['detail_tech_stack'] = self.extract_tech_stack(full_text)

            job.update(detail)
            return job

        except Exception as e:
            job['error'] = str(e)
            return job

    def crawl_wanted_list(self, keyword: str, limit: int = 50) -> List[Dict]:
        """ì›í‹°ë“œ ì±„ìš©ê³µê³  ëª©ë¡ í¬ë¡¤ë§"""
        print(f"\n[ì›í‹°ë“œ] '{keyword}' ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
        jobs = []

        url = f"https://www.wanted.co.kr/api/v4/jobs?country=kr&job_sort=company.response_rate_order&years=-1&locations=all&query={keyword}&limit={limit}"

        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                for item in data.get('data', []):
                    job = {
                        'source': 'ì›í‹°ë“œ',
                        'title': item.get('position', ''),
                        'company': item.get('company', {}).get('name', ''),
                        'job_id': item.get('id', ''),
                        'link': f"https://www.wanted.co.kr/wd/{item.get('id', '')}",
                        'conditions': [],
                    }
                    jobs.append(job)
                print(f"[ì›í‹°ë“œ] ëª©ë¡ {len(jobs)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        except Exception as e:
            print(f"[ì›í‹°ë“œ] ì˜¤ë¥˜: {e}")

        return jobs

    def get_wanted_detail(self, job: Dict) -> Dict:
        """ì›í‹°ë“œ ìƒì„¸ ê³µê³  í¬ë¡¤ë§ (API)"""
        try:
            job_id = job.get('job_id', '')
            if not job_id:
                return job

            url = f"https://www.wanted.co.kr/api/v4/jobs/{job_id}"
            response = self.session.get(url, timeout=10)

            if response.status_code != 200:
                return job

            data = response.json()
            job_data = data.get('job', {})

            detail = {
                'qualifications': job_data.get('requirements', ''),
                'preferred': job_data.get('preferred', ''),
                'responsibilities': job_data.get('responsibilities', ''),
                'benefits': job_data.get('benefits', ''),
                'salary': '',
                'detail_tech_stack': [],
                'experience_years': '',
                'education': '',
                'full_description': job_data.get('detail', ''),
            }

            # ê¸°ìˆ  íƒœê·¸
            skill_tags = job_data.get('skill_tags', [])
            if skill_tags:
                detail['detail_tech_stack'] = [tag.get('title', '') for tag in skill_tags]

            # ì¶”ê°€ ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ
            full_text = f"{detail['qualifications']} {detail['preferred']} {detail['responsibilities']} {detail['full_description']}"
            extracted_techs = self.extract_tech_stack(full_text)
            detail['detail_tech_stack'] = list(set(detail['detail_tech_stack'] + extracted_techs))

            job.update(detail)
            return job

        except Exception as e:
            job['error'] = str(e)
            return job

    def extract_tech_stack(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ"""
        found_techs = []
        text_lower = text.lower()

        for tech, keywords in TECH_KEYWORDS.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    found_techs.append(tech)
                    break

        return list(set(found_techs))

    def extract_experience(self, text: str) -> str:
        """ê²½ë ¥ ì—°ì°¨ ì¶”ì¶œ"""
        for pattern in QUALIFICATION_PATTERNS['experience']:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        return ''

    def crawl_with_details(self, keywords: List[str], saramin_pages: int = 3, wanted_limit: int = 30) -> List[Dict]:
        """ëª©ë¡ + ìƒì„¸ ì •ë³´ í¬ë¡¤ë§"""
        all_jobs = []

        for keyword in keywords:
            # ì‚¬ëŒì¸
            saramin_jobs = self.crawl_saramin_list(keyword, pages=saramin_pages)
            print(f"[ì‚¬ëŒì¸] '{keyword}' ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘... ({len(saramin_jobs)}ê°œ)")

            for i, job in enumerate(saramin_jobs):
                self.get_saramin_detail(job)
                if (i + 1) % 10 == 0:
                    print(f"  {i+1}/{len(saramin_jobs)} ì™„ë£Œ")
                time.sleep(0.3)  # ìš”ì²­ ê°„ê²©

            all_jobs.extend(saramin_jobs)

            # ì›í‹°ë“œ
            wanted_jobs = self.crawl_wanted_list(keyword, limit=wanted_limit)
            print(f"[ì›í‹°ë“œ] '{keyword}' ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘... ({len(wanted_jobs)}ê°œ)")

            for i, job in enumerate(wanted_jobs):
                self.get_wanted_detail(job)
                if (i + 1) % 10 == 0:
                    print(f"  {i+1}/{len(wanted_jobs)} ì™„ë£Œ")
                time.sleep(0.3)

            all_jobs.extend(wanted_jobs)
            time.sleep(1)

        return all_jobs

    def analyze_detailed_jobs(self, jobs: List[Dict]) -> Dict:
        """ìƒì„¸ ê³µê³  ë¶„ì„"""
        print("\n" + "="*70)
        print("ğŸ“Š ìƒì„¸ ì±„ìš©ê³µê³  ë¶„ì„ ê²°ê³¼")
        print("="*70)

        # ê¸°ìˆ ìŠ¤íƒ ë¹ˆë„ ë¶„ì„ (ìƒì„¸ ì •ë³´ ê¸°ë°˜)
        all_techs = []
        for job in jobs:
            techs = job.get('detail_tech_stack', [])
            all_techs.extend(techs)

        tech_counter = Counter(all_techs)

        # ìê²©ìš”ê±´ í‚¤ì›Œë“œ ë¶„ì„
        qual_keywords = []
        for job in jobs:
            qual_text = f"{job.get('qualifications', '')} {job.get('preferred', '')}"
            qual_keywords.extend(self.extract_tech_stack(qual_text))

        qual_counter = Counter(qual_keywords)

        # ê²½ë ¥ ë¶„ì„
        experience_list = []
        for job in jobs:
            exp = job.get('experience_years', '')
            if exp:
                experience_list.append(exp)

        # ê²°ê³¼ ì •ë¦¬
        result = {
            'total_jobs': len(jobs),
            'jobs_with_details': len([j for j in jobs if j.get('qualifications') or j.get('full_description')]),
            'by_source': dict(Counter(job['source'] for job in jobs)),
            'tech_frequency': dict(tech_counter.most_common(40)),
            'qualification_tech_frequency': dict(qual_counter.most_common(30)),
            'top_companies': Counter(job['company'] for job in jobs if job['company']).most_common(20),
            'experience_distribution': Counter(experience_list).most_common(10),
        }

        # ì¶œë ¥
        print(f"\nğŸ“Œ ì´ ìˆ˜ì§‘ ê³µê³ : {result['total_jobs']}ê°œ")
        print(f"   ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì„±ê³µ: {result['jobs_with_details']}ê°œ")

        print(f"\nğŸ“ ì¶œì²˜ë³„:")
        for source, count in result['by_source'].items():
            print(f"   - {source}: {count}ê°œ")

        print(f"\nğŸ”§ ê¸°ìˆ ìŠ¤íƒ Top 25 (ìƒì„¸ ê³µê³  ê¸°ë°˜):")
        for i, (tech, count) in enumerate(tech_counter.most_common(25), 1):
            percentage = (count / len(jobs)) * 100
            bar = 'â–ˆ' * int(percentage / 2)
            print(f"  {i:2}. {tech:18} | {bar:25} {count:3}ê°œ ({percentage:.1f}%)")

        print(f"\nğŸ“‹ ìê²©ìš”ê±´ì—ì„œ ë§ì´ ì–¸ê¸‰ëœ ê¸°ìˆ  Top 15:")
        for i, (tech, count) in enumerate(qual_counter.most_common(15), 1):
            percentage = (count / len(jobs)) * 100
            print(f"  {i:2}. {tech:18} - {count:3}ê°œ ({percentage:.1f}%)")

        print(f"\nğŸ¢ ì±„ìš© í™œë°œí•œ íšŒì‚¬ Top 10:")
        for company, count in result['top_companies'][:10]:
            print(f"   - {company}: {count}ê°œ")

        if result['experience_distribution']:
            print(f"\nğŸ“… ê²½ë ¥ ìš”êµ¬ì‚¬í•­:")
            for exp, count in result['experience_distribution']:
                print(f"   - {exp}: {count}ê°œ")

        return result

    def save_detailed_results(self, jobs: List[Dict], result: Dict, output_dir: str = '.'):
        """ìƒì„¸ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # CSV ì €ì¥ (ìƒì„¸ ì •ë³´ í¬í•¨)
        df_data = []
        for job in jobs:
            df_data.append({
                'source': job.get('source', ''),
                'company': job.get('company', ''),
                'title': job.get('title', ''),
                'link': job.get('link', ''),
                'experience_years': job.get('experience_years', ''),
                'education': job.get('education', ''),
                'salary': job.get('salary', ''),
                'tech_stack': ', '.join(job.get('detail_tech_stack', [])),
                'qualifications': job.get('qualifications', '')[:500],  # 500ì ì œí•œ
                'preferred': job.get('preferred', '')[:500],
                'responsibilities': job.get('responsibilities', '')[:500],
                'benefits': job.get('benefits', '')[:300],
            })

        df = pd.DataFrame(df_data)
        csv_path = f"{output_dir}/jobs_detailed_{timestamp}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“ ìƒì„¸ CSV ì €ì¥: {csv_path}")

        # JSON ì €ì¥ (ì „ì²´ ë°ì´í„°)
        json_path = f"{output_dir}/jobs_full_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(jobs, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ ì „ì²´ JSON ì €ì¥: {json_path}")

        # ë¶„ì„ ê²°ê³¼ JSON
        analysis_path = f"{output_dir}/analysis_detailed_{timestamp}.json"
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ ë¶„ì„ ê²°ê³¼ ì €ì¥: {analysis_path}")

        # ìƒì„¸ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸
        md_path = f"{output_dir}/report_detailed_{timestamp}.md"
        self._generate_detailed_report(result, jobs, md_path)
        print(f"ğŸ“ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: {md_path}")

        return timestamp

    def _generate_detailed_report(self, result: Dict, jobs: List[Dict], path: str):
        """ìƒì„¸ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        with open(path, 'w', encoding='utf-8') as f:
            f.write(f"# ì±„ìš©ê³µê³  ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸\n\n")
            f.write(f"ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            f.write(f"## ê°œìš”\n\n")
            f.write(f"- **ì´ ë¶„ì„ ê³µê³ **: {result['total_jobs']}ê°œ\n")
            f.write(f"- **ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì„±ê³µ**: {result['jobs_with_details']}ê°œ\n")
            for source, count in result['by_source'].items():
                f.write(f"- {source}: {count}ê°œ\n")

            f.write(f"\n## ê¸°ìˆ ìŠ¤íƒ ë¶„ì„ (ìƒì„¸ ê³µê³  ê¸°ë°˜)\n\n")
            f.write(f"| ìˆœìœ„ | ê¸°ìˆ  | ë“±ì¥ íšŸìˆ˜ | ë¹„ìœ¨ |\n")
            f.write(f"|------|------|----------|------|\n")
            for i, (tech, count) in enumerate(result['tech_frequency'].items(), 1):
                if i > 30:
                    break
                percentage = (count / result['total_jobs']) * 100
                f.write(f"| {i} | {tech} | {count} | {percentage:.1f}% |\n")

            f.write(f"\n## ìê²©ìš”ê±´ í•„ìˆ˜ ê¸°ìˆ \n\n")
            f.write(f"| ìˆœìœ„ | ê¸°ìˆ  | ë“±ì¥ íšŸìˆ˜ | ë¹„ìœ¨ |\n")
            f.write(f"|------|------|----------|------|\n")
            for i, (tech, count) in enumerate(result['qualification_tech_frequency'].items(), 1):
                if i > 20:
                    break
                percentage = (count / result['total_jobs']) * 100
                f.write(f"| {i} | {tech} | {count} | {percentage:.1f}% |\n")

            f.write(f"\n## ì±„ìš© í™œë°œí•œ íšŒì‚¬\n\n")
            for company, count in result['top_companies'][:15]:
                f.write(f"- **{company}**: {count}ê°œ\n")

            f.write(f"\n## ì£¼ìš” ìê²©ìš”ê±´ ìƒ˜í”Œ\n\n")
            sample_jobs = [j for j in jobs if j.get('qualifications')][:5]
            for job in sample_jobs:
                f.write(f"### {job.get('company', '')} - {job.get('title', '')[:50]}\n\n")
                f.write(f"**ê¸°ìˆ ìŠ¤íƒ**: {', '.join(job.get('detail_tech_stack', []))}\n\n")
                qual = job.get('qualifications', '')[:800]
                if qual:
                    f.write(f"**ìê²©ìš”ê±´**:\n```\n{qual}\n```\n\n")
                pref = job.get('preferred', '')[:500]
                if pref:
                    f.write(f"**ìš°ëŒ€ì‚¬í•­**:\n```\n{pref}\n```\n\n")
                f.write(f"---\n\n")


def main():
    crawler = DetailedJobCrawler()

    keywords = ['ë°ì´í„° ì—”ì§€ë‹ˆì–´', 'ë°±ì—”ë“œ ê°œë°œì', 'backend developer', 'data engineer']

    print("="*70)
    print("ğŸ” ì±„ìš©ê³µê³  ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘")
    print("   (ê° ê³µê³ ì˜ ìƒì„¸ í˜ì´ì§€ë¥¼ ë°©ë¬¸í•˜ì—¬ ìê²©ìš”ê±´/ìš°ëŒ€ì‚¬í•­ ë“± ìˆ˜ì§‘)")
    print("="*70)

    # ìƒì„¸ í¬ë¡¤ë§ (ì‹œê°„ì´ ì¢€ ê±¸ë¦¼)
    jobs = crawler.crawl_with_details(
        keywords=keywords,
        saramin_pages=3,  # ì‚¬ëŒì¸ í˜ì´ì§€ ìˆ˜
        wanted_limit=30   # ì›í‹°ë“œ ê³µê³  ìˆ˜
    )

    # ì¤‘ë³µ ì œê±°
    seen = set()
    unique_jobs = []
    for job in jobs:
        key = (job.get('company', ''), job.get('title', ''))
        if key not in seen and key[0]:
            seen.add(key)
            unique_jobs.append(job)

    print(f"\nì¤‘ë³µ ì œê±° í›„: {len(unique_jobs)}ê°œ (ì›ë³¸: {len(jobs)}ê°œ)")

    # ë¶„ì„
    result = crawler.analyze_detailed_jobs(unique_jobs)

    # ì €ì¥
    timestamp = crawler.save_detailed_results(
        unique_jobs,
        result,
        output_dir='/home/junhyun/job_crawler/employment'
    )

    print("\n" + "="*70)
    print("âœ… ìƒì„¸ í¬ë¡¤ë§ ë° ë¶„ì„ ì™„ë£Œ!")
    print("="*70)


if __name__ == '__main__':
    main()
